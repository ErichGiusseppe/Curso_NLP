{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "200ddfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/erich/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from utils_taller3 import SenticLexiconFeaturizer\n",
    "from data.EN_Lexicons.senticnet5 import senticnet\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "ACTUAL_PATH = os.getcwd()\n",
    "PATH_20N = os.path.join(ACTUAL_PATH, \"data/20news-18828\")\n",
    "PATH_MD = os.path.join(ACTUAL_PATH, \"data/Multi Domain Sentiment/processed_acl\")\n",
    "PATH_FINAL_FILES = os.path.join(ACTUAL_PATH, \"data/final_files\")\n",
    "\n",
    "stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "val_ratio_within_train = 1.0 / 7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd1d99",
   "metadata": {},
   "source": [
    "### Parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c6feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(opt, X_test, y_test, print_flag=False):\n",
    "    \"\"\"\n",
    "    Evalúa un modelo optimizado (BayesSearchCV) en el conjunto de test.\n",
    "    Devuelve un diccionario con métricas.\n",
    "    \"\"\"\n",
    "    y_pred = opt.predict(X_test)\n",
    "\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"macro\"\n",
    "    )\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"micro\"\n",
    "    )\n",
    "\n",
    "    resultados = {\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"recall_macro\": recall_macro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"precision_micro\": precision_micro,\n",
    "        \"recall_micro\": recall_micro,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"best_params\": opt.best_params_,\n",
    "    }\n",
    "    if print_flag:\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return resultados\n",
    "\n",
    "\n",
    "def top_features_pos_neg(est, k):\n",
    "    \"\"\"\n",
    "    Funcion que devuelve los mas positivos y los mas negativos\n",
    "    \"\"\"\n",
    "    pipe = est.best_estimator_ if hasattr(est, \"best_estimator_\") else est\n",
    "    vect = pipe.named_steps[\"vect\"]\n",
    "    lr = pipe.named_steps[\"model\"]\n",
    "\n",
    "    nombres = vect.get_feature_names_out()\n",
    "    w = lr.coef_.ravel()\n",
    "\n",
    "    idx_pos = np.argsort(w)[-k:][::-1]\n",
    "    idx_neg = np.argsort(w)[:k]\n",
    "\n",
    "    top_pos = list(zip(nombres[idx_pos], w[idx_pos]))\n",
    "    top_neg = list(zip(nombres[idx_neg], w[idx_neg]))\n",
    "    return top_pos, top_neg\n",
    "\n",
    "\n",
    "def mostrar_resultados_tabulate(resultados_segunda_parte, ordenar_por=\"f1_macro\"):\n",
    "    \"\"\"\n",
    "    Muestra los resultados en formato de tabla usando tabulate.\n",
    "    Ordena por la métrica especificada (default: f1_macro).\n",
    "    \"\"\"\n",
    "    ejemplo = next(iter(resultados_segunda_parte.values()))\n",
    "    columnas = [\"Modelo\"] + list(ejemplo.keys())\n",
    "\n",
    "    filas = []\n",
    "    for modelo, metricas in resultados_segunda_parte.items():\n",
    "        fila = [modelo]\n",
    "        for valor in metricas.values():\n",
    "            fila.append(round(valor, 4) if isinstance(valor, (int, float)) else valor)\n",
    "        filas.append(fila)\n",
    "\n",
    "    if ordenar_por in ejemplo:\n",
    "        idx = columnas.index(ordenar_por)\n",
    "        filas.sort(key=lambda x: x[idx], reverse=True)\n",
    "\n",
    "    print(tabulate(filas, headers=columnas, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62032fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_review_line(line):\n",
    "    \"\"\"\n",
    "    Devuelve (dict(token->count), label_or_None)\n",
    "    \"\"\"\n",
    "    line = line.strip()\n",
    "    label = None\n",
    "    m = re.search(r\"#label#:(positive|negative)$\", line)\n",
    "    if m:\n",
    "        label = m.group(1)\n",
    "        line = line[: m.start()].strip()\n",
    "    parts = line.split()\n",
    "    d = {}\n",
    "    for p in parts:\n",
    "        if \":\" in p:\n",
    "            tok, cnt = p.rsplit(\":\", 1)\n",
    "            try:\n",
    "                d[tok] = int(cnt)\n",
    "            except:\n",
    "                try:\n",
    "                    d[tok] = float(cnt)\n",
    "                except:\n",
    "                    d[tok] = 1\n",
    "    return d, label\n",
    "\n",
    "\n",
    "def load_reviews(filepath):\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            d, label = parse_review_line(line)\n",
    "            if d:\n",
    "                X.append(d)\n",
    "                y.append(label)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def build_train_test(base_path: str):\n",
    "    domains = [\n",
    "        d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))\n",
    "    ]\n",
    "    data = {}\n",
    "\n",
    "    for domain in domains:\n",
    "        domain_path = os.path.join(base_path, domain)\n",
    "\n",
    "        X_pos, y_pos = load_reviews(os.path.join(domain_path, \"positive.review\"))\n",
    "        X_neg, y_neg = load_reviews(os.path.join(domain_path, \"negative.review\"))\n",
    "        X_unl, y_unl = load_reviews(os.path.join(domain_path, \"unlabeled.review\"))\n",
    "\n",
    "        min_len = min(len(X_pos), len(X_neg))\n",
    "        # print(max(len(X_pos), len(X_neg)))\n",
    "        X_pos, y_pos = X_pos[:min_len], y_pos[:min_len]\n",
    "        X_neg, y_neg = X_neg[:min_len], y_neg[:min_len]\n",
    "\n",
    "        data[domain] = {\"X\": X_pos + X_neg, \"y\": y_pos + y_neg, \"rest\": (X_unl, y_unl)}\n",
    "\n",
    "        print(\n",
    "            f\"[{domain}] Positive: {len(X_pos)}, Negative: {len(X_neg)}, Rest(unlabeled): {len(X_unl)}\"\n",
    "        )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def generar_modelo_val_train(data_domain, espacio, model, mode=\"bow\", iteraciones=30):\n",
    "    \"\"\"\n",
    "    Entrena y valida un modelo con BayesSearchCV para un dominio específico.\n",
    "\n",
    "    Parametetros\n",
    "    ----------\n",
    "    data_domain : dict con estructura {\"X\":..., \"y\":..., \"rest\": (X_unl, y_unl)}\n",
    "    espacio     : dict con los hiperparámetros a optimizar\n",
    "    model       : estimador sklearn (e.g. LogisticRegression, MultinomialNB)\n",
    "    mode        : \"bow\" o \"tfidf\"\n",
    "    iteraciones : número de iteraciones de búsqueda\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, y_train = data_domain[\"X\"], data_domain[\"y\"]\n",
    "    # X_test, y_test = data_domain[\"rest\"]\n",
    "\n",
    "    if mode == \"bow\":\n",
    "        steps = [(\"vect\", DictVectorizer()), (\"model\", model)]\n",
    "    elif mode == \"lexicon\":\n",
    "        steps = [(\"vect\", SenticLexiconFeaturizer(senticnet))]\n",
    "        if isinstance(model, MultinomialNB):\n",
    "            steps.append((\"nonneg\", MinMaxScaler()))  # ← asegura X ≥ 0\n",
    "        steps.append((\"model\", model))\n",
    "\n",
    "    elif mode == \"tfidf\":\n",
    "        steps = [\n",
    "            (\"vect\", DictVectorizer()),\n",
    "            (\"tfidf\", TfidfTransformer()),\n",
    "            (\"model\", model),\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(\"mode debe ser 'bow' o 'tfidf'\")\n",
    "    pipeline = Pipeline(steps)\n",
    "\n",
    "    opt = BayesSearchCV(\n",
    "        estimator=pipeline,\n",
    "        search_spaces=espacio,\n",
    "        n_iter=iteraciones,\n",
    "        cv=10,\n",
    "        scoring=\"f1_macro\",\n",
    "        refit=True,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    opt.fit(X_train, y_train)\n",
    "    print(\"Mejores hiperparámetros:\", opt.best_params_)\n",
    "\n",
    "    # y_pred = opt.predict(X_test)\n",
    "    # print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return opt\n",
    "\n",
    "\n",
    "def entrenar_y_evaluar_por_clase(data, espacios, iteraciones=30):\n",
    "    \"\"\"\n",
    "    Entrena y evalúa 6 modelos por clase:\n",
    "    - Lexicon + NB\n",
    "    - Lexicon + LR\n",
    "    - BoW + NB\n",
    "    - BoW + LR\n",
    "    - TF-IDF + NB\n",
    "    - TF-IDF + LR\n",
    "    \"\"\"\n",
    "    resultados_segunda_parte = {}\n",
    "    mejores_lr_por_clase = {}  # ← añadimos esto\n",
    "    modelos_lr_por_clase = {}\n",
    "\n",
    "    for clase, valores in data.items():\n",
    "        print(f\"\\nEntrenando modelos para la clase: {clase}\")\n",
    "\n",
    "        print(f\"[{clase}] Entrenando LEXICON + NB\")\n",
    "        opt_lex_nb = generar_modelo_val_train(\n",
    "            valores,\n",
    "            espacios[\"lex_nb\"],\n",
    "            MultinomialNB(),\n",
    "            mode=\"lexicon\",\n",
    "            iteraciones=iteraciones,\n",
    "        )\n",
    "        resultados_segunda_parte[f\"{clase}_LEXICON_NB\"] = evaluate(\n",
    "            opt_lex_nb, valores[\"rest\"][0], valores[\"rest\"][1], print_flag=True\n",
    "        )\n",
    "\n",
    "        print(f\"[{clase}] Entrenando LEXICON + LR\")\n",
    "        opt_lex_lr = generar_modelo_val_train(\n",
    "            valores,\n",
    "            espacios[\"lex_lr\"],\n",
    "            LogisticRegression(max_iter=1000),\n",
    "            mode=\"lexicon\",\n",
    "            iteraciones=iteraciones,\n",
    "        )\n",
    "        resultados_segunda_parte[f\"{clase}_LEXICON_LR\"] = evaluate(\n",
    "            opt_lex_lr, valores[\"rest\"][0], valores[\"rest\"][1], print_flag=True\n",
    "        )\n",
    "\n",
    "        print(f\"[{clase}] Entrenando BoW + NB\")\n",
    "        opt_bow_nb = generar_modelo_val_train(\n",
    "            valores,\n",
    "            espacios[\"bow_nb\"],\n",
    "            MultinomialNB(),\n",
    "            mode=\"bow\",\n",
    "            iteraciones=iteraciones,\n",
    "        )\n",
    "        resultados_segunda_parte[f\"{clase}_BoW_NB\"] = evaluate(\n",
    "            opt_bow_nb, valores[\"rest\"][0], valores[\"rest\"][1], print_flag=True\n",
    "        )\n",
    "\n",
    "        print(f\"[{clase}] Entrenando BoW + LR\")\n",
    "        opt_bow_lr = generar_modelo_val_train(\n",
    "            valores,\n",
    "            espacios[\"bow_lr\"],\n",
    "            LogisticRegression(max_iter=1000),\n",
    "            mode=\"bow\",\n",
    "            iteraciones=iteraciones,\n",
    "        )\n",
    "        resultados_segunda_parte[f\"{clase}_BoW_LR\"] = evaluate(\n",
    "            opt_bow_lr, valores[\"rest\"][0], valores[\"rest\"][1], print_flag=True\n",
    "        )\n",
    "\n",
    "        print(f\"[{clase}] Entrenando TF-IDF + NB\")\n",
    "        opt_tfidf_nb = generar_modelo_val_train(\n",
    "            valores,\n",
    "            espacios[\"tfidf_nb\"],\n",
    "            MultinomialNB(),\n",
    "            mode=\"tfidf\",\n",
    "            iteraciones=iteraciones,\n",
    "        )\n",
    "        resultados_segunda_parte[f\"{clase}_TFIDF_NB\"] = evaluate(\n",
    "            opt_tfidf_nb, valores[\"rest\"][0], valores[\"rest\"][1], print_flag=True\n",
    "        )\n",
    "\n",
    "        print(f\"[{clase}] Entrenando TF-IDF + LR\")\n",
    "        opt_tfidf_lr = generar_modelo_val_train(\n",
    "            valores,\n",
    "            espacios[\"tfidf_lr\"],\n",
    "            LogisticRegression(max_iter=1000),\n",
    "            mode=\"tfidf\",\n",
    "            iteraciones=iteraciones,\n",
    "        )\n",
    "        resultados_segunda_parte[f\"{clase}_TFIDF_LR\"] = evaluate(\n",
    "            opt_tfidf_lr, valores[\"rest\"][0], valores[\"rest\"][1], print_flag=True\n",
    "        )\n",
    "\n",
    "        modelos_lr_por_clase[clase] = {\n",
    "            \"lexicon\": opt_lex_lr,\n",
    "            \"bow\": opt_bow_lr,\n",
    "            \"tfidf\": opt_tfidf_lr,\n",
    "        }\n",
    "        mejor_lr = max(\n",
    "            [opt_lex_lr, opt_bow_lr, opt_tfidf_lr], key=lambda o: o.best_score_\n",
    "        )\n",
    "        mejores_lr_por_clase[clase] = mejor_lr\n",
    "\n",
    "    return resultados_segunda_parte, mejores_lr_por_clase\n",
    "\n",
    "\n",
    "espacios = {\n",
    "    \"bow_nb\": {\n",
    "        \"model__alpha\": (1e-3, 1.0, \"log-uniform\"),\n",
    "        \"model__fit_prior\": [True, False],\n",
    "    },\n",
    "    \"bow_lr\": {\n",
    "        \"model__C\": (1e-3, 1e2, \"log-uniform\"),\n",
    "        \"model__solver\": [\"liblinear\", \"lbfgs\", \"saga\"],\n",
    "        \"model__penalty\": [\n",
    "            \"l2\",\n",
    "            None,\n",
    "        ],\n",
    "        \"model__class_weight\": [None, \"balanced\"],\n",
    "        \"model__max_iter\": (50, 150),\n",
    "    },\n",
    "    \"tfidf_nb\": {\n",
    "        \"model__alpha\": (1e-3, 1.0, \"log-uniform\"),\n",
    "        \"model__fit_prior\": [True, False],\n",
    "    },\n",
    "    \"tfidf_lr\": {\n",
    "        \"model__C\": (1e-3, 1e2, \"log-uniform\"),\n",
    "        \"model__solver\": [\"liblinear\", \"lbfgs\", \"saga\"],\n",
    "        \"model__penalty\": [\"l2\", None],\n",
    "        \"model__class_weight\": [None, \"balanced\"],\n",
    "        \"model__max_iter\": (50, 150),\n",
    "    },\n",
    "    \"lex_nb\": {\n",
    "        \"model__alpha\": (1e-3, 1.0, \"log-uniform\"),\n",
    "        \"model__fit_prior\": [True, False],\n",
    "    },\n",
    "    \"lex_lr\": {\n",
    "        \"model__C\": (1e-3, 1e2, \"log-uniform\"),\n",
    "        \"model__solver\": [\"liblinear\", \"lbfgs\", \"saga\"],\n",
    "        \"model__penalty\": [\"l2\", None],\n",
    "        \"model__class_weight\": [None, \"balanced\"],\n",
    "        \"model__max_iter\": (50, 150),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfba293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[electronics] Positive: 1000, Negative: 1000, Rest(unlabeled): 5681\n",
      "[kitchen] Positive: 1000, Negative: 1000, Rest(unlabeled): 5945\n",
      "[dvd] Positive: 1000, Negative: 1000, Rest(unlabeled): 3586\n",
      "[books] Positive: 1000, Negative: 1000, Rest(unlabeled): 4465\n",
      "\n",
      "Entrenando modelos para la clase: electronics\n",
      "[electronics] Entrenando LEXICON + NB\n",
      "Mejores hiperparámetros: OrderedDict([('model__alpha', 0.016994636371262764), ('model__fit_prior', False)])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.64      0.65      2824\n",
      "    positive       0.65      0.66      0.66      2857\n",
      "\n",
      "    accuracy                           0.65      5681\n",
      "   macro avg       0.65      0.65      0.65      5681\n",
      "weighted avg       0.65      0.65      0.65      5681\n",
      "\n",
      "[electronics] Entrenando LEXICON + LR\n",
      "Mejores hiperparámetros: OrderedDict([('model__C', 11.533999859559563), ('model__class_weight', None), ('model__max_iter', 110), ('model__penalty', None), ('model__solver', 'lbfgs')])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.70      0.69      2824\n",
      "    positive       0.69      0.68      0.68      2857\n",
      "\n",
      "    accuracy                           0.69      5681\n",
      "   macro avg       0.69      0.69      0.69      5681\n",
      "weighted avg       0.69      0.69      0.69      5681\n",
      "\n",
      "[electronics] Entrenando BoW + NB\n",
      "Mejores hiperparámetros: OrderedDict([('model__alpha', 0.3252108800594495), ('model__fit_prior', False)])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85      2824\n",
      "    positive       0.85      0.85      0.85      2857\n",
      "\n",
      "    accuracy                           0.85      5681\n",
      "   macro avg       0.85      0.85      0.85      5681\n",
      "weighted avg       0.85      0.85      0.85      5681\n",
      "\n",
      "[electronics] Entrenando BoW + LR\n",
      "Mejores hiperparámetros: OrderedDict([('model__C', 11.533999859559563), ('model__class_weight', None), ('model__max_iter', 110), ('model__penalty', None), ('model__solver', 'lbfgs')])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.84      0.86      2824\n",
      "    positive       0.84      0.88      0.86      2857\n",
      "\n",
      "    accuracy                           0.86      5681\n",
      "   macro avg       0.86      0.86      0.86      5681\n",
      "weighted avg       0.86      0.86      0.86      5681\n",
      "\n",
      "[electronics] Entrenando TF-IDF + NB\n",
      "Mejores hiperparámetros: OrderedDict([('model__alpha', 0.2736452822078245), ('model__fit_prior', True)])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.89      0.87      2824\n",
      "    positive       0.88      0.86      0.87      2857\n",
      "\n",
      "    accuracy                           0.87      5681\n",
      "   macro avg       0.87      0.87      0.87      5681\n",
      "weighted avg       0.87      0.87      0.87      5681\n",
      "\n",
      "[electronics] Entrenando TF-IDF + LR\n",
      "Mejores hiperparámetros: OrderedDict([('model__C', 15.37948446580078), ('model__class_weight', 'balanced'), ('model__max_iter', 80), ('model__penalty', None), ('model__solver', 'saga')])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.87      0.88      2824\n",
      "    positive       0.87      0.89      0.88      2857\n",
      "\n",
      "    accuracy                           0.88      5681\n",
      "   macro avg       0.88      0.88      0.88      5681\n",
      "weighted avg       0.88      0.88      0.88      5681\n",
      "\n",
      "\n",
      "Entrenando modelos para la clase: kitchen\n",
      "[kitchen] Entrenando LEXICON + NB\n",
      "Mejores hiperparámetros: OrderedDict([('model__alpha', 0.016994636371262764), ('model__fit_prior', False)])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.65      0.66      2991\n",
      "    positive       0.65      0.67      0.66      2954\n",
      "\n",
      "    accuracy                           0.66      5945\n",
      "   macro avg       0.66      0.66      0.66      5945\n",
      "weighted avg       0.66      0.66      0.66      5945\n",
      "\n",
      "[kitchen] Entrenando LEXICON + LR\n",
      "Mejores hiperparámetros: OrderedDict([('model__C', 11.533999859559563), ('model__class_weight', None), ('model__max_iter', 110), ('model__penalty', None), ('model__solver', 'lbfgs')])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.68      0.69      2991\n",
      "    positive       0.68      0.69      0.69      2954\n",
      "\n",
      "    accuracy                           0.69      5945\n",
      "   macro avg       0.69      0.69      0.69      5945\n",
      "weighted avg       0.69      0.69      0.69      5945\n",
      "\n",
      "[kitchen] Entrenando BoW + NB\n",
      "Mejores hiperparámetros: OrderedDict([('model__alpha', 0.25041499136197737), ('model__fit_prior', True)])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.88      0.88      2991\n",
      "    positive       0.88      0.86      0.87      2954\n",
      "\n",
      "    accuracy                           0.88      5945\n",
      "   macro avg       0.88      0.87      0.87      5945\n",
      "weighted avg       0.88      0.88      0.88      5945\n",
      "\n",
      "[kitchen] Entrenando BoW + LR\n",
      "Mejores hiperparámetros: OrderedDict([('model__C', 0.1675569944093659), ('model__class_weight', 'balanced'), ('model__max_iter', 60), ('model__penalty', 'l2'), ('model__solver', 'liblinear')])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88      2991\n",
      "    positive       0.88      0.87      0.88      2954\n",
      "\n",
      "    accuracy                           0.88      5945\n",
      "   macro avg       0.88      0.88      0.88      5945\n",
      "weighted avg       0.88      0.88      0.88      5945\n",
      "\n",
      "[kitchen] Entrenando TF-IDF + NB\n",
      "Mejores hiperparámetros: OrderedDict([('model__alpha', 0.3252108800594495), ('model__fit_prior', False)])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.90      0.88      2991\n",
      "    positive       0.89      0.87      0.88      2954\n",
      "\n",
      "    accuracy                           0.88      5945\n",
      "   macro avg       0.88      0.88      0.88      5945\n",
      "weighted avg       0.88      0.88      0.88      5945\n",
      "\n",
      "[kitchen] Entrenando TF-IDF + LR\n",
      "Mejores hiperparámetros: OrderedDict([('model__C', 15.37948446580078), ('model__class_weight', 'balanced'), ('model__max_iter', 80), ('model__penalty', None), ('model__solver', 'saga')])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.89      0.89      2991\n",
      "    positive       0.89      0.89      0.89      2954\n",
      "\n",
      "    accuracy                           0.89      5945\n",
      "   macro avg       0.89      0.89      0.89      5945\n",
      "weighted avg       0.89      0.89      0.89      5945\n",
      "\n",
      "\n",
      "Entrenando modelos para la clase: dvd\n",
      "[dvd] Entrenando LEXICON + NB\n",
      "Mejores hiperparámetros: OrderedDict([('model__alpha', 0.3252108800594495), ('model__fit_prior', False)])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.66      0.65      1779\n",
      "    positive       0.65      0.63      0.64      1807\n",
      "\n",
      "    accuracy                           0.65      3586\n",
      "   macro avg       0.65      0.65      0.64      3586\n",
      "weighted avg       0.65      0.65      0.64      3586\n",
      "\n",
      "[dvd] Entrenando LEXICON + LR\n",
      "Mejores hiperparámetros: OrderedDict([('model__C', 11.533999859559563), ('model__class_weight', None), ('model__max_iter', 110), ('model__penalty', None), ('model__solver', 'lbfgs')])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.66      0.66      0.66      1779\n",
      "    positive       0.67      0.67      0.67      1807\n",
      "\n",
      "    accuracy                           0.66      3586\n",
      "   macro avg       0.66      0.66      0.66      3586\n",
      "weighted avg       0.66      0.66      0.66      3586\n",
      "\n",
      "[dvd] Entrenando BoW + NB\n",
      "Mejores hiperparámetros: OrderedDict([('model__alpha', 0.3252108800594495), ('model__fit_prior', False)])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.84      0.82      1779\n",
      "    positive       0.83      0.79      0.81      1807\n",
      "\n",
      "    accuracy                           0.81      3586\n",
      "   macro avg       0.82      0.81      0.81      3586\n",
      "weighted avg       0.82      0.81      0.81      3586\n",
      "\n",
      "[dvd] Entrenando BoW + LR\n",
      "Mejores hiperparámetros: OrderedDict([('model__C', 0.1675569944093659), ('model__class_weight', 'balanced'), ('model__max_iter', 60), ('model__penalty', 'l2'), ('model__solver', 'liblinear')])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.81      0.82      1779\n",
      "    positive       0.82      0.84      0.83      1807\n",
      "\n",
      "    accuracy                           0.83      3586\n",
      "   macro avg       0.83      0.83      0.83      3586\n",
      "weighted avg       0.83      0.83      0.83      3586\n",
      "\n",
      "[dvd] Entrenando TF-IDF + NB\n",
      "Mejores hiperparámetros: OrderedDict([('model__alpha', 0.3252108800594495), ('model__fit_prior', False)])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.86      0.84      1779\n",
      "    positive       0.85      0.83      0.84      1807\n",
      "\n",
      "    accuracy                           0.84      3586\n",
      "   macro avg       0.84      0.84      0.84      3586\n",
      "weighted avg       0.84      0.84      0.84      3586\n",
      "\n",
      "[dvd] Entrenando TF-IDF + LR\n",
      "Mejores hiperparámetros: OrderedDict([('model__C', 9.948719998234102), ('model__class_weight', None), ('model__max_iter', 103), ('model__penalty', None), ('model__solver', 'saga')])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.83      0.85      1779\n",
      "    positive       0.84      0.87      0.85      1807\n",
      "\n",
      "    accuracy                           0.85      3586\n",
      "   macro avg       0.85      0.85      0.85      3586\n",
      "weighted avg       0.85      0.85      0.85      3586\n",
      "\n",
      "\n",
      "Entrenando modelos para la clase: books\n",
      "[books] Entrenando LEXICON + NB\n",
      "Mejores hiperparámetros: OrderedDict([('model__alpha', 0.016994636371262764), ('model__fit_prior', False)])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.63      0.62      0.63      2201\n",
      "    positive       0.64      0.64      0.64      2264\n",
      "\n",
      "    accuracy                           0.63      4465\n",
      "   macro avg       0.63      0.63      0.63      4465\n",
      "weighted avg       0.63      0.63      0.63      4465\n",
      "\n",
      "[books] Entrenando LEXICON + LR\n",
      "Mejores hiperparámetros: OrderedDict([('model__C', 11.533999859559563), ('model__class_weight', None), ('model__max_iter', 110), ('model__penalty', None), ('model__solver', 'lbfgs')])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.63      0.63      0.63      2201\n",
      "    positive       0.64      0.64      0.64      2264\n",
      "\n",
      "    accuracy                           0.64      4465\n",
      "   macro avg       0.64      0.64      0.64      4465\n",
      "weighted avg       0.64      0.64      0.64      4465\n",
      "\n",
      "[books] Entrenando BoW + NB\n",
      "Mejores hiperparámetros: OrderedDict([('model__alpha', 0.3252108800594495), ('model__fit_prior', False)])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.87      0.84      2201\n",
      "    positive       0.86      0.80      0.83      2264\n",
      "\n",
      "    accuracy                           0.83      4465\n",
      "   macro avg       0.83      0.83      0.83      4465\n",
      "weighted avg       0.83      0.83      0.83      4465\n",
      "\n",
      "[books] Entrenando BoW + LR\n",
      "Mejores hiperparámetros: OrderedDict([('model__C', 0.1675569944093659), ('model__class_weight', 'balanced'), ('model__max_iter', 60), ('model__penalty', 'l2'), ('model__solver', 'liblinear')])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.84      0.83      2201\n",
      "    positive       0.84      0.83      0.83      2264\n",
      "\n",
      "    accuracy                           0.83      4465\n",
      "   macro avg       0.83      0.83      0.83      4465\n",
      "weighted avg       0.83      0.83      0.83      4465\n",
      "\n",
      "[books] Entrenando TF-IDF + NB\n",
      "Mejores hiperparámetros: OrderedDict([('model__alpha', 0.3252108800594495), ('model__fit_prior', False)])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.90      0.85      2201\n",
      "    positive       0.89      0.79      0.84      2264\n",
      "\n",
      "    accuracy                           0.85      4465\n",
      "   macro avg       0.85      0.85      0.85      4465\n",
      "weighted avg       0.85      0.85      0.85      4465\n",
      "\n",
      "[books] Entrenando TF-IDF + LR\n",
      "Mejores hiperparámetros: OrderedDict([('model__C', 15.37948446580078), ('model__class_weight', 'balanced'), ('model__max_iter', 80), ('model__penalty', None), ('model__solver', 'saga')])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.86      0.86      2201\n",
      "    positive       0.87      0.85      0.86      2264\n",
      "\n",
      "    accuracy                           0.86      4465\n",
      "   macro avg       0.86      0.86      0.86      4465\n",
      "weighted avg       0.86      0.86      0.86      4465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/Multi Domain Sentiment/processed_acl\"\n",
    "dataset = build_train_test(path)\n",
    "resultados_segunda_parte, mejores_lr_por_clase = entrenar_y_evaluar_por_clase(\n",
    "    dataset, espacios, iteraciones=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68e65c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Modelo                 |   precision_macro |   recall_macro |   f1_macro |   precision_micro |   recall_micro |   f1_micro |   accuracy | best_params                                                                                                                                                             |\n",
      "+========================+===================+================+============+===================+================+============+============+=========================================================================================================================================================================+\n",
      "| kitchen_TFIDF_LR       |            0.891  |         0.891  |     0.891  |            0.891  |         0.891  |     0.891  |     0.891  | OrderedDict([('model__C', 15.37948446580078), ('model__class_weight', 'balanced'), ('model__max_iter', 80), ('model__penalty', None), ('model__solver', 'saga')])       |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| kitchen_TFIDF_NB       |            0.8822 |         0.8818 |     0.8819 |            0.8819 |         0.8819 |     0.8819 |     0.8819 | OrderedDict([('model__alpha', 0.3252108800594495), ('model__fit_prior', False)])                                                                                        |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| kitchen_BoW_LR         |            0.8781 |         0.878  |     0.878  |            0.878  |         0.878  |     0.878  |     0.878  | OrderedDict([('model__C', 0.1675569944093659), ('model__class_weight', 'balanced'), ('model__max_iter', 60), ('model__penalty', 'l2'), ('model__solver', 'liblinear')]) |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| electronics_TFIDF_LR   |            0.8773 |         0.8771 |     0.8771 |            0.8771 |         0.8771 |     0.8771 |     0.8771 | OrderedDict([('model__C', 15.37948446580078), ('model__class_weight', 'balanced'), ('model__max_iter', 80), ('model__penalty', None), ('model__solver', 'saga')])       |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| kitchen_BoW_NB         |            0.8752 |         0.875  |     0.875  |            0.875  |         0.875  |     0.875  |     0.875  | OrderedDict([('model__alpha', 0.25041499136197737), ('model__fit_prior', True)])                                                                                        |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| electronics_TFIDF_NB   |            0.8739 |         0.8737 |     0.8736 |            0.8736 |         0.8736 |     0.8736 |     0.8736 | OrderedDict([('model__alpha', 0.2736452822078245), ('model__fit_prior', True)])                                                                                         |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| electronics_BoW_LR     |            0.8607 |         0.8597 |     0.8598 |            0.8599 |         0.8599 |     0.8599 |     0.8599 | OrderedDict([('model__C', 11.533999859559563), ('model__class_weight', None), ('model__max_iter', 110), ('model__penalty', None), ('model__solver', 'lbfgs')])          |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| books_TFIDF_LR         |            0.8565 |         0.8565 |     0.8564 |            0.8564 |         0.8564 |     0.8564 |     0.8564 | OrderedDict([('model__C', 15.37948446580078), ('model__class_weight', 'balanced'), ('model__max_iter', 80), ('model__penalty', None), ('model__solver', 'saga')])       |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| electronics_BoW_NB     |            0.8516 |         0.8516 |     0.8516 |            0.8516 |         0.8516 |     0.8516 |     0.8516 | OrderedDict([('model__alpha', 0.3252108800594495), ('model__fit_prior', False)])                                                                                        |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| dvd_TFIDF_LR           |            0.8504 |         0.8498 |     0.8499 |            0.85   |         0.85   |     0.85   |     0.85   | OrderedDict([('model__C', 9.948719998234102), ('model__class_weight', None), ('model__max_iter', 103), ('model__penalty', None), ('model__solver', 'saga')])            |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| books_TFIDF_NB         |            0.8502 |         0.8469 |     0.8459 |            0.8461 |         0.8461 |     0.8461 |     0.8461 | OrderedDict([('model__alpha', 0.3252108800594495), ('model__fit_prior', False)])                                                                                        |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| dvd_TFIDF_NB           |            0.8435 |         0.8434 |     0.8433 |            0.8433 |         0.8433 |     0.8433 |     0.8433 | OrderedDict([('model__alpha', 0.3252108800594495), ('model__fit_prior', False)])                                                                                        |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| books_BoW_NB           |            0.8338 |         0.8327 |     0.8322 |            0.8323 |         0.8323 |     0.8323 |     0.8323 | OrderedDict([('model__alpha', 0.3252108800594495), ('model__fit_prior', False)])                                                                                        |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| books_BoW_LR           |            0.8318 |         0.8319 |     0.8318 |            0.8318 |         0.8318 |     0.8318 |     0.8318 | OrderedDict([('model__C', 0.1675569944093659), ('model__class_weight', 'balanced'), ('model__max_iter', 60), ('model__penalty', 'l2'), ('model__solver', 'liblinear')]) |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| dvd_BoW_LR             |            0.8259 |         0.8256 |     0.8256 |            0.8257 |         0.8257 |     0.8257 |     0.8257 | OrderedDict([('model__C', 0.1675569944093659), ('model__class_weight', 'balanced'), ('model__max_iter', 60), ('model__penalty', 'l2'), ('model__solver', 'liblinear')]) |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| dvd_BoW_NB             |            0.8154 |         0.815  |     0.8148 |            0.8148 |         0.8148 |     0.8148 |     0.8148 | OrderedDict([('model__alpha', 0.3252108800594495), ('model__fit_prior', False)])                                                                                        |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| kitchen_LEXICON_LR     |            0.6887 |         0.6887 |     0.6886 |            0.6886 |         0.6886 |     0.6886 |     0.6886 | OrderedDict([('model__C', 11.533999859559563), ('model__class_weight', None), ('model__max_iter', 110), ('model__penalty', None), ('model__solver', 'lbfgs')])          |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| electronics_LEXICON_LR |            0.6872 |         0.6871 |     0.687  |            0.687  |         0.687  |     0.687  |     0.687  | OrderedDict([('model__C', 11.533999859559563), ('model__class_weight', None), ('model__max_iter', 110), ('model__penalty', None), ('model__solver', 'lbfgs')])          |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| dvd_LEXICON_LR         |            0.6639 |         0.6639 |     0.6639 |            0.664  |         0.664  |     0.664  |     0.664  | OrderedDict([('model__C', 11.533999859559563), ('model__class_weight', None), ('model__max_iter', 110), ('model__penalty', None), ('model__solver', 'lbfgs')])          |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| kitchen_LEXICON_NB     |            0.6597 |         0.6596 |     0.6595 |            0.6595 |         0.6595 |     0.6595 |     0.6595 | OrderedDict([('model__alpha', 0.016994636371262764), ('model__fit_prior', False)])                                                                                      |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| electronics_LEXICON_NB |            0.6531 |         0.653  |     0.653  |            0.6531 |         0.6531 |     0.6531 |     0.6531 | OrderedDict([('model__alpha', 0.016994636371262764), ('model__fit_prior', False)])                                                                                      |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| dvd_LEXICON_NB         |            0.6452 |         0.6451 |     0.645  |            0.645  |         0.645  |     0.645  |     0.645  | OrderedDict([('model__alpha', 0.3252108800594495), ('model__fit_prior', False)])                                                                                        |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| books_LEXICON_LR       |            0.636  |         0.636  |     0.636  |            0.6361 |         0.6361 |     0.6361 |     0.6361 | OrderedDict([('model__C', 11.533999859559563), ('model__class_weight', None), ('model__max_iter', 110), ('model__penalty', None), ('model__solver', 'lbfgs')])          |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| books_LEXICON_NB       |            0.6328 |         0.6328 |     0.6328 |            0.6329 |         0.6329 |     0.6329 |     0.6329 | OrderedDict([('model__alpha', 0.016994636371262764), ('model__fit_prior', False)])                                                                                      |\n",
      "+------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "mostrar_resultados_tabulate(resultados_segunda_parte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ba7af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Clase :electronics\n",
      "\n",
      "Atributos mas positivos\n",
      "el token great con un peso de 0.86\n",
      "el token excellent con un peso de 0.66\n",
      "el token price con un peso de 0.56\n",
      "el token perfect con un peso de 0.53\n",
      "el token best con un peso de 0.45\n",
      "\n",
      "Atributos mas negativos\n",
      "el token not con un peso de -0.57\n",
      "el token poor con un peso de -0.49\n",
      "el token bad con un peso de -0.43\n",
      "el token work con un peso de -0.36\n",
      "el token back con un peso de -0.36\n",
      "\n",
      " Clase :kitchen\n",
      "\n",
      "Atributos mas positivos\n",
      "el token great con un peso de 0.88\n",
      "el token easy con un peso de 0.71\n",
      "el token love con un peso de 0.61\n",
      "el token best con un peso de 0.55\n",
      "el token excellent con un peso de 0.54\n",
      "\n",
      "Atributos mas negativos\n",
      "el token not con un peso de -0.66\n",
      "el token disappointed con un peso de -0.58\n",
      "el token poor con un peso de -0.43\n",
      "el token too con un peso de -0.39\n",
      "el token return con un peso de -0.37\n",
      "\n",
      " Clase :dvd\n",
      "\n",
      "Atributos mas positivos\n",
      "el token great con un peso de 0.58\n",
      "el token best con un peso de 0.47\n",
      "el token excellent con un peso de 0.39\n",
      "el token love con un peso de 0.37\n",
      "el token well con un peso de 0.32\n",
      "\n",
      "Atributos mas negativos\n",
      "el token bad con un peso de -0.53\n",
      "el token worst con un peso de -0.45\n",
      "el token boring con un peso de -0.42\n",
      "el token not con un peso de -0.39\n",
      "el token no con un peso de -0.35\n",
      "\n",
      " Clase :books\n",
      "\n",
      "Atributos mas positivos\n",
      "el token great con un peso de 0.37\n",
      "el token excellent con un peso de 0.35\n",
      "el token easy con un peso de 0.23\n",
      "el token best con un peso de 0.22\n",
      "el token my con un peso de 0.22\n",
      "\n",
      "Atributos mas negativos\n",
      "el token no con un peso de -0.39\n",
      "el token bad con un peso de -0.32\n",
      "el token not con un peso de -0.28\n",
      "el token don't con un peso de -0.26\n",
      "el token if con un peso de -0.25\n"
     ]
    }
   ],
   "source": [
    "for key in mejores_lr_por_clase.keys():\n",
    "    print(f\"\\n Clase :{key}\")\n",
    "    pos, neg = top_features_pos_neg(mejores_lr_por_clase[key], k=5)\n",
    "    print(\"\\nAtributos mas positivos\")\n",
    "    for key, value in pos:\n",
    "        print(f\"el token {key} con un peso de {value:.2f}\")\n",
    "    print(\"\\nAtributos mas negativos\")\n",
    "    for key, value in neg:\n",
    "        print(f\"el token {key} con un peso de {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b3b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_dataset_global(data):\n",
    "    \"\"\"\n",
    "    Une todos los subconjuntos de categorías en un dataset único (train+test).\n",
    "    \"\"\"\n",
    "    X_train, y_train, X_test, y_test = [], [], [], []\n",
    "\n",
    "    for data_domain in data.values():\n",
    "        X_train.extend(data_domain[\"X\"])\n",
    "        y_train.extend(data_domain[\"y\"])\n",
    "        X_test.extend(data_domain[\"rest\"][0])\n",
    "        y_test.extend(data_domain[\"rest\"][1])\n",
    "\n",
    "    return {\"X\": X_train, \"y\": y_train, \"rest\": (X_test, y_test)}\n",
    "\n",
    "\n",
    "def entrenar_y_evaluar_global(data_domain, espacios, iteraciones=30):\n",
    "    resultados = {}\n",
    "\n",
    "    print(f\"LEXICON + NB\")\n",
    "    opt_lex_nb = generar_modelo_val_train(\n",
    "        data_domain,\n",
    "        espacios[\"lex_nb\"],\n",
    "        MultinomialNB(),\n",
    "        mode=\"lexicon\",\n",
    "        iteraciones=iteraciones,\n",
    "    )\n",
    "    resultados[\"LEXICON_NB\"] = evaluate(\n",
    "        opt_lex_nb, data_domain[\"rest\"][0], data_domain[\"rest\"][1], print_flag=True\n",
    "    )\n",
    "\n",
    "    print(\"LEXICON + LR\")\n",
    "    opt_lex_lr = generar_modelo_val_train(\n",
    "        data_domain,\n",
    "        espacios[\"lex_lr\"],\n",
    "        LogisticRegression(),\n",
    "        mode=\"lexicon\",\n",
    "        iteraciones=iteraciones,\n",
    "    )\n",
    "    resultados[\"LEXICON_LR\"] = evaluate(\n",
    "        opt_lex_lr, data_domain[\"rest\"][0], data_domain[\"rest\"][1], print_flag=True\n",
    "    )\n",
    "    print(\"BoW_NB\")\n",
    "    opt_bow_nb = generar_modelo_val_train(\n",
    "        data_domain,\n",
    "        espacios[\"bow_nb\"],\n",
    "        MultinomialNB(),\n",
    "        mode=\"bow\",\n",
    "        iteraciones=iteraciones,\n",
    "    )\n",
    "    resultados[\"BoW_NB\"] = evaluate(opt_bow_nb, *data_domain[\"rest\"], print_flag=True)\n",
    "    print(\"BoW_LR\")\n",
    "    opt_bow_lr = generar_modelo_val_train(\n",
    "        data_domain,\n",
    "        espacios[\"bow_lr\"],\n",
    "        LogisticRegression(),\n",
    "        mode=\"bow\",\n",
    "        iteraciones=iteraciones,\n",
    "    )\n",
    "    resultados[\"BoW_LR\"] = evaluate(opt_bow_lr, *data_domain[\"rest\"], print_flag=True)\n",
    "    print(\"TFIDF_NB\")\n",
    "    opt_tfidf_nb = generar_modelo_val_train(\n",
    "        data_domain,\n",
    "        espacios[\"tfidf_nb\"],\n",
    "        MultinomialNB(),\n",
    "        mode=\"tfidf\",\n",
    "        iteraciones=iteraciones,\n",
    "    )\n",
    "    resultados[\"TFIDF_NB\"] = evaluate(\n",
    "        opt_tfidf_nb, *data_domain[\"rest\"], print_flag=True\n",
    "    )\n",
    "    print(\"TFIDF_LR\")\n",
    "    opt_tfidf_lr = generar_modelo_val_train(\n",
    "        data_domain,\n",
    "        espacios[\"tfidf_lr\"],\n",
    "        LogisticRegression(),\n",
    "        mode=\"tfidf\",\n",
    "        iteraciones=iteraciones,\n",
    "    )\n",
    "\n",
    "    mejor_lr = max([opt_lex_lr, opt_bow_lr, opt_tfidf_lr], key=lambda o: o.best_score_)\n",
    "\n",
    "    return resultados, mejor_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce1b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[electronics] Positive: 1000, Negative: 1000, Rest(unlabeled): 5681\n",
      "[kitchen] Positive: 1000, Negative: 1000, Rest(unlabeled): 5945\n",
      "[dvd] Positive: 1000, Negative: 1000, Rest(unlabeled): 3586\n",
      "[books] Positive: 1000, Negative: 1000, Rest(unlabeled): 4465\n",
      "LEXICON + NB\n",
      "Mejores hiperparámetros: OrderedDict([('model__alpha', 0.3252108800594495), ('model__fit_prior', False)])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.64      0.64      9795\n",
      "    positive       0.64      0.65      0.65      9882\n",
      "\n",
      "    accuracy                           0.64     19677\n",
      "   macro avg       0.64      0.64      0.64     19677\n",
      "weighted avg       0.64      0.64      0.64     19677\n",
      "\n",
      "LEXICON + LR\n",
      "Mejores hiperparámetros: OrderedDict([('model__C', 11.533999859559563), ('model__class_weight', None), ('model__max_iter', 110), ('model__penalty', None), ('model__solver', 'lbfgs')])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.66      0.66      0.66      9795\n",
      "    positive       0.67      0.66      0.66      9882\n",
      "\n",
      "    accuracy                           0.66     19677\n",
      "   macro avg       0.66      0.66      0.66     19677\n",
      "weighted avg       0.66      0.66      0.66     19677\n",
      "\n",
      "BoW_NB\n",
      "Mejores hiperparámetros: OrderedDict([('model__alpha', 0.3252108800594495), ('model__fit_prior', False)])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.88      0.86      9795\n",
      "    positive       0.88      0.83      0.85      9882\n",
      "\n",
      "    accuracy                           0.85     19677\n",
      "   macro avg       0.86      0.85      0.85     19677\n",
      "weighted avg       0.86      0.85      0.85     19677\n",
      "\n",
      "BoW_LR\n",
      "Mejores hiperparámetros: OrderedDict([('model__C', 0.1675569944093659), ('model__class_weight', 'balanced'), ('model__max_iter', 60), ('model__penalty', 'l2'), ('model__solver', 'liblinear')])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87      9795\n",
      "    positive       0.87      0.87      0.87      9882\n",
      "\n",
      "    accuracy                           0.87     19677\n",
      "   macro avg       0.87      0.87      0.87     19677\n",
      "weighted avg       0.87      0.87      0.87     19677\n",
      "\n",
      "TFIDF_NB\n",
      "Mejores hiperparámetros: OrderedDict([('model__alpha', 0.3252108800594495), ('model__fit_prior', False)])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.91      0.88      9795\n",
      "    positive       0.90      0.84      0.87      9882\n",
      "\n",
      "    accuracy                           0.88     19677\n",
      "   macro avg       0.88      0.88      0.88     19677\n",
      "weighted avg       0.88      0.88      0.88     19677\n",
      "\n",
      "TFIDF_LR\n",
      "Mejores hiperparámetros: OrderedDict([('model__C', 15.37948446580078), ('model__class_weight', 'balanced'), ('model__max_iter', 80), ('model__penalty', None), ('model__solver', 'saga')])\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/Multi Domain Sentiment/processed_acl\"\n",
    "dataset = build_train_test(path)\n",
    "dataset_global = preparar_dataset_global(dataset)\n",
    "\n",
    "resultados_globales, mejor_lr = entrenar_y_evaluar_global(\n",
    "    dataset_global, espacios, iteraciones=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c8802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------------------------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+\n",
      "| Modelo   | best_params                                           |   precision_macro |   recall_macro |   f1_macro |   precision_micro |   recall_micro |   f1_micro |   accuracy |\n",
      "+==========+=======================================================+===================+================+============+===================+================+============+============+\n",
      "| BoW_LR   | OrderedDict([('model__C', 0.11233621690895233)])      |            0.8734 |         0.8734 |     0.8734 |            0.8734 |         0.8734 |     0.8734 |     0.8734 |\n",
      "+----------+-------------------------------------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+\n",
      "| TFIDF_NB | OrderedDict([('model__alpha', 0.016994636371262764)]) |            0.8518 |         0.8518 |     0.8518 |            0.8518 |         0.8518 |     0.8518 |     0.8518 |\n",
      "+----------+-------------------------------------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+\n",
      "| TFIDF_LR | OrderedDict([('model__C', 0.11233621690895233)])      |            0.8415 |         0.8407 |     0.8405 |            0.8406 |         0.8406 |     0.8406 |     0.8406 |\n",
      "+----------+-------------------------------------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+\n",
      "| BoW_NB   | OrderedDict([('model__alpha', 0.016994636371262764)]) |            0.8302 |         0.83   |     0.8299 |            0.83   |         0.83   |     0.83   |     0.83   |\n",
      "+----------+-------------------------------------------------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "mostrar_resultados_tabulate(resultados_globales, ordenar_por=\"f1_macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0a82a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Mejores atributos\n",
      "\n",
      "Atributos mas positivos\n",
      "el token great con un peso de 30.32\n",
      "el token excellent con un peso de 24.99\n",
      "el token best con un peso de 20.01\n",
      "el token perfect con un peso de 17.30\n",
      "el token easy con un peso de 16.40\n",
      "\n",
      "Atributos mas negativos\n",
      "el token not con un peso de -27.21\n",
      "el token bad con un peso de -20.67\n",
      "el token disappointed con un peso de -18.68\n",
      "el token poor con un peso de -18.13\n",
      "el token disappointing con un peso de -16.92\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Mejores atributos\")\n",
    "pos, neg = top_features_pos_neg(mejor_lr, k=5)\n",
    "print(\"\\nAtributos mas positivos\")\n",
    "for key, value in pos:\n",
    "    print(f\"el token {key} con un peso de {value:.2f}\")\n",
    "print(\"\\nAtributos mas negativos\")\n",
    "for key, value in neg:\n",
    "    print(f\"el token {key} con un peso de {value:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
