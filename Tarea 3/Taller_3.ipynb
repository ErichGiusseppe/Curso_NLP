{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ddfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from charset_normalizer import from_path\n",
    "from gensim.corpora import Dictionary\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "Se importan las librerias que se necesiten, \n",
    "si se quiere ejecutar el notebook, se recomienda crear la carpeta de data, y poner ahi los files como se describe\n",
    "\n",
    "\"\"\"\n",
    "ACTUAL_PATH = os.getcwd()\n",
    "# Donde esta el 20 News\n",
    "PATH_20N = os.path.join(ACTUAL_PATH, \"data/20news-18828\")\n",
    "# Donde se encuentra el BAC\n",
    "PATH_BAC = os.path.join(ACTUAL_PATH, \"data/BAC/blogs\")\n",
    "# Donde se van a guardar los files que se van obteniendo\n",
    "PATH_FINAL_FILES = os.path.join(ACTUAL_PATH, \"data/final_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d022f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4a711",
   "metadata": {},
   "source": [
    "### Upload 20N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15256f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Para esto se define en que formato y donde se quiere el archivo completo de 20N\"\"\"\n",
    "\n",
    "NEW_20N_FILE = os.path.join(PATH_FINAL_FILES, \"20N.jsonl\")\n",
    "\n",
    "mayor_folders_20N = os.listdir(PATH_20N)\n",
    "dictionary = {}\n",
    "\"\"\"\n",
    "Para cada archivo disponible en 20N,  \n",
    "se generan registros con:  \n",
    "- el ID del archivo  \n",
    "- el tema del archivo  \n",
    "- el texto del contenido  \n",
    "\n",
    "Todo se guarda en formato JSONL por cuestiones de formato.\n",
    "\"\"\"\n",
    "with open(NEW_20N_FILE, \"w\", encoding=\"utf-8\", errors=\"replace\") as f_n:\n",
    "    for folder in mayor_folders_20N:\n",
    "        minor_files_path = os.path.join(PATH_20N, folder)\n",
    "        minor_files = os.listdir(minor_files_path)\n",
    "        for file in minor_files:\n",
    "            file_path = os.path.join(minor_files_path, file)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "                text = f.read().strip()\n",
    "\n",
    "            record = {\"id\": file, \"theme\": folder, \"text\": text}\n",
    "            unit = folder + file\n",
    "            if file in dictionary.keys():\n",
    "                dictionary[unit] += 1\n",
    "            else:\n",
    "                dictionary[unit] = 1\n",
    "            f_n.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad81e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"Limpia y preprocesa texto: elimina correos, URLs,\n",
    "    normaliza, tokeniza y aplica stemming (para inglÃ©s).\"\"\"\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\([^)]*\\)\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\d+\", \" NUM \", text)\n",
    "\n",
    "    text = re.sub(r\"[^a-z0-9'\\-]\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    tokens = nltk.word_tokenize(text, language=\"english\")\n",
    "\n",
    "    tokens = [\n",
    "        stemmer.stem(token)\n",
    "        for token in tokens\n",
    "        if token.isalpha() and token not in stop_words\n",
    "    ]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def extract_body(text: str) -> str:\n",
    "    \"\"\"Extrae el cuerpo del texto dejando el Subject al inicio y eliminando From.\"\"\"\n",
    "\n",
    "    text = re.sub(r\"^From:.*\\n\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    subject_match = re.search(\n",
    "        r\"^Subject:\\s*(.*)\", text, flags=re.MULTILINE | re.IGNORECASE\n",
    "    )\n",
    "    subject = subject_match.group(1).strip() if subject_match else \"\"\n",
    "\n",
    "    body = re.sub(\n",
    "        r\"^Subject:.*\\n\", \"\", text, flags=re.MULTILINE | re.IGNORECASE\n",
    "    ).strip()\n",
    "\n",
    "    if subject:\n",
    "        body = subject + \"\\n\\n\" + body\n",
    "\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550e1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"76013\", \"theme\": \"misc.forsale\", \"text\": \"From: xorcist@cyberden.sf.ca.us\\nSubject: [ For Sale ] EMS Freq. Shifter\\n\\nThis is yet for a friend again.\\n\\nEMS Freq. Shifter. (The machine that made those 3-D swirling guitar effects\\n                    way back in the 70's.)\\n\\nSpacial panner with harmonic shifting. Very rare - A collectors Item. This\\nis the last unit EMS ever made. Rack mountable. Mono in, Up/Down signals out\\nwith seperate Pan out and sine/unsine voltage outs. VCO input too to control\\nLFO. If interested contact Kevin before 9 pm PST (California) at 818-362-7883\\nand make an offer.\\n\\nDo not reply to this account.\\n\\nHave a nice day\\n\\n__________________________________________________________________________\\n   |       /         |\\\\\\n   | H E   \\\\ Y B E R |/ E N            [ xorcist@cyberden.sf.ca.us ]\\n\\n   The CyberDen - Public Access Waffle Usenet System - 415/472-5527\"}\n",
      "\n",
      "['sale', 'em', 'freq', 'shifter', 'yet', 'friend', 'em', 'freq', 'shifter', 'spacial', 'panner', 'harmon', 'shift', 'rare', 'collector', 'item', 'last', 'unit', 'em', 'ever', 'made', 'rack', 'mountabl', 'mono', 'signal', 'seper', 'pan', 'sine', 'unsin', 'voltag', 'out', 'vco', 'input', 'control', 'lfo', 'interest', 'contact', 'kevin', 'pm', 'pst', 'make', 'offer', 'repli', 'account', 'nice', 'day', 'h', 'e', 'b', 'e', 'r', 'e', 'n', 'cyberden', 'public', 'access', 'waffl', 'usenet', 'system']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "with open(os.path.join(PATH_FINAL_FILES, \"20N.jsonl\")) as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        labels.append(data[\"theme\"])      \n",
    "        texts.append(extract_body(data[\"text\"]))\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c0dd05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=preprocess_text,   \n",
    "    preprocessor=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105422d7",
   "metadata": {},
   "source": [
    "## I. For the 20N dataset compare two classifiers NB and LR to identify the 20 different newsgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b52193",
   "metadata": {},
   "source": [
    "### Create your own processing pipeline for the task and justify it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ebee4b",
   "metadata": {},
   "source": [
    "### Divide the dataset into training (60%), validation (10%) and test (30%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057226d9",
   "metadata": {},
   "source": [
    "### Train NB and LR using the following vector representations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83fc1aa",
   "metadata": {},
   "source": [
    "#### tf (counts) representation (sklearn: CountVectorizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ce750",
   "metadata": {},
   "source": [
    "#### tfidf representation (sklearn: TfidfVectorizer)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
