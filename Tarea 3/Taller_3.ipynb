{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "200ddfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from charset_normalizer import from_path\n",
    "from gensim.corpora import Dictionary\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "Se importan las librerias que se necesiten, \n",
    "si se quiere ejecutar el notebook, se recomienda crear la carpeta de data, y poner ahi los files como se describe\n",
    "\n",
    "\"\"\"\n",
    "ACTUAL_PATH = os.getcwd()\n",
    "# Donde esta el 20 News\n",
    "PATH_20N = os.path.join(ACTUAL_PATH, \"data/20news-18828\")\n",
    "# Donde se encuentra el BAC\n",
    "PATH_BAC = os.path.join(ACTUAL_PATH, \"data/BAC/blogs\")\n",
    "# Donde se van a guardar los files que se van obteniendo\n",
    "PATH_FINAL_FILES = os.path.join(ACTUAL_PATH, \"data/final_files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d022f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4a711",
   "metadata": {},
   "source": [
    "### Upload 20N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15256f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Para esto se define en que formato y donde se quiere el archivo completo de 20N\"\"\"\n",
    "\n",
    "NEW_20N_FILE = os.path.join(PATH_FINAL_FILES, \"20N.jsonl\")\n",
    "\n",
    "mayor_folders_20N = os.listdir(PATH_20N)\n",
    "dictionary = {}\n",
    "\"\"\"\n",
    "Para cada archivo disponible en 20N,  \n",
    "se generan registros con:  \n",
    "- el ID del archivo  \n",
    "- el tema del archivo  \n",
    "- el texto del contenido  \n",
    "\n",
    "Todo se guarda en formato JSONL por cuestiones de formato.\n",
    "\"\"\"\n",
    "with open(NEW_20N_FILE, \"w\", encoding=\"utf-8\", errors=\"replace\") as f_n:\n",
    "    for folder in mayor_folders_20N:\n",
    "        minor_files_path = os.path.join(PATH_20N, folder)\n",
    "        minor_files = os.listdir(minor_files_path)\n",
    "        for file in minor_files:\n",
    "            file_path = os.path.join(minor_files_path, file)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "                text = f.read().strip()\n",
    "\n",
    "            record = {\"id\": file, \"theme\": folder, \"text\": text}\n",
    "            unit = folder + file\n",
    "            if file in dictionary.keys():\n",
    "                dictionary[unit] += 1\n",
    "            else:\n",
    "                dictionary[unit] = 1\n",
    "            f_n.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"procesador de sentencias\n",
    "\n",
    "    Args:\n",
    "        text (str): sentencia a procesar\n",
    "\n",
    "    Returns:\n",
    "        list[str]: lista de palabras y simbolos a dejar.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,!?]\", \" \", text, flags=re.I | re.A | re.MULTILINE)\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\d+\", \"NUM\", text)\n",
    "    text = \"<s> \" + text + \" </s>\"\n",
    "    text = text.strip().split()\n",
    "    return text\n",
    "\n",
    "\n",
    "def divide_sentences(text: str) -> list[str]:\n",
    "    \"\"\"Divide los textos en sentencias\n",
    "\n",
    "    Args:\n",
    "        text (str): texto completo\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Lista completa de sentencias\n",
    "    \"\"\"\n",
    "    sentences = nltk.word_tokenize(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad81e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9'\\-]\", \" \", text)\n",
    "    text = re.sub(r\"\\d+\", \"NUM\", text) \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip() \n",
    "\n",
    "    tokens = nltk.word_tokenize(text, language=\"english\")\n",
    "    \n",
    "    tokens = [stemmer.stem(token) for token in tokens if token.isalpha()]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9550e1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'num', 'theme', 'misc', 'forsal', 'text', 'from', 'xorcist', 'cyberden', 'sf', 'ca', 'us', 'nsubject', 'for', 'sale', 'em', 'freq', 'shifter', 'n', 'nthis', 'is', 'yet', 'for', 'a', 'friend', 'again', 'n', 'nem', 'freq', 'shifter', 'the', 'machin', 'that', 'made', 'those', 'swirl', 'guitar', 'effect', 'n', 'way', 'back', 'in', 'the', 'num', 'n', 'nspacial', 'panner', 'with', 'harmon', 'shift', 'veri', 'rare', 'a', 'collector', 'item', 'this', 'nis', 'the', 'last', 'unit', 'em', 'ever', 'made', 'rack', 'mountabl', 'mono', 'in', 'up', 'down', 'signal', 'out', 'nwith', 'seper', 'pan', 'out', 'and', 'sine', 'unsin', 'voltag', 'out', 'vco', 'input', 'too', 'to', 'control', 'nlfo', 'if', 'interest', 'contact', 'kevin', 'befor', 'num', 'pm', 'pst', 'california', 'at', 'nand', 'make', 'an', 'offer', 'n', 'ndo', 'not', 'repli', 'to', 'this', 'account', 'n', 'nhave', 'a', 'nice', 'day', 'n', 'n', 'n', 'n', 'h', 'e', 'y', 'b', 'e', 'r', 'e', 'n', 'xorcist', 'cyberden', 'sf', 'ca', 'us', 'n', 'n', 'the', 'cyberden', 'public', 'access', 'waffl', 'usenet', 'system', 'num']\n",
      "{\"id\": \"75981\", \"theme\": \"misc.forsale\", \"text\": \"From: cthulhu@mosquito.cis.ufl.edu (Mark Kupper)\\nSubject: ** Comics for sale **\\n\\n  I want to get rid of alot of comics that I have. I am selling for 30% off\\nthe Overstreet Price Guide. \\n\\nCOMIC                                           CONDITION\\n-----                                           ---------\\n\\nArion #1                                        M\\nBatman's Detective Comics #480                  VF-NM\\nContest of Champions #1                         M\\nContest of Champions #2                         M\\nContest of Champions #3                         M\\nCrystar #1                                      M\\nDaredevil #181 (Elektra Dies)                   NM-M\\nDaredevil #186                                  M\\nFantastic Four #52 (1st app. Black Panther)     F-VF\\nG.I. Joe #1                                     M\\nHercules #1                                     M\\nIncredible Hulk #181 (1st app. Wolverine)       VF\\nThe Krypton Chronicles #1                       M\\nThe Man-Thing #1                                M\\nThe Man-Thing #5                                M\\nMarvel Age #1                                   VF\\nMarvel Age #2                                   NM\\nMarvel and DC Present (X-men and New\\n        Teen Titans)                            M\\nMarvel Graphic Novel #4 (1st app. New Mutants)  M\\nThe Marvel Guide to Collecting Comics           NM\\nMarvel Team-up #1                               VF-NM\\nMarvel Team-up #95                              M\\nMaster of Kung Fu #90                           M\\nThe Micronauts #1                               M\\nMicronauts King-Size Annual #1                  M\\nNew Mutants #1 (5 copies!)                      M\\nNew Mutants #2                                  M\\nNew Mutants #3                                  M\\nThe Omega Men #1                                M\\nRed Sonja #1                                    M\\nRipley's Believe It or Not True War Strories #1 VF\\nRom Spaceknight #1                              M\\nRom Spaceknight #8                              M\\nThe Secret Society of Super Villains #1         NM\\nPeter Parker, the Spectacular Spiderman #44     M\\nAmazing Spiderman #188                          M\\nStar Trek #4                                    M\\nSuper-Villain Classics #1 (Origin Galactus)     M\\nNew Teen Titans #1                              M\\nUncanny Tales #33 (Publisher's File Copy)       NM-M\\nVision and the Scarlet Witch #1                 M\\nWhat If #3 (The Avengers Had Never Been)        NM\\nWolverine #1 (limited series)                   M\\nWolverine #2 (limited series)                   M\\nWolverine #3 (limited series)                   M\\nWolverine #4 (limited series)                   M\\nX-men #25                                       F\\nX-men #26                                       F\\nX-men #30                                       F\\nX-men #34                                       F\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(PATH_FINAL_FILES, \"20N.jsonl\")) as f:\n",
    "    print(preprocess_text(f.readline()))\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105422d7",
   "metadata": {},
   "source": [
    "## I. For the 20N dataset compare two classifiers NB and LR to identify the 20 different newsgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b52193",
   "metadata": {},
   "source": [
    "### Create your own processing pipeline for the task and justify it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ebee4b",
   "metadata": {},
   "source": [
    "### Divide the dataset into training (60%), validation (10%) and test (30%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057226d9",
   "metadata": {},
   "source": [
    "### Train NB and LR using the following vector representations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83fc1aa",
   "metadata": {},
   "source": [
    "#### tf (counts) representation (sklearn: CountVectorizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ce750",
   "metadata": {},
   "source": [
    "#### tfidf representation (sklearn: TfidfVectorizer)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
