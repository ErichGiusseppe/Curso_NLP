{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ddfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from charset_normalizer import from_path\n",
    "from gensim.corpora import Dictionary\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "Se importan las librerias que se necesiten, \n",
    "si se quiere ejecutar el notebook, se recomienda crear la carpeta de data, y poner ahi los files como se describe\n",
    "\n",
    "\"\"\"\n",
    "ACTUAL_PATH = os.getcwd()\n",
    "# Donde esta el 20 News\n",
    "PATH_20N = os.path.join(ACTUAL_PATH, \"data/20news-18828\")\n",
    "# Donde se encuentra el BAC\n",
    "PATH_BAC = os.path.join(ACTUAL_PATH, \"data/BAC/blogs\")\n",
    "# Donde se van a guardar los files que se van obteniendo\n",
    "PATH_FINAL_FILES = os.path.join(ACTUAL_PATH, \"data/final_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d022f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from skopt import BayesSearchCV\n",
    "import numpy as np\n",
    "\n",
    "stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "val_ratio_within_train = 1.0 / 7.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4a711",
   "metadata": {},
   "source": [
    "### Upload 20N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15256f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Para esto se define en que formato y donde se quiere el archivo completo de 20N\"\"\"\n",
    "\n",
    "NEW_20N_FILE = os.path.join(PATH_FINAL_FILES, \"20N.jsonl\")\n",
    "\n",
    "mayor_folders_20N = os.listdir(PATH_20N)\n",
    "dictionary = {}\n",
    "\"\"\"\n",
    "Para cada archivo disponible en 20N,  \n",
    "se generan registros con:  \n",
    "- el ID del archivo  \n",
    "- el tema del archivo  \n",
    "- el texto del contenido  \n",
    "\n",
    "Todo se guarda en formato JSONL por cuestiones de formato.\n",
    "\"\"\"\n",
    "with open(NEW_20N_FILE, \"w\", encoding=\"utf-8\", errors=\"replace\") as f_n:\n",
    "    for folder in mayor_folders_20N:\n",
    "        minor_files_path = os.path.join(PATH_20N, folder)\n",
    "        minor_files = os.listdir(minor_files_path)\n",
    "        for file in minor_files:\n",
    "            file_path = os.path.join(minor_files_path, file)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "                text = f.read().strip()\n",
    "\n",
    "            record = {\"id\": file, \"theme\": folder, \"text\": text}\n",
    "            unit = folder + file\n",
    "            if file in dictionary.keys():\n",
    "                dictionary[unit] += 1\n",
    "            else:\n",
    "                dictionary[unit] = 1\n",
    "            f_n.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad81e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"Limpia y preprocesa texto: elimina correos, URLs,\n",
    "    normaliza, tokeniza y aplica stemming (para inglés).\"\"\"\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\([^)]*\\)\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\d+\", \" NUM \", text)\n",
    "\n",
    "    text = re.sub(r\"[^a-z0-9'\\-]\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    tokens = nltk.word_tokenize(text, language=\"english\")\n",
    "\n",
    "    tokens = [\n",
    "        stemmer.stem(token)\n",
    "        for token in tokens\n",
    "        if token.isalpha() and token not in stop_words\n",
    "    ]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def extract_body(text: str) -> str:\n",
    "    \"\"\"Extrae el cuerpo del texto dejando el Subject al inicio y eliminando From.\"\"\"\n",
    "\n",
    "    text = re.sub(r\"^From:.*\\n\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    subject_match = re.search(\n",
    "        r\"^Subject:\\s*(.*)\", text, flags=re.MULTILINE | re.IGNORECASE\n",
    "    )\n",
    "    subject = subject_match.group(1).strip() if subject_match else \"\"\n",
    "\n",
    "    body = re.sub(\n",
    "        r\"^Subject:.*\\n\", \"\", text, flags=re.MULTILINE | re.IGNORECASE\n",
    "    ).strip()\n",
    "\n",
    "    if subject:\n",
    "        body = subject + \"\\n\\n\" + body\n",
    "\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "with open(os.path.join(PATH_FINAL_FILES, \"20N.jsonl\")) as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        labels.append(data[\"theme\"])      \n",
    "        texts.append(extract_body(data[\"text\"]))\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ecd4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_modelo_val_train(X_train, y_train, espacio, model, iteraciones=30):\n",
    "    # Seria mas optimo tener esta seccion apartada, pero dado el numero de outputs que genera prefiero \n",
    "    # por limieza tenerlo aca, ademas el dataset no es tan grande como para preocuparme por cuanto se demora esta operacion.\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=val_ratio_within_train,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y_train\n",
    "    )\n",
    "\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"vect\", CountVectorizer(\n",
    "            tokenizer=preprocess_text,\n",
    "            preprocessor=None,\n",
    "            token_pattern=None \n",
    "        )),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    X_total = np.array(list(X_tr) + list(X_val), dtype=object)\n",
    "    y_total = np.array(list(y_tr) + list(y_val))\n",
    "\n",
    "    test_fold = np.array([-1] * len(X_tr) + [0] * len(X_val))\n",
    "    ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "    opt = BayesSearchCV(\n",
    "        estimator=pipeline,\n",
    "        search_spaces=espacio,\n",
    "        n_iter=iteraciones,\n",
    "        cv=ps,                   \n",
    "        scoring='f1_macro',\n",
    "        refit=True,              \n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbose= 0 \n",
    "    )\n",
    "\n",
    "    opt.fit(X_total, y_total)\n",
    "    print(\"Mejores hiperparámetros:\", opt.best_params_)\n",
    "\n",
    "    y_pred = opt.predict(X_test) \n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "espacio = {\n",
    "    'vect__max_df': (0.5, 1.0),\n",
    "    'model__alpha': (1e-3, 1.0, 'log-uniform')\n",
    "}\n",
    "\n",
    "generar_modelo_val_train(X_train,y_train,espacio,MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105422d7",
   "metadata": {},
   "source": [
    "## I. For the 20N dataset compare two classifiers NB and LR to identify the 20 different newsgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b52193",
   "metadata": {},
   "source": [
    "### Create your own processing pipeline for the task and justify it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ebee4b",
   "metadata": {},
   "source": [
    "### Divide the dataset into training (60%), validation (10%) and test (30%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057226d9",
   "metadata": {},
   "source": [
    "### Train NB and LR using the following vector representations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83fc1aa",
   "metadata": {},
   "source": [
    "#### tf (counts) representation (sklearn: CountVectorizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ce750",
   "metadata": {},
   "source": [
    "#### tfidf representation (sklearn: TfidfVectorizer)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
