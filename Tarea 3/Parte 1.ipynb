{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ddfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/erich/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from tabulate import tabulate\n",
    "\n",
    "from utils_taller3 import SenticLexiconFeaturizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "ACTUAL_PATH = os.getcwd()\n",
    "PATH_20N = os.path.join(ACTUAL_PATH, \"data/20news-18828\")\n",
    "PATH_MD = os.path.join(ACTUAL_PATH, \"data/Multi Domain Sentiment/processed_acl\")\n",
    "PATH_FINAL_FILES = os.path.join(ACTUAL_PATH, \"data/final_files\")\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "val_ratio_within_train = 1.0 / 7.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4a711",
   "metadata": {},
   "source": [
    "### Upload 20N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15256f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Para esto se define en que formato y donde se quiere el archivo completo de 20N\"\"\"\n",
    "\n",
    "NEW_20N_FILE = os.path.join(PATH_FINAL_FILES, \"20N.jsonl\")\n",
    "\n",
    "mayor_folders_20N = os.listdir(PATH_20N)\n",
    "dictionary = {}\n",
    "\"\"\"\n",
    "Para cada archivo disponible en 20N,  \n",
    "se generan registros con:  \n",
    "- el ID del archivo  \n",
    "- el tema del archivo  \n",
    "- el texto del contenido  \n",
    "\n",
    "Todo se guarda en formato JSONL por cuestiones de formato.\n",
    "\"\"\"\n",
    "with open(NEW_20N_FILE, \"w\", encoding=\"utf-8\", errors=\"replace\") as f_n:\n",
    "    for folder in mayor_folders_20N:\n",
    "        minor_files_path = os.path.join(PATH_20N, folder)\n",
    "        minor_files = os.listdir(minor_files_path)\n",
    "        for file in minor_files:\n",
    "            file_path = os.path.join(minor_files_path, file)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "                text = f.read().strip()\n",
    "\n",
    "            record = {\"id\": file, \"theme\": folder, \"text\": text}\n",
    "            unit = folder + file\n",
    "            if file in dictionary.keys():\n",
    "                dictionary[unit] += 1\n",
    "            else:\n",
    "                dictionary[unit] = 1\n",
    "            f_n.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c89a12",
   "metadata": {},
   "source": [
    "### I. For the 20N dataset compare two classifiers NB and LR to identify the 20 different newsgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad81e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"Limpia y preprocesa texto: elimina correos, URLs,\n",
    "    normaliza, tokeniza y aplica stemming (para inglés).\"\"\"\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\([^)]*\\)\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\d+\", \" NUM \", text)\n",
    "\n",
    "    text = re.sub(r\"[^a-z0-9'\\-]\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    tokens = nltk.word_tokenize(text, language=\"english\")\n",
    "\n",
    "    tokens = [\n",
    "        stemmer.stem(token)\n",
    "        for token in tokens\n",
    "        if token.isalpha() and token not in stop_words\n",
    "    ]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def extract_body(text: str) -> str:\n",
    "    \"\"\"Extrae el cuerpo del texto dejando el Subject al inicio y eliminando From.\"\"\"\n",
    "\n",
    "    text = re.sub(r\"^From:.*\\n\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    subject_match = re.search(\n",
    "        r\"^Subject:\\s*(.*)\", text, flags=re.MULTILINE | re.IGNORECASE\n",
    "    )\n",
    "    subject = subject_match.group(1).strip() if subject_match else \"\"\n",
    "\n",
    "    body = re.sub(\n",
    "        r\"^Subject:.*\\n\", \"\", text, flags=re.MULTILINE | re.IGNORECASE\n",
    "    ).strip()\n",
    "\n",
    "    if subject:\n",
    "        body = subject + \"\\n\\n\" + body\n",
    "\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9550e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "with open(os.path.join(PATH_FINAL_FILES, \"20N.jsonl\")) as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        labels.append(data[\"theme\"])\n",
    "        texts.append(extract_body(data[\"text\"]))\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02a41c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_de_modelos = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86e2fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostrar_resultados_tabulate(resultados_segunda_parte, ordenar_por=\"f1_macro\"):\n",
    "    \"\"\"\n",
    "    Muestra los resultados en formato de tabla usando tabulate.\n",
    "    Ordena por la métrica especificada (default: f1_macro).\n",
    "    \"\"\"\n",
    "    ejemplo = next(iter(resultados_segunda_parte.values()))\n",
    "    columnas = [\"Modelo\"] + list(ejemplo.keys())\n",
    "\n",
    "    filas = []\n",
    "    for modelo, metricas in resultados_segunda_parte.items():\n",
    "        fila = [modelo]\n",
    "        for valor in metricas.values():\n",
    "            fila.append(round(valor, 4) if isinstance(valor, (int, float)) else valor)\n",
    "        filas.append(fila)\n",
    "\n",
    "    if ordenar_por in ejemplo:\n",
    "        idx = columnas.index(ordenar_por)\n",
    "        filas.sort(key=lambda x: x[idx], reverse=True)\n",
    "\n",
    "    print(tabulate(filas, headers=columnas, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ecd4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_modelo_val_train(X_train, y_train, model, text_repr):\n",
    "    # Seria mas optimo tener esta seccion apartada, pero dado el numero de outputs que genera prefiero\n",
    "    # por limieza tenerlo aca, ademas el dataset no es tan grande como para preocuparme por cuanto se demora esta operacion.\n",
    "    # X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    #     X_train,\n",
    "    #     y_train,\n",
    "    #     test_size=val_ratio_within_train,\n",
    "    #     random_state=RANDOM_STATE,\n",
    "    #     stratify=y_train,\n",
    "    # )\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"repr\",\n",
    "                text_repr,\n",
    "            ),\n",
    "            (\"model\", model),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # X_total = np.array(list(X_tr) + list(X_val), dtype=object)\n",
    "    # y_total = np.array(list(y_tr) + list(y_val))\n",
    "\n",
    "    # test_fold = np.array([-1] * len(X_tr) + [0] * len(X_val))\n",
    "    # ps = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Mejores hiperparámetros:\", pipeline.get_params())\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def evaluate(opt, X_test, y_test, print_flag=False):\n",
    "    \"\"\"\n",
    "    Evalúa un modelo optimizado (BayesSearchCV) en el conjunto de test.\n",
    "    Devuelve un diccionario con métricas.\n",
    "    \"\"\"\n",
    "    y_pred = opt.predict(X_test)\n",
    "\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"macro\"\n",
    "    )\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"micro\"\n",
    "    )\n",
    "\n",
    "    resultados = {\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"recall_macro\": recall_macro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"precision_micro\": precision_micro,\n",
    "        \"recall_micro\": recall_micro,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"best_params\": opt.get_params(),\n",
    "    }\n",
    "    if print_flag:\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return resultados\n",
    "\n",
    "\n",
    "def entrenar_modelos(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Itera por diferentes combinaciones de representaciones y modelos.\n",
    "    Devuelve un diccionario con los resultados de cada configuración.\n",
    "    \"\"\"\n",
    "\n",
    "    configuraciones = {\n",
    "        \"BOW_LR_1_VAL\": {\n",
    "            \"model\": LogisticRegression(\n",
    "                solver=\"saga\", penalty=\"l2\"  # , max_iter=iteraciones\n",
    "            ),\n",
    "            \"text_repr\": CountVectorizer(\n",
    "                tokenizer=preprocess_text, preprocessor=None, token_pattern=None\n",
    "            ),\n",
    "            \"espacio\": {\n",
    "                # \"repr__max_df\": (0.5, 1.0),\n",
    "                # \"model__C\": (1e-3, 1e2, \"log-uniform\"),\n",
    "            },\n",
    "        },\n",
    "        \"TF-IDF_LR_1_VAL\": {\n",
    "            \"model\": LogisticRegression(solver=\"saga\", penalty=\"l2\"),  # , max_iter=2000\n",
    "            \"text_repr\": TfidfVectorizer(\n",
    "                tokenizer=preprocess_text, preprocessor=None, token_pattern=None\n",
    "            ),\n",
    "            \"espacio\": {\n",
    "                # \"repr__max_df\": (0.5, 1.0),\n",
    "                # \"model__C\": (1e-3, 1e2, \"log-uniform\"),\n",
    "            },\n",
    "        },\n",
    "        \"BOW_NB_1_VAL\": {\n",
    "            \"model\": MultinomialNB(),\n",
    "            \"text_repr\": CountVectorizer(\n",
    "                tokenizer=preprocess_text, preprocessor=None, token_pattern=None\n",
    "            ),\n",
    "            \"espacio\": {\n",
    "                # \"repr__max_df\": (0.5, 1.0),\n",
    "                # \"model__alpha\": (1e-3, 1.0, \"log-uniform\"),\n",
    "            },\n",
    "        },\n",
    "        \"TF-IDF_NB_1_VAL\": {\n",
    "            \"model\": MultinomialNB(),\n",
    "            \"text_repr\": TfidfVectorizer(\n",
    "                tokenizer=preprocess_text, preprocessor=None, token_pattern=None\n",
    "            ),\n",
    "            \"espacio\": {\n",
    "                # \"repr__max_df\": (0.5, 1.0),\n",
    "                # \"model__alpha\": (1e-3, 1.0, \"log-uniform\"),\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    resultados = {}\n",
    "    for nombre, cfg in configuraciones.items():\n",
    "        print(f\"\\nEntrenando modelo: {nombre}\")\n",
    "        opt = generar_modelo_val_train(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cfg[\"model\"],\n",
    "            cfg[\"text_repr\"],\n",
    "        )\n",
    "        resultados[nombre] = evaluate(opt, X_test, y_test, print_flag=True)\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d3a1b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando modelo: BOW_LR_1_VAL\n",
      "Mejores hiperparámetros: {'memory': None, 'steps': [('repr', CountVectorizer(token_pattern=None,\n",
      "                tokenizer=<function preprocess_text at 0x7560c4f34e50>)), ('model', LogisticRegression(solver='saga'))], 'transform_input': None, 'verbose': False, 'repr': CountVectorizer(token_pattern=None,\n",
      "                tokenizer=<function preprocess_text at 0x7560c4f34e50>), 'model': LogisticRegression(solver='saga'), 'repr__analyzer': 'word', 'repr__binary': False, 'repr__decode_error': 'strict', 'repr__dtype': <class 'numpy.int64'>, 'repr__encoding': 'utf-8', 'repr__input': 'content', 'repr__lowercase': True, 'repr__max_df': 1.0, 'repr__max_features': None, 'repr__min_df': 1, 'repr__ngram_range': (1, 1), 'repr__preprocessor': None, 'repr__stop_words': None, 'repr__strip_accents': None, 'repr__token_pattern': None, 'repr__tokenizer': <function preprocess_text at 0x7560c4f34e50>, 'repr__vocabulary': None, 'model__C': 1.0, 'model__class_weight': None, 'model__dual': False, 'model__fit_intercept': True, 'model__intercept_scaling': 1, 'model__l1_ratio': None, 'model__max_iter': 100, 'model__multi_class': 'deprecated', 'model__n_jobs': None, 'model__penalty': 'l2', 'model__random_state': None, 'model__solver': 'saga', 'model__tol': 0.0001, 'model__verbose': 0, 'model__warm_start': False}\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.52      0.55      0.54       240\n",
      "           comp.graphics       0.73      0.48      0.58       292\n",
      " comp.os.ms-windows.misc       0.56      0.77      0.65       296\n",
      "comp.sys.ibm.pc.hardware       0.52      0.64      0.57       295\n",
      "   comp.sys.mac.hardware       0.85      0.50      0.63       288\n",
      "          comp.windows.x       0.67      0.66      0.66       294\n",
      "            misc.forsale       0.86      0.43      0.58       292\n",
      "               rec.autos       0.78      0.73      0.76       297\n",
      "         rec.motorcycles       0.94      0.78      0.85       298\n",
      "      rec.sport.baseball       0.90      0.73      0.81       298\n",
      "        rec.sport.hockey       0.73      0.94      0.82       300\n",
      "               sci.crypt       0.60      0.92      0.73       297\n",
      "         sci.electronics       0.74      0.39      0.52       294\n",
      "                 sci.med       0.84      0.79      0.81       297\n",
      "               sci.space       0.82      0.78      0.80       296\n",
      "  soc.religion.christian       0.53      0.86      0.66       299\n",
      "      talk.politics.guns       0.61      0.88      0.72       273\n",
      "   talk.politics.mideast       0.73      0.93      0.82       282\n",
      "      talk.politics.misc       0.58      0.62      0.60       233\n",
      "      talk.religion.misc       0.56      0.03      0.05       188\n",
      "\n",
      "                accuracy                           0.68      5649\n",
      "               macro avg       0.70      0.67      0.66      5649\n",
      "            weighted avg       0.71      0.68      0.67      5649\n",
      "\n",
      "\n",
      "Entrenando modelo: TF-IDF_LR_1_VAL\n",
      "Mejores hiperparámetros: {'memory': None, 'steps': [('repr', TfidfVectorizer(token_pattern=None,\n",
      "                tokenizer=<function preprocess_text at 0x7560c4f34e50>)), ('model', LogisticRegression(solver='saga'))], 'transform_input': None, 'verbose': False, 'repr': TfidfVectorizer(token_pattern=None,\n",
      "                tokenizer=<function preprocess_text at 0x7560c4f34e50>), 'model': LogisticRegression(solver='saga'), 'repr__analyzer': 'word', 'repr__binary': False, 'repr__decode_error': 'strict', 'repr__dtype': <class 'numpy.float64'>, 'repr__encoding': 'utf-8', 'repr__input': 'content', 'repr__lowercase': True, 'repr__max_df': 1.0, 'repr__max_features': None, 'repr__min_df': 1, 'repr__ngram_range': (1, 1), 'repr__norm': 'l2', 'repr__preprocessor': None, 'repr__smooth_idf': True, 'repr__stop_words': None, 'repr__strip_accents': None, 'repr__sublinear_tf': False, 'repr__token_pattern': None, 'repr__tokenizer': <function preprocess_text at 0x7560c4f34e50>, 'repr__use_idf': True, 'repr__vocabulary': None, 'model__C': 1.0, 'model__class_weight': None, 'model__dual': False, 'model__fit_intercept': True, 'model__intercept_scaling': 1, 'model__l1_ratio': None, 'model__max_iter': 100, 'model__multi_class': 'deprecated', 'model__n_jobs': None, 'model__penalty': 'l2', 'model__random_state': None, 'model__solver': 'saga', 'model__tol': 0.0001, 'model__verbose': 0, 'model__warm_start': False}\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.83      0.83      0.83       240\n",
      "           comp.graphics       0.80      0.85      0.82       292\n",
      " comp.os.ms-windows.misc       0.77      0.86      0.81       296\n",
      "comp.sys.ibm.pc.hardware       0.77      0.76      0.76       295\n",
      "   comp.sys.mac.hardware       0.90      0.86      0.88       288\n",
      "          comp.windows.x       0.86      0.85      0.85       294\n",
      "            misc.forsale       0.86      0.86      0.86       292\n",
      "               rec.autos       0.91      0.86      0.88       297\n",
      "         rec.motorcycles       0.96      0.94      0.95       298\n",
      "      rec.sport.baseball       0.96      0.97      0.96       298\n",
      "        rec.sport.hockey       0.97      0.98      0.97       300\n",
      "               sci.crypt       0.96      0.93      0.95       297\n",
      "         sci.electronics       0.79      0.86      0.82       294\n",
      "                 sci.med       0.91      0.95      0.93       297\n",
      "               sci.space       0.90      0.94      0.92       296\n",
      "  soc.religion.christian       0.84      0.89      0.87       299\n",
      "      talk.politics.guns       0.89      0.90      0.89       273\n",
      "   talk.politics.mideast       0.95      0.95      0.95       282\n",
      "      talk.politics.misc       0.82      0.80      0.81       233\n",
      "      talk.religion.misc       0.90      0.55      0.68       188\n",
      "\n",
      "                accuracy                           0.88      5649\n",
      "               macro avg       0.88      0.87      0.87      5649\n",
      "            weighted avg       0.88      0.88      0.88      5649\n",
      "\n",
      "\n",
      "Entrenando modelo: BOW_NB_1_VAL\n",
      "Mejores hiperparámetros: {'memory': None, 'steps': [('repr', CountVectorizer(token_pattern=None,\n",
      "                tokenizer=<function preprocess_text at 0x7560c4f34e50>)), ('model', MultinomialNB())], 'transform_input': None, 'verbose': False, 'repr': CountVectorizer(token_pattern=None,\n",
      "                tokenizer=<function preprocess_text at 0x7560c4f34e50>), 'model': MultinomialNB(), 'repr__analyzer': 'word', 'repr__binary': False, 'repr__decode_error': 'strict', 'repr__dtype': <class 'numpy.int64'>, 'repr__encoding': 'utf-8', 'repr__input': 'content', 'repr__lowercase': True, 'repr__max_df': 1.0, 'repr__max_features': None, 'repr__min_df': 1, 'repr__ngram_range': (1, 1), 'repr__preprocessor': None, 'repr__stop_words': None, 'repr__strip_accents': None, 'repr__token_pattern': None, 'repr__tokenizer': <function preprocess_text at 0x7560c4f34e50>, 'repr__vocabulary': None, 'model__alpha': 1.0, 'model__class_prior': None, 'model__fit_prior': True, 'model__force_alpha': True}\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.80      0.86      0.83       240\n",
      "           comp.graphics       0.65      0.85      0.74       292\n",
      " comp.os.ms-windows.misc       0.82      0.38      0.52       296\n",
      "comp.sys.ibm.pc.hardware       0.65      0.82      0.72       295\n",
      "   comp.sys.mac.hardware       0.86      0.88      0.87       288\n",
      "          comp.windows.x       0.76      0.88      0.82       294\n",
      "            misc.forsale       0.93      0.66      0.77       292\n",
      "               rec.autos       0.92      0.89      0.90       297\n",
      "         rec.motorcycles       0.96      0.95      0.96       298\n",
      "      rec.sport.baseball       0.98      0.95      0.96       298\n",
      "        rec.sport.hockey       0.96      0.97      0.96       300\n",
      "               sci.crypt       0.84      0.96      0.90       297\n",
      "         sci.electronics       0.85      0.79      0.82       294\n",
      "                 sci.med       0.95      0.94      0.94       297\n",
      "               sci.space       0.92      0.92      0.92       296\n",
      "  soc.religion.christian       0.79      0.92      0.85       299\n",
      "      talk.politics.guns       0.79      0.91      0.85       273\n",
      "   talk.politics.mideast       0.92      0.95      0.93       282\n",
      "      talk.politics.misc       0.74      0.79      0.76       233\n",
      "      talk.religion.misc       0.92      0.38      0.54       188\n",
      "\n",
      "                accuracy                           0.84      5649\n",
      "               macro avg       0.85      0.83      0.83      5649\n",
      "            weighted avg       0.85      0.84      0.83      5649\n",
      "\n",
      "\n",
      "Entrenando modelo: TF-IDF_NB_1_VAL\n",
      "Mejores hiperparámetros: {'memory': None, 'steps': [('repr', TfidfVectorizer(token_pattern=None,\n",
      "                tokenizer=<function preprocess_text at 0x7560c4f34e50>)), ('model', MultinomialNB())], 'transform_input': None, 'verbose': False, 'repr': TfidfVectorizer(token_pattern=None,\n",
      "                tokenizer=<function preprocess_text at 0x7560c4f34e50>), 'model': MultinomialNB(), 'repr__analyzer': 'word', 'repr__binary': False, 'repr__decode_error': 'strict', 'repr__dtype': <class 'numpy.float64'>, 'repr__encoding': 'utf-8', 'repr__input': 'content', 'repr__lowercase': True, 'repr__max_df': 1.0, 'repr__max_features': None, 'repr__min_df': 1, 'repr__ngram_range': (1, 1), 'repr__norm': 'l2', 'repr__preprocessor': None, 'repr__smooth_idf': True, 'repr__stop_words': None, 'repr__strip_accents': None, 'repr__sublinear_tf': False, 'repr__token_pattern': None, 'repr__tokenizer': <function preprocess_text at 0x7560c4f34e50>, 'repr__use_idf': True, 'repr__vocabulary': None, 'model__alpha': 1.0, 'model__class_prior': None, 'model__fit_prior': True, 'model__force_alpha': True}\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.86      0.60      0.71       240\n",
      "           comp.graphics       0.82      0.80      0.81       292\n",
      " comp.os.ms-windows.misc       0.76      0.83      0.79       296\n",
      "comp.sys.ibm.pc.hardware       0.70      0.80      0.75       295\n",
      "   comp.sys.mac.hardware       0.90      0.84      0.87       288\n",
      "          comp.windows.x       0.89      0.87      0.88       294\n",
      "            misc.forsale       0.93      0.67      0.78       292\n",
      "               rec.autos       0.92      0.90      0.91       297\n",
      "         rec.motorcycles       0.95      0.96      0.95       298\n",
      "      rec.sport.baseball       0.96      0.96      0.96       298\n",
      "        rec.sport.hockey       0.95      0.97      0.96       300\n",
      "               sci.crypt       0.76      0.97      0.85       297\n",
      "         sci.electronics       0.88      0.79      0.83       294\n",
      "                 sci.med       0.97      0.94      0.95       297\n",
      "               sci.space       0.92      0.95      0.93       296\n",
      "  soc.religion.christian       0.57      0.95      0.71       299\n",
      "      talk.politics.guns       0.72      0.97      0.83       273\n",
      "   talk.politics.mideast       0.87      0.96      0.92       282\n",
      "      talk.politics.misc       0.98      0.57      0.72       233\n",
      "      talk.religion.misc       1.00      0.08      0.15       188\n",
      "\n",
      "                accuracy                           0.84      5649\n",
      "               macro avg       0.86      0.82      0.81      5649\n",
      "            weighted avg       0.86      0.84      0.83      5649\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultado_de_modelos = entrenar_modelos(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f843d155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BOW_LR_1_VAL': {'precision_macro': 0.7032877316346806,\n",
       "  'recall_macro': 0.6707547437254145,\n",
       "  'f1_macro': 0.6568456131351404,\n",
       "  'precision_micro': 0.6843689148521862,\n",
       "  'recall_micro': 0.6843689148521862,\n",
       "  'f1_micro': 0.6843689148521862,\n",
       "  'accuracy': 0.6843689148521862,\n",
       "  'best_params': {'memory': None,\n",
       "   'steps': [('repr',\n",
       "     CountVectorizer(token_pattern=None,\n",
       "                     tokenizer=<function preprocess_text at 0x7560c4f34e50>)),\n",
       "    ('model', LogisticRegression(solver='saga'))],\n",
       "   'transform_input': None,\n",
       "   'verbose': False,\n",
       "   'repr': CountVectorizer(token_pattern=None,\n",
       "                   tokenizer=<function preprocess_text at 0x7560c4f34e50>),\n",
       "   'model': LogisticRegression(solver='saga'),\n",
       "   'repr__analyzer': 'word',\n",
       "   'repr__binary': False,\n",
       "   'repr__decode_error': 'strict',\n",
       "   'repr__dtype': numpy.int64,\n",
       "   'repr__encoding': 'utf-8',\n",
       "   'repr__input': 'content',\n",
       "   'repr__lowercase': True,\n",
       "   'repr__max_df': 1.0,\n",
       "   'repr__max_features': None,\n",
       "   'repr__min_df': 1,\n",
       "   'repr__ngram_range': (1, 1),\n",
       "   'repr__preprocessor': None,\n",
       "   'repr__stop_words': None,\n",
       "   'repr__strip_accents': None,\n",
       "   'repr__token_pattern': None,\n",
       "   'repr__tokenizer': <function __main__.preprocess_text(text: str) -> list[str]>,\n",
       "   'repr__vocabulary': None,\n",
       "   'model__C': 1.0,\n",
       "   'model__class_weight': None,\n",
       "   'model__dual': False,\n",
       "   'model__fit_intercept': True,\n",
       "   'model__intercept_scaling': 1,\n",
       "   'model__l1_ratio': None,\n",
       "   'model__max_iter': 100,\n",
       "   'model__multi_class': 'deprecated',\n",
       "   'model__n_jobs': None,\n",
       "   'model__penalty': 'l2',\n",
       "   'model__random_state': None,\n",
       "   'model__solver': 'saga',\n",
       "   'model__tol': 0.0001,\n",
       "   'model__verbose': 0,\n",
       "   'model__warm_start': False}},\n",
       " 'TF-IDF_LR_1_VAL': {'precision_macro': 0.8770579544484193,\n",
       "  'recall_macro': 0.8690038333118366,\n",
       "  'f1_macro': 0.8706127438014976,\n",
       "  'precision_micro': 0.8760842627013631,\n",
       "  'recall_micro': 0.8760842627013631,\n",
       "  'f1_micro': 0.8760842627013631,\n",
       "  'accuracy': 0.8760842627013631,\n",
       "  'best_params': {'memory': None,\n",
       "   'steps': [('repr',\n",
       "     TfidfVectorizer(token_pattern=None,\n",
       "                     tokenizer=<function preprocess_text at 0x7560c4f34e50>)),\n",
       "    ('model', LogisticRegression(solver='saga'))],\n",
       "   'transform_input': None,\n",
       "   'verbose': False,\n",
       "   'repr': TfidfVectorizer(token_pattern=None,\n",
       "                   tokenizer=<function preprocess_text at 0x7560c4f34e50>),\n",
       "   'model': LogisticRegression(solver='saga'),\n",
       "   'repr__analyzer': 'word',\n",
       "   'repr__binary': False,\n",
       "   'repr__decode_error': 'strict',\n",
       "   'repr__dtype': numpy.float64,\n",
       "   'repr__encoding': 'utf-8',\n",
       "   'repr__input': 'content',\n",
       "   'repr__lowercase': True,\n",
       "   'repr__max_df': 1.0,\n",
       "   'repr__max_features': None,\n",
       "   'repr__min_df': 1,\n",
       "   'repr__ngram_range': (1, 1),\n",
       "   'repr__norm': 'l2',\n",
       "   'repr__preprocessor': None,\n",
       "   'repr__smooth_idf': True,\n",
       "   'repr__stop_words': None,\n",
       "   'repr__strip_accents': None,\n",
       "   'repr__sublinear_tf': False,\n",
       "   'repr__token_pattern': None,\n",
       "   'repr__tokenizer': <function __main__.preprocess_text(text: str) -> list[str]>,\n",
       "   'repr__use_idf': True,\n",
       "   'repr__vocabulary': None,\n",
       "   'model__C': 1.0,\n",
       "   'model__class_weight': None,\n",
       "   'model__dual': False,\n",
       "   'model__fit_intercept': True,\n",
       "   'model__intercept_scaling': 1,\n",
       "   'model__l1_ratio': None,\n",
       "   'model__max_iter': 100,\n",
       "   'model__multi_class': 'deprecated',\n",
       "   'model__n_jobs': None,\n",
       "   'model__penalty': 'l2',\n",
       "   'model__random_state': None,\n",
       "   'model__solver': 'saga',\n",
       "   'model__tol': 0.0001,\n",
       "   'model__verbose': 0,\n",
       "   'model__warm_start': False}},\n",
       " 'BOW_NB_1_VAL': {'precision_macro': 0.8500041196090736,\n",
       "  'recall_macro': 0.8319336527298293,\n",
       "  'f1_macro': 0.8277003661457792,\n",
       "  'precision_micro': 0.8405027438484688,\n",
       "  'recall_micro': 0.8405027438484688,\n",
       "  'f1_micro': 0.8405027438484688,\n",
       "  'accuracy': 0.8405027438484688,\n",
       "  'best_params': {'memory': None,\n",
       "   'steps': [('repr',\n",
       "     CountVectorizer(token_pattern=None,\n",
       "                     tokenizer=<function preprocess_text at 0x7560c4f34e50>)),\n",
       "    ('model', MultinomialNB())],\n",
       "   'transform_input': None,\n",
       "   'verbose': False,\n",
       "   'repr': CountVectorizer(token_pattern=None,\n",
       "                   tokenizer=<function preprocess_text at 0x7560c4f34e50>),\n",
       "   'model': MultinomialNB(),\n",
       "   'repr__analyzer': 'word',\n",
       "   'repr__binary': False,\n",
       "   'repr__decode_error': 'strict',\n",
       "   'repr__dtype': numpy.int64,\n",
       "   'repr__encoding': 'utf-8',\n",
       "   'repr__input': 'content',\n",
       "   'repr__lowercase': True,\n",
       "   'repr__max_df': 1.0,\n",
       "   'repr__max_features': None,\n",
       "   'repr__min_df': 1,\n",
       "   'repr__ngram_range': (1, 1),\n",
       "   'repr__preprocessor': None,\n",
       "   'repr__stop_words': None,\n",
       "   'repr__strip_accents': None,\n",
       "   'repr__token_pattern': None,\n",
       "   'repr__tokenizer': <function __main__.preprocess_text(text: str) -> list[str]>,\n",
       "   'repr__vocabulary': None,\n",
       "   'model__alpha': 1.0,\n",
       "   'model__class_prior': None,\n",
       "   'model__fit_prior': True,\n",
       "   'model__force_alpha': True}},\n",
       " 'TF-IDF_NB_1_VAL': {'precision_macro': 0.8649135387750112,\n",
       "  'recall_macro': 0.819822869580689,\n",
       "  'f1_macro': 0.8133406551081531,\n",
       "  'precision_micro': 0.8383784740662064,\n",
       "  'recall_micro': 0.8383784740662064,\n",
       "  'f1_micro': 0.8383784740662064,\n",
       "  'accuracy': 0.8383784740662064,\n",
       "  'best_params': {'memory': None,\n",
       "   'steps': [('repr',\n",
       "     TfidfVectorizer(token_pattern=None,\n",
       "                     tokenizer=<function preprocess_text at 0x7560c4f34e50>)),\n",
       "    ('model', MultinomialNB())],\n",
       "   'transform_input': None,\n",
       "   'verbose': False,\n",
       "   'repr': TfidfVectorizer(token_pattern=None,\n",
       "                   tokenizer=<function preprocess_text at 0x7560c4f34e50>),\n",
       "   'model': MultinomialNB(),\n",
       "   'repr__analyzer': 'word',\n",
       "   'repr__binary': False,\n",
       "   'repr__decode_error': 'strict',\n",
       "   'repr__dtype': numpy.float64,\n",
       "   'repr__encoding': 'utf-8',\n",
       "   'repr__input': 'content',\n",
       "   'repr__lowercase': True,\n",
       "   'repr__max_df': 1.0,\n",
       "   'repr__max_features': None,\n",
       "   'repr__min_df': 1,\n",
       "   'repr__ngram_range': (1, 1),\n",
       "   'repr__norm': 'l2',\n",
       "   'repr__preprocessor': None,\n",
       "   'repr__smooth_idf': True,\n",
       "   'repr__stop_words': None,\n",
       "   'repr__strip_accents': None,\n",
       "   'repr__sublinear_tf': False,\n",
       "   'repr__token_pattern': None,\n",
       "   'repr__tokenizer': <function __main__.preprocess_text(text: str) -> list[str]>,\n",
       "   'repr__use_idf': True,\n",
       "   'repr__vocabulary': None,\n",
       "   'model__alpha': 1.0,\n",
       "   'model__class_prior': None,\n",
       "   'model__fit_prior': True,\n",
       "   'model__force_alpha': True}}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado_de_modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e1177c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Modelo          |   precision_macro |   recall_macro |   f1_macro |   precision_micro |   recall_micro |   f1_micro |   accuracy | best_params                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+=================+===================+================+============+===================+================+============+============+=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================+\n",
      "| TF-IDF_LR_1_VAL |            0.8771 |         0.869  |     0.8706 |            0.8761 |         0.8761 |     0.8761 |     0.8761 | {'memory': None, 'steps': [('repr', TfidfVectorizer(token_pattern=None,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|                 |                   |                |            |                   |                |            |            |                 tokenizer=<function preprocess_text at 0x7560c4f34e50>)), ('model', LogisticRegression(solver='saga'))], 'transform_input': None, 'verbose': False, 'repr': TfidfVectorizer(token_pattern=None,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|                 |                   |                |            |                   |                |            |            |                 tokenizer=<function preprocess_text at 0x7560c4f34e50>), 'model': LogisticRegression(solver='saga'), 'repr__analyzer': 'word', 'repr__binary': False, 'repr__decode_error': 'strict', 'repr__dtype': <class 'numpy.float64'>, 'repr__encoding': 'utf-8', 'repr__input': 'content', 'repr__lowercase': True, 'repr__max_df': 1.0, 'repr__max_features': None, 'repr__min_df': 1, 'repr__ngram_range': (1, 1), 'repr__norm': 'l2', 'repr__preprocessor': None, 'repr__smooth_idf': True, 'repr__stop_words': None, 'repr__strip_accents': None, 'repr__sublinear_tf': False, 'repr__token_pattern': None, 'repr__tokenizer': <function preprocess_text at 0x7560c4f34e50>, 'repr__use_idf': True, 'repr__vocabulary': None, 'model__C': 1.0, 'model__class_weight': None, 'model__dual': False, 'model__fit_intercept': True, 'model__intercept_scaling': 1, 'model__l1_ratio': None, 'model__max_iter': 100, 'model__multi_class': 'deprecated', 'model__n_jobs': None, 'model__penalty': 'l2', 'model__random_state': None, 'model__solver': 'saga', 'model__tol': 0.0001, 'model__verbose': 0, 'model__warm_start': False} |\n",
      "+-----------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| BOW_NB_1_VAL    |            0.85   |         0.8319 |     0.8277 |            0.8405 |         0.8405 |     0.8405 |     0.8405 | {'memory': None, 'steps': [('repr', CountVectorizer(token_pattern=None,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|                 |                   |                |            |                   |                |            |            |                 tokenizer=<function preprocess_text at 0x7560c4f34e50>)), ('model', MultinomialNB())], 'transform_input': None, 'verbose': False, 'repr': CountVectorizer(token_pattern=None,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|                 |                   |                |            |                   |                |            |            |                 tokenizer=<function preprocess_text at 0x7560c4f34e50>), 'model': MultinomialNB(), 'repr__analyzer': 'word', 'repr__binary': False, 'repr__decode_error': 'strict', 'repr__dtype': <class 'numpy.int64'>, 'repr__encoding': 'utf-8', 'repr__input': 'content', 'repr__lowercase': True, 'repr__max_df': 1.0, 'repr__max_features': None, 'repr__min_df': 1, 'repr__ngram_range': (1, 1), 'repr__preprocessor': None, 'repr__stop_words': None, 'repr__strip_accents': None, 'repr__token_pattern': None, 'repr__tokenizer': <function preprocess_text at 0x7560c4f34e50>, 'repr__vocabulary': None, 'model__alpha': 1.0, 'model__class_prior': None, 'model__fit_prior': True, 'model__force_alpha': True}                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "+-----------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| TF-IDF_NB_1_VAL |            0.8649 |         0.8198 |     0.8133 |            0.8384 |         0.8384 |     0.8384 |     0.8384 | {'memory': None, 'steps': [('repr', TfidfVectorizer(token_pattern=None,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|                 |                   |                |            |                   |                |            |            |                 tokenizer=<function preprocess_text at 0x7560c4f34e50>)), ('model', MultinomialNB())], 'transform_input': None, 'verbose': False, 'repr': TfidfVectorizer(token_pattern=None,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|                 |                   |                |            |                   |                |            |            |                 tokenizer=<function preprocess_text at 0x7560c4f34e50>), 'model': MultinomialNB(), 'repr__analyzer': 'word', 'repr__binary': False, 'repr__decode_error': 'strict', 'repr__dtype': <class 'numpy.float64'>, 'repr__encoding': 'utf-8', 'repr__input': 'content', 'repr__lowercase': True, 'repr__max_df': 1.0, 'repr__max_features': None, 'repr__min_df': 1, 'repr__ngram_range': (1, 1), 'repr__norm': 'l2', 'repr__preprocessor': None, 'repr__smooth_idf': True, 'repr__stop_words': None, 'repr__strip_accents': None, 'repr__sublinear_tf': False, 'repr__token_pattern': None, 'repr__tokenizer': <function preprocess_text at 0x7560c4f34e50>, 'repr__use_idf': True, 'repr__vocabulary': None, 'model__alpha': 1.0, 'model__class_prior': None, 'model__fit_prior': True, 'model__force_alpha': True}                                                                                                                                                                                                                                                                                                              |\n",
      "+-----------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| BOW_LR_1_VAL    |            0.7033 |         0.6708 |     0.6568 |            0.6844 |         0.6844 |     0.6844 |     0.6844 | {'memory': None, 'steps': [('repr', CountVectorizer(token_pattern=None,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|                 |                   |                |            |                   |                |            |            |                 tokenizer=<function preprocess_text at 0x7560c4f34e50>)), ('model', LogisticRegression(solver='saga'))], 'transform_input': None, 'verbose': False, 'repr': CountVectorizer(token_pattern=None,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|                 |                   |                |            |                   |                |            |            |                 tokenizer=<function preprocess_text at 0x7560c4f34e50>), 'model': LogisticRegression(solver='saga'), 'repr__analyzer': 'word', 'repr__binary': False, 'repr__decode_error': 'strict', 'repr__dtype': <class 'numpy.int64'>, 'repr__encoding': 'utf-8', 'repr__input': 'content', 'repr__lowercase': True, 'repr__max_df': 1.0, 'repr__max_features': None, 'repr__min_df': 1, 'repr__ngram_range': (1, 1), 'repr__preprocessor': None, 'repr__stop_words': None, 'repr__strip_accents': None, 'repr__token_pattern': None, 'repr__tokenizer': <function preprocess_text at 0x7560c4f34e50>, 'repr__vocabulary': None, 'model__C': 1.0, 'model__class_weight': None, 'model__dual': False, 'model__fit_intercept': True, 'model__intercept_scaling': 1, 'model__l1_ratio': None, 'model__max_iter': 100, 'model__multi_class': 'deprecated', 'model__n_jobs': None, 'model__penalty': 'l2', 'model__random_state': None, 'model__solver': 'saga', 'model__tol': 0.0001, 'model__verbose': 0, 'model__warm_start': False}                                                                                                     |\n",
      "+-----------------+-------------------+----------------+------------+-------------------+----------------+------------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "mostrar_resultados_tabulate(resultado_de_modelos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e1b39",
   "metadata": {},
   "source": [
    "### II Investigate cross-validation technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0f40e",
   "metadata": {},
   "source": [
    "#### Modelo 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c2a1da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(opt, X_test, y_test, print_flag=False):\n",
    "    y_pred = opt.predict(X_test)\n",
    "\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"macro\"\n",
    "    )\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"micro\"\n",
    "    )\n",
    "\n",
    "    resultados = {\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"recall_macro\": recall_macro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"precision_micro\": precision_micro,\n",
    "        \"recall_micro\": recall_micro,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"best_params\": opt.best_params_,\n",
    "    }\n",
    "    if print_flag:\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    return resultados\n",
    "\n",
    "\n",
    "def cv_summary_from_opt(opt, X_test=None, y_test=None, print_flag=False):\n",
    "    \"\"\"\n",
    "    Si se suministra X_test,y_test, devuelve exactamente lo mismo que evaluate(opt,...).\n",
    "    Si no, devuelve un resumen desde opt.cv_results_ (valores medios de CV).\n",
    "    \"\"\"\n",
    "\n",
    "    if X_test is not None and y_test is not None:\n",
    "        return evaluate(opt, X_test, y_test, print_flag=print_flag)\n",
    "    bi = opt.best_index_\n",
    "    cv = opt.cv_results_\n",
    "\n",
    "    out = {\n",
    "        \"precision_macro\": float(cv[\"mean_test_precision_macro\"][bi]),\n",
    "        \"recall_macro\": float(cv[\"mean_test_recall_macro\"][bi]),\n",
    "        \"f1_macro\": float(cv[\"mean_test_f1_macro\"][bi]),\n",
    "        \"precision_micro\": float(cv[\"mean_test_precision_micro\"][bi]),\n",
    "        \"recall_micro\": float(cv[\"mean_test_recall_micro\"][bi]),\n",
    "        \"f1_micro\": float(cv[\"mean_test_f1_micro\"][bi]),\n",
    "        \"accuracy\": float(cv[\"mean_test_accuracy\"][bi]),\n",
    "        \"best_params\": opt.best_params_,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "def generar_modelo_val_train(\n",
    "    X_train, y_train, espacio, model, text_repr, iteraciones=30\n",
    "):\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"repr\", text_repr),\n",
    "            (\"model\", model),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dict_scoring = {\n",
    "        \"f1_macro\": \"f1_macro\",\n",
    "        \"precision_macro\": \"precision_macro\",\n",
    "        \"recall_macro\": \"recall_macro\",\n",
    "        \"accuracy\": \"accuracy\",\n",
    "        \"f1_micro\": \"f1_micro\",\n",
    "        \"precision_micro\": \"precision_micro\",\n",
    "        \"recall_micro\": \"recall_micro\",\n",
    "    }\n",
    "\n",
    "    opt = BayesSearchCV(\n",
    "        estimator=pipeline,\n",
    "        search_spaces=espacio,\n",
    "        n_iter=iteraciones,\n",
    "        scoring=dict_scoring,\n",
    "        refit=\"f1_macro\",\n",
    "        cv=10,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    opt.fit(X_train, y_train)\n",
    "    print(\"Mejores hiperparámetros:\", opt.best_params_)\n",
    "\n",
    "    return opt\n",
    "\n",
    "\n",
    "def entrenar_modelos(X_train, y_train, X_test, y_test, iteraciones=30):\n",
    "    \"\"\"\n",
    "    Itera por combinaciones de representaciones y modelos (SGD y NB).\n",
    "    Reporta métricas de CROSS (CV del BayesSearchCV) y TEST (hold-out).\n",
    "    \"\"\"\n",
    "    configuraciones = {\n",
    "        \"BOW_SGD_10_VAL\": {\n",
    "            \"model\": SGDClassifier(loss=\"log_loss\", random_state=RANDOM_STATE),\n",
    "            \"text_repr\": CountVectorizer(\n",
    "                tokenizer=preprocess_text, preprocessor=None, token_pattern=None\n",
    "            ),\n",
    "            \"espacio\": {\n",
    "                \"repr__max_df\": (0.7, 1.0),\n",
    "                \"model__alpha\": (1e-5, 1e-2, \"log-uniform\"),\n",
    "                \"model__penalty\": [\"l2\", None, \"elasticnet\"],\n",
    "                \"model__learning_rate\": [\"optimal\", \"constant\"],\n",
    "                \"model__eta0\": (\n",
    "                    1e-3,\n",
    "                    1e-1,\n",
    "                    \"log-uniform\",\n",
    "                ),\n",
    "                \"model__early_stopping\": [True],\n",
    "                \"model__max_iter\": (50, 150),\n",
    "                \"model__tol\": (1e-4, 1e-2, \"log-uniform\"),\n",
    "            },\n",
    "        },\n",
    "        \"TF-IDF_SGD_10_VAL\": {\n",
    "            \"model\": SGDClassifier(loss=\"log_loss\", random_state=RANDOM_STATE),\n",
    "            \"text_repr\": TfidfVectorizer(\n",
    "                tokenizer=preprocess_text, preprocessor=None, token_pattern=None\n",
    "            ),\n",
    "            \"espacio\": {\n",
    "                \"repr__max_df\": (0.7, 1.0),\n",
    "                \"model__alpha\": (1e-5, 1e-2, \"log-uniform\"),\n",
    "                \"model__penalty\": [\"l2\", None, \"elasticnet\"],\n",
    "                \"model__learning_rate\": [\"optimal\", \"constant\"],\n",
    "                \"model__eta0\": (1e-3, 1e-1, \"log-uniform\"),\n",
    "                \"model__early_stopping\": [True],\n",
    "                \"model__max_iter\": (50, 150),\n",
    "                \"model__tol\": (1e-4, 1e-2, \"log-uniform\"),\n",
    "            },\n",
    "        },\n",
    "        \"BOW_NB_10_VAL\": {\n",
    "            \"model\": MultinomialNB(),\n",
    "            \"text_repr\": CountVectorizer(\n",
    "                tokenizer=preprocess_text, preprocessor=None, token_pattern=None\n",
    "            ),\n",
    "            \"espacio\": {\n",
    "                \"repr__max_df\": (0.5, 1.0),\n",
    "                \"model__alpha\": (1e-3, 1.0, \"log-uniform\"),\n",
    "                \"model__fit_prior\": [True, False],\n",
    "            },\n",
    "        },\n",
    "        \"TF-IDF_NB_10_VAL\": {\n",
    "            \"model\": MultinomialNB(),\n",
    "            \"text_repr\": TfidfVectorizer(\n",
    "                tokenizer=preprocess_text, preprocessor=None, token_pattern=None\n",
    "            ),\n",
    "            \"espacio\": {\n",
    "                \"repr__max_df\": (0.5, 1.0),\n",
    "                \"model__alpha\": (1e-3, 1.0, \"log-uniform\"),\n",
    "                \"model__fit_prior\": [True, False],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    resultados = {}\n",
    "    for nombre, cfg in configuraciones.items():\n",
    "        print(f\"\\nEntrenando modelo: {nombre}\")\n",
    "        opt = generar_modelo_val_train(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cfg[\"espacio\"],\n",
    "            cfg[\"model\"],\n",
    "            cfg[\"text_repr\"],\n",
    "            iteraciones=iteraciones,\n",
    "        )\n",
    "\n",
    "        resultados[f\"{nombre}__TEST\"] = evaluate(opt, X_test, y_test, print_flag=True)\n",
    "        resultados[f\"{nombre}__CROSS\"] = cv_summary_from_opt(opt)\n",
    "\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12363c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando modelo: BOW_SGD_10_VAL\n"
     ]
    }
   ],
   "source": [
    "resultado_de_modelos = entrenar_modelos(X_train, y_train, X_test, y_test, iteraciones=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf6725c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-------------------+----------------+------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Modelo                   |   precision_macro |   recall_macro |   f1_macro |   precision_micro | recall_micro                                                                                                                                                                                                                                                                                           |   f1_micro |   accuracy | best_params                                                                                                                                                                                                                                                                                            |\n",
      "+==========================+===================+================+============+===================+========================================================================================================================================================================================================================================================================================================+============+============+========================================================================================================================================================================================================================================================================================================+\n",
      "| TF-IDF_NB_10_VAL__TEST   |            0.8947 |         0.891  |     0.8917 |            0.8954 | 0.8954                                                                                                                                                                                                                                                                                                 |     0.8954 |     0.8954 | OrderedDict([('model__alpha', 0.016994636371262764), ('model__fit_prior', False), ('repr__max_df', 0.966433999423917)])                                                                                                                                                                                |\n",
      "+--------------------------+-------------------+----------------+------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| TF-IDF_NB_10_VAL__CROSS  |            0.8941 |         0.8885 |     0.8891 |            0.8943 | OrderedDict([('model__alpha', 0.016994636371262764), ('model__fit_prior', False), ('repr__max_df', 0.966433999423917)])                                                                                                                                                                                |     0.8943 |     0.8943 | 0.8943                                                                                                                                                                                                                                                                                                 |\n",
      "+--------------------------+-------------------+----------------+------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| TF-IDF_SGD_10_VAL__CROSS |            0.8774 |         0.8717 |     0.8727 |            0.8778 | OrderedDict([('model__alpha', 0.00016994636371262762), ('model__early_stopping', True), ('model__eta0', 0.07340675018434775), ('model__learning_rate', 'optimal'), ('model__max_iter', 117), ('model__penalty', None), ('model__tol', 0.0005033414197773552), ('repr__max_df', 0.9218512702571262)])   |     0.8778 |     0.8778 | 0.8778                                                                                                                                                                                                                                                                                                 |\n",
      "+--------------------------+-------------------+----------------+------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| TF-IDF_SGD_10_VAL__TEST  |            0.8764 |         0.8705 |     0.8721 |            0.8764 | 0.8764                                                                                                                                                                                                                                                                                                 |     0.8764 |     0.8764 | OrderedDict([('model__alpha', 0.00016994636371262762), ('model__early_stopping', True), ('model__eta0', 0.07340675018434775), ('model__learning_rate', 'optimal'), ('model__max_iter', 117), ('model__penalty', None), ('model__tol', 0.0005033414197773552), ('repr__max_df', 0.9218512702571262)])   |\n",
      "+--------------------------+-------------------+----------------+------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| BOW_NB_10_VAL__TEST      |            0.8706 |         0.8598 |     0.8574 |            0.8633 | 0.8633                                                                                                                                                                                                                                                                                                 |     0.8633 |     0.8633 | OrderedDict([('model__alpha', 0.016994636371262764), ('model__fit_prior', False), ('repr__max_df', 0.966433999423917)])                                                                                                                                                                                |\n",
      "+--------------------------+-------------------+----------------+------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| BOW_NB_10_VAL__CROSS     |            0.8686 |         0.8576 |     0.8542 |            0.8617 | OrderedDict([('model__alpha', 0.016994636371262764), ('model__fit_prior', False), ('repr__max_df', 0.966433999423917)])                                                                                                                                                                                |     0.8617 |     0.8617 | 0.8617                                                                                                                                                                                                                                                                                                 |\n",
      "+--------------------------+-------------------+----------------+------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| BOW_SGD_10_VAL__TEST     |            0.8412 |         0.8385 |     0.8391 |            0.8432 | 0.8432                                                                                                                                                                                                                                                                                                 |     0.8432 |     0.8432 | OrderedDict([('model__alpha', 0.0032521088005944944), ('model__early_stopping', True), ('model__eta0', 0.004044084484117812), ('model__learning_rate', 'constant'), ('model__max_iter', 136), ('model__penalty', 'l2'), ('model__tol', 0.00018906758484967925), ('repr__max_df', 0.8060762192885254)]) |\n",
      "+--------------------------+-------------------+----------------+------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| BOW_SGD_10_VAL__CROSS    |            0.8337 |         0.8279 |     0.829  |            0.8336 | OrderedDict([('model__alpha', 0.0032521088005944944), ('model__early_stopping', True), ('model__eta0', 0.004044084484117812), ('model__learning_rate', 'constant'), ('model__max_iter', 136), ('model__penalty', 'l2'), ('model__tol', 0.00018906758484967925), ('repr__max_df', 0.8060762192885254)]) |     0.8336 |     0.8336 | 0.8336                                                                                                                                                                                                                                                                                                 |\n",
      "+--------------------------+-------------------+----------------+------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "mostrar_resultados_tabulate(resultado_de_modelos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
