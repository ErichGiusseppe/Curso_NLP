{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6621d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from charset_normalizer import from_path\n",
    "from gensim.corpora import Dictionary\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "Se importan las librerias que se necesiten, \n",
    "si se quiere ejecutar el notebook, se recomienda crear la carpeta de data, y poner ahi los files como se describe\n",
    "\n",
    "\"\"\"\n",
    "ACTUAL_PATH = os.getcwd()\n",
    "# Donde esta el 20 News\n",
    "PATH_20N = os.path.join(ACTUAL_PATH, \"data/20news-18828\")\n",
    "# Donde se encuentra el BAC\n",
    "PATH_BAC = os.path.join(ACTUAL_PATH, \"data/BAC/blogs\")\n",
    "# Donde se van a guardar los files que se van obteniendo\n",
    "PATH_FINAL_FILES = os.path.join(ACTUAL_PATH, \"data/final_files\")\n",
    "# Numero de grupo (realmente como no hay pues simplemente se pusimos nuestros nombres)\n",
    "GRUPO = \"Erich_Carlos\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717f9467",
   "metadata": {},
   "source": [
    "## Clases para los n gramas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287c3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "    \"\"\" \n",
    "    Modelo de unigramas, \n",
    "    \"\"\"\n",
    "    def __init__(self, filename: str, file_is_training = True):\n",
    "        \"\"\"Makes the diccionary that the model needs to work,\n",
    "        ge\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo a procesar\n",
    "            file_is_training (bool, optional): \n",
    "                Indica cómo manejar el archivo de entrada.  \n",
    "                - Si es False, se carga el objeto ya procesado desde un archivo `.pickle`.  \n",
    "                - Si es True, el archivo se procesa desde cero.  \n",
    "\n",
    "        \"\"\"\n",
    "        print(filename)\n",
    "        file = self.get_pickle(filename)\n",
    "        if file_is_training:\n",
    "            self.word_counter_20N = {}\n",
    "            for sentence in file:\n",
    "                for word in sentence:\n",
    "                    self.word_counter_20N[word] = self.word_counter_20N.get(word, 0) + 1\n",
    "            self.total_words = sum(self.word_counter_20N.values())\n",
    "            self.V = len(self.word_counter_20N)\n",
    "        else:\n",
    "            self.word_counter_20N = file[\"word_counter_20N\"]\n",
    "            self.total_words = file[\"total_words\"]\n",
    "            self.V = file[\"V\"]\n",
    "\n",
    "        self.total_words = sum(self.word_counter_20N.values())\n",
    "        self.V = len(self.word_counter_20N)\n",
    "    \n",
    "    def get_pickle(self, filename: str):\n",
    "        \"\"\"\n",
    "        Abre un file en formato .pickle, \n",
    "        dentro de PATH_FINAL_FILES y lo devuelve.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del file a abrir\n",
    "\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            sentences = pickle.load(f)\n",
    "        return sentences\n",
    "    def generate_unigrams(self, filename: str):\n",
    "        \"\"\"Genera los unigramas en un archivo (jsonl) es lo que se \n",
    "        espera\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for word in self.word_counter_20N.keys():\n",
    "                prob = self.get_prob(word)\n",
    "                f.write(json.dumps({\"word\": word, \"probability\": prob}) + \"\\n\")\n",
    "    def get_prob(self, word: str) -> float:\n",
    "        \"\"\"\n",
    "        Calcula la probabilidad de un unigrama.  \n",
    "        Si la palabra existe en el vocabulario V, se devuelve su probabilidad.  \n",
    "        En caso contrario, se asigna al token <UNK>.  \n",
    "\n",
    "        Args:\n",
    "            word (str): Palabra a consultar.  \n",
    "\n",
    "        Returns:\n",
    "            float: Probabilidad asociada a la palabra.  \n",
    "        \"\"\"\n",
    "        if word.lower() in self.word_counter_20N.keys():\n",
    "            prob = self.word_counter_20N[word] / self.total_words\n",
    "        else:\n",
    "            prob = self.word_counter_20N[\"<UNK>\"] / self.total_words\n",
    "        return prob\n",
    "    def get_next_token(self) -> str:\n",
    "        \"\"\" Genera un token según las probabilidades unigramales.\"\"\"\n",
    "        probabilities = [self.get_prob([self.token_of(k)]) for k in range(self.V)]\n",
    "        probs = [math.exp(p) for p in probabilities]\n",
    "        index = random.choices(range(self.V), weights=probs, k=1)[0]\n",
    "        return self.token_of(index)\n",
    "    \n",
    "    def generate_sentences(self, limit:int= 50) -> list[str]:\n",
    "        \"\"\" Genera una sentencia basado en unigramas\n",
    "\n",
    "        Args:\n",
    "            limit (int, optional): limite de palabras a predecir. Defaults to 50.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: sentencia en una lista de strings.\n",
    "        \"\"\"\n",
    "        sentence = [\"<s>\"]\n",
    "        for _ in range(limit):\n",
    "            token = self.get_next_token()\n",
    "            if token == \"</s>\":\n",
    "                break\n",
    "            sentence.append(token)    \n",
    "    def generate_bigrams(self, filename: str):\n",
    "        \"\"\"\n",
    "        Genera y guarda las probabilidades de todos los bigramas observados y no observados,\n",
    "        escribiéndolos progresivamente en un archivo JSONL legible.\n",
    "\n",
    "        Para cada par (i, j) en la matriz `self.V`, se obtiene el token correspondiente \n",
    "        mediante `self.token_of`, se calcula su probabilidad con `self.get_prob([w1, w2])` \n",
    "        y se escribe directamente en el archivo.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida (.jsonl) donde se guardarán \n",
    "                            las probabilidades de los bigramas.\n",
    "\n",
    "        Efectos:\n",
    "            Crea un archivo JSONL en PATH_FINAL_FILES/filename donde cada línea \n",
    "            tiene la forma:\n",
    "            {\"w1\": \"...\", \"w2\": \"...\", \"probabilidad\": ...}\n",
    "        \"\"\"\n",
    "\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            iterator = tqdm(\n",
    "                [(i, j) for i in range(self.V) for j in range(self.V)],\n",
    "                desc=\"Generando bigramas\",\n",
    "                unit=\"bigrama\",\n",
    "                total=self.V * self.V,\n",
    "                leave=True,\n",
    "                file=sys.stdout\n",
    "            )\n",
    "\n",
    "            for i, j in iterator:\n",
    "                w1 = self.token_of(i)\n",
    "                w2 = self.token_of(j)\n",
    "                prob = self.get_prob([w1, w2])\n",
    "\n",
    "                record = {\"w1\": w1, \"w2\": w2, \"probabilidad\": prob}\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "        sentence.append(\"</s>\")\n",
    "        return \" \".join(sentence)\n",
    "    def save_model(self, filename: str):\n",
    "        \"\"\"\n",
    "        Guarda el modelo de unigramas entrenado en un archivo `.pickle`.\n",
    "\n",
    "        El archivo contendrá:\n",
    "        - word_counter_20N: Diccionario de conteos de palabras.\n",
    "        - total_words: Número total de palabras en el corpus.\n",
    "        - V: Tamaño del vocabulario.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"word_counter_20N\": self.word_counter_20N,\n",
    "            \"total_words\": self.total_words,\n",
    "            \"V\": self.V\n",
    "        }\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(payload, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "class BigramModel:\n",
    "    \"\"\"Bigram Model\n",
    "    \"\"\"\n",
    "    def __init__(self, filename:str, file_is_training=True):\n",
    "        \"\"\"\n",
    "        Inicializador del modelo de bigramas.\n",
    "\n",
    "        En este paso se construyen varias estructuras necesarias:\n",
    "\n",
    "        - dictionary: objeto que permite mapear palabras ↔ tokens.\n",
    "        - V: número total de tokens en el corpus (palabras + caracteres especiales).\n",
    "        - matrix: diccionario que guarda el conteo de ocurrencias de los bigramas \n",
    "            observados. (La matriz completa sería inviable de almacenar).\n",
    "        - row_sums: para agilizar el cálculo de probabilidades se guarda, para cada \n",
    "            token, la suma total de sus ocurrencias como primer elemento en un bigrama. \n",
    "            De esta forma, el denominador de la probabilidad condicional ya está \n",
    "            precomputado y no es necesario recalcularlo en cada consulta.\n",
    "        \"\"\"\n",
    "\n",
    "        data = self._load_pickle(filename)\n",
    "\n",
    "        if file_is_training:\n",
    "            self.dictionary = Dictionary(data)\n",
    "            self.V = len(self.dictionary)\n",
    "\n",
    "            self.matrix = {}                  \n",
    "            self.row_sums = {}            \n",
    "\n",
    "            for sentence in data:\n",
    "                for i in range(len(sentence)):\n",
    "                    w_idx = self._word_index(sentence[i])\n",
    "                    if i < len(sentence) - 1:\n",
    "                        w_next_idx = self._word_index(sentence[i + 1])\n",
    "                        key = (w_idx, w_next_idx)\n",
    "                        self.matrix[key] = self.matrix.get(key, 0) + 1\n",
    "                        self.row_sums[w_idx] = self.row_sums.get(w_idx, 0) + 1\n",
    "\n",
    "        else:\n",
    "            self.dictionary = data[\"dictionary\"]\n",
    "            self.V = data[\"V\"]\n",
    "            self.matrix = dict(data[\"matrix\"])\n",
    "            self.row_sums = dict(data[\"row_sums\"])\n",
    "\n",
    "    def _word_index(self, word: str) -> int:\n",
    "        \"\"\"\n",
    "        Devuelve el ID asociado a un token. \n",
    "        Si el token no existe en el diccionario, se asigna el ID correspondiente de <UNK>.\n",
    "\n",
    "        Args:\n",
    "            word (str): Palabra o token cuyo ID se desea obtener.\n",
    "\n",
    "        Returns:\n",
    "            int: ID de la palabra o, en caso de no estar en el diccionario, \n",
    "                el ID de <UNK>.\n",
    "        \"\"\"\n",
    "        tid = self.dictionary.token2id.get(word)\n",
    "        if tid is None:\n",
    "            tid = self.dictionary.token2id[\"<UNK>\"]\n",
    "        return tid\n",
    "\n",
    "    def _load_pickle(self, filename: str):\n",
    "        \"\"\"Carga de un pickle\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del file\n",
    "\n",
    "        Returns:\n",
    "            _type_: Estructura que posea el pickle\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_model(self, filename: str):\n",
    "        \"\"\"Guarda el modelo, para no tener que\n",
    "        volver a recalcular.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del file en el cual se va a guardar el modelo\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"dictionary\": self.dictionary,\n",
    "            \"V\": self.V,\n",
    "            \"matrix\": dict(self.matrix),\n",
    "            \"row_sums\": dict(self.row_sums)\n",
    "        }\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(payload, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def token_of(self, idx: int) -> str:\n",
    "        \"\"\"Obtiene el token asociado a una ID\n",
    "\n",
    "        Args:\n",
    "            idx (int): ID del token\n",
    "\n",
    "        Returns:\n",
    "            str: Token asociado a la ID\n",
    "        \"\"\"\n",
    "        return self.dictionary.id2token.get(idx, \"<UNK>\")\n",
    "\n",
    "    def get_prob(self, words: list[str]) -> float:\n",
    "        \"\"\"Se obtiene la probabilidad de una lista palabras\n",
    "        [w1,w2]\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista de palabras sobre la que \n",
    "            se obtiene \n",
    "\n",
    "        Returns:\n",
    "            float: probabilidad de w1, w2\n",
    "        \"\"\"\n",
    "        m_i = self._word_index(words[0])\n",
    "        m_j = self._word_index(words[1])\n",
    "        c_bigram = self.matrix.get((m_i, m_j), 0)\n",
    "        row_sum  = self.row_sums.get(m_i, 0)\n",
    "        return np.log((c_bigram + 1) / (row_sum + self.V))\n",
    "\n",
    "    def generate_bigrams(self, filename: str):\n",
    "        \"\"\"\n",
    "        Genera y guarda SOLO las probabilidades de los bigramas OBSERVADOS\n",
    "        (claves en self.matrix), incluyendo aquellos que involucren <UNK>.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida (.jsonl).\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for (i, j), _count in self.matrix.items():\n",
    "                w1 = self.token_of(i)\n",
    "                w2 = self.token_of(j)\n",
    "                prob = self.get_prob([w1, w2])\n",
    "                record = {\"w1\": w1, \"w2\": w2, \"probabilidad\": prob}\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "    def get_next_token(self, words:list[str]) -> str:\n",
    "        \"\"\"\n",
    "        Predice el siguiente token a partir del modelo de bigramas.\n",
    "\n",
    "        A partir del token actual (`words[0]`), se calculan las probabilidades\n",
    "        de transición hacia todos los tokens del vocabulario. Luego, se elige \n",
    "        aleatoriamente un token ponderado por dichas probabilidades.\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista que contiene el token actual en la primera posición.\n",
    "\n",
    "        Returns:\n",
    "            str: El siguiente token predicho.\n",
    "        \"\"\"\n",
    "        probabilities = []\n",
    "        for k in range(self.V):\n",
    "            probabilities.append(self.get_prob([words[0], self.token_of(k)]))\n",
    "        probs = [math.exp(p) for p in probabilities] \n",
    "        index = random.choices(range(self.V), weights=probs, k=1)[0]\n",
    "        return self.token_of(index)\n",
    "    \n",
    "    def generate_sentences(self, words:list[str], limit= 50) -> list[str]:\n",
    "        \"\"\"\n",
    "        Genera una oración utilizando un modelo de bigramas.\n",
    "\n",
    "        La generación inicia a partir del token en `words[0]`. En cada paso, \n",
    "        se predice el siguiente token con `get_next_token`, se agrega a la \n",
    "        oración y se actualiza el contexto. El proceso termina al alcanzar el \n",
    "        token de fin de secuencia `</s>` o al superar el límite de tokens.\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista inicial con el token de partida en la \n",
    "                primera posición.\n",
    "            limit (int, optional): Número máximo de tokens a generar. \n",
    "                Por defecto 50.\n",
    "\n",
    "        Returns:\n",
    "            str: Oración generada.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        sentence = words[0]\n",
    "        predicted_token = self.get_next_token(words)\n",
    "        words[0] = predicted_token\n",
    "        while i != limit or predicted_token == \"</s>\":\n",
    "            predicted_token = self.get_next_token(words)\n",
    "            sentence += \" \" + predicted_token\n",
    "            words[0] = predicted_token\n",
    "            i+=1\n",
    "        return sentence\n",
    "    \n",
    "class TrigramModel:\n",
    "    \"\"\"\n",
    "    Inicializa el modelo de trigramas.\n",
    "\n",
    "    Si `file_is_training` es True, procesa los datos para construir el diccionario \n",
    "    y las estructuras necesarias para calcular probabilidades.  \n",
    "    Si es False, carga el modelo previamente entrenado desde un archivo `.pickle`.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Nombre del archivo de entrada.\n",
    "        file_is_training (bool, optional): \n",
    "            - True: procesa los datos desde cero.  \n",
    "            - False: carga un modelo ya procesado.  \n",
    "    \"\"\"\n",
    "    def __init__(self, filename, file_is_training=True):\n",
    "        data = self._load_pickle(filename)\n",
    "        if file_is_training:\n",
    "            self.dictionary = Dictionary(data)\n",
    "            self.dictionary.add_documents([[\"<UNK>\"]])\n",
    "            self.V = len(self.dictionary)\n",
    "            self.matrix_trigram = {}      \n",
    "            self.pair_sums = {}           \n",
    "            for sent in data:\n",
    "                ids = [self._word_index(w) for w in sent]\n",
    "                for t in range(len(ids) - 2):\n",
    "                    i, j, k = ids[t], ids[t+1], ids[t+2]\n",
    "                    key3 = (i, j, k)\n",
    "                    key2 = (i, j)\n",
    "                    self.matrix_trigram[key3] = self.matrix_trigram.get(key3, 0) + 1\n",
    "                    self.pair_sums[key2] = self.pair_sums.get(key2, 0) + 1\n",
    "        else:\n",
    "            self.dictionary     = data[\"dictionary\"]\n",
    "            self.V              = data[\"V\"]\n",
    "            self.matrix_trigram = dict(data[\"matrix_trigram\"])\n",
    "            self.pair_sums      = dict(data[\"pair_sums\"])\n",
    "\n",
    "    def _word_index(self, word: str) -> int:\n",
    "        \"\"\"\n",
    "        Devuelve el ID asociado a una palabra.\n",
    "\n",
    "        Si la palabra no existe en el diccionario, devuelve el ID de <UNK>.\n",
    "\n",
    "        Args:\n",
    "            word (str): Palabra a consultar.\n",
    "\n",
    "        Returns:\n",
    "            int: ID asociado a la palabra o al token <UNK>.\n",
    "        \"\"\"\n",
    "        tid = self.dictionary.token2id.get(word)\n",
    "        if tid is None:\n",
    "            tid = self.dictionary.token2id[\"<UNK>\"]\n",
    "        return tid\n",
    "\n",
    "    def _load_pickle(self, filename: str):\n",
    "        \"\"\"\n",
    "        Carga un archivo `.pickle` desde PATH_FINAL_FILES.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo a cargar.\n",
    "\n",
    "        Returns:\n",
    "            object: Contenido del pickle (corpus o modelo guardado).\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_model(self, filename: str):\n",
    "        \"\"\"\n",
    "        Guarda el modelo entrenado en un archivo `.pickle`.\n",
    "\n",
    "        El archivo incluye:\n",
    "        - Diccionario de tokens.\n",
    "        - Tamaño del vocabulario.\n",
    "        - Conteo de trigramas observados.\n",
    "        - Conteo de pares de tokens.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"dictionary\": self.dictionary,\n",
    "            \"V\": self.V,\n",
    "            \"matrix_trigram\": dict(self.matrix_trigram),\n",
    "            \"pair_sums\": dict(self.pair_sums),\n",
    "        }\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(payload, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def token_of(self, idx: int) -> str:\n",
    "        \"\"\"\n",
    "        Devuelve el token asociado a un ID.\n",
    "\n",
    "        Args:\n",
    "            idx (int): ID del token.\n",
    "\n",
    "        Returns:\n",
    "            str: Token correspondiente o <UNK> si no existe.\n",
    "        \"\"\"\n",
    "        return self.dictionary.id2token.get(idx, \"<UNK>\")\n",
    "\n",
    "    def get_prob(self, words: list[str]) -> float:\n",
    "        \"\"\"\n",
    "        Calcula la probabilidad de un trigrama.\n",
    "\n",
    "        Usa la fórmula:\n",
    "            P(w_k | w_i, w_j) = (conteo(i, j, k)) / (conteo(i, j) + V)\n",
    "\n",
    "        con suavizado de Laplace.\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista con tres tokens [w_i, w_j, w_k].\n",
    "\n",
    "        Returns:\n",
    "            float: Probabilidad logarítmica del trigrama.\n",
    "        \"\"\"\n",
    "        i = self._word_index(words[0])\n",
    "        j = self._word_index(words[1])\n",
    "        k = self._word_index(words[2])\n",
    "        V = self.V\n",
    "        c_ijk = self.matrix_trigram.get((i, j, k), 0)\n",
    "        denom = self.pair_sums.get((i, j), 0)\n",
    "        return float(np.log((c_ijk + 1) / (denom + V)))\n",
    "    def generate_trigrams(self, filename: str):\n",
    "        \"\"\"\n",
    "        Guarda SOLO trigramas OBSERVADOS (claves de self.matrix_trigram),\n",
    "        incluyendo aquellos que involucren <UNK>.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida (.jsonl).\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for (i, j, k), _count in self.matrix_trigram.items():\n",
    "                w1, w2, w3 = self.token_of(i), self.token_of(j), self.token_of(k)\n",
    "                prob = self.get_prob([w1, w2, w3])\n",
    "                record = {\"w1\": w1, \"w2\": w2, \"w3\": w3, \"probabilidad\": prob}\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    def get_next_token(self, words:list[str]) -> str:\n",
    "        \"\"\"\n",
    "        Predice el siguiente token a partir del modelo de trigramas.\n",
    "\n",
    "        A partir del par actual `(words[0], words[1])`, calcula las probabilidades\n",
    "        de transición hacia todos los tokens del vocabulario y selecciona \n",
    "        aleatoriamente el siguiente token según dichas probabilidades.\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista de dos tokens que sirven como contexto.\n",
    "\n",
    "        Returns:\n",
    "            str: El siguiente token predicho.\n",
    "        \"\"\"\n",
    "        probabilities = []\n",
    "        if (words[0], words[1]) in self.pair_sums:\n",
    "            for k in range(self.V):\n",
    "                probabilities.append(self.get_prob([words[0], words[1], self.token_of(k)]))\n",
    "        probs = [math.exp(p) for p in probabilities] \n",
    "        return random.choices(range(self.V), weights=probs, k=1)[0]\n",
    "    def generate_sentences(self, words:list[str], limit= 50) -> list[str]:\n",
    "        \"\"\"\n",
    "        Genera una oración utilizando un modelo de trigramas.\n",
    "\n",
    "        La generación comienza con dos palabras iniciales (`words[0]` y `words[1]`).  \n",
    "        En cada paso se predice el siguiente token con `get_next_token`, se añade \n",
    "        a la oración y se actualiza el contexto.  \n",
    "        El proceso se detiene al alcanzar el token de fin de secuencia `</s>` \n",
    "        o al llegar al número máximo de tokens (`limit`).\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista inicial con dos tokens de contexto.\n",
    "            limit (int, optional): Número máximo de tokens generados. \n",
    "                Por defecto 50.\n",
    "\n",
    "        Returns:\n",
    "            str: Oración generada.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        sentence = \" \".join(words)\n",
    "        predicted_token = self.get_next_token(words)\n",
    "        words[0] = words[1]\n",
    "        words[1] = predicted_token\n",
    "        while i != limit or predicted_token == \"<s>\":\n",
    "            predicted_token = self.get_next_token(words)\n",
    "            sentence += \" \" + predicted_token\n",
    "            words[0] = words[1]\n",
    "            words[1] = predicted_token\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b0bb5b",
   "metadata": {},
   "source": [
    "## Funciones para calcular perplejidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cdde122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def perplexity_unigram(model: UnigramModel, filename:str) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad de un modelo de unigramas sobre un corpus de prueba.  \n",
    "\n",
    "    La perplejidad mide qué tan bien el modelo predice un conjunto de oraciones.  \n",
    "    Se calcula como:  \n",
    "\n",
    "        PP = exp( - (1/T) * Σ log P(w_i) )  \n",
    "\n",
    "    donde T es el número total de palabras en el corpus.  \n",
    "\n",
    "    Args:\n",
    "        model (UnigramModel): Modelo de unigramas sobre el que se evalúa.  \n",
    "        filename (str): Nombre del archivo de prueba (.pickle) que contiene las oraciones.  \n",
    "\n",
    "    Returns:\n",
    "        float: Valor de la perplejidad.  \n",
    "                Devuelve `inf` si alguna palabra tiene probabilidad 0 o si el corpus está vacío.  \n",
    "    \"\"\"\n",
    "    sentences = model.get_pickle(filename)\n",
    "    log_sum = 0.0\n",
    "    T = 0\n",
    "    for s in sentences:\n",
    "        for w in s:\n",
    "            p = model.get_prob(w)\n",
    "            if p == 0.0:\n",
    "                return float(\"inf\")\n",
    "            log_sum += math.log(p)\n",
    "            T += 1\n",
    "    return math.exp(-log_sum / T) if T else float(\"inf\")\n",
    "\n",
    "def perplexity_bigram_from_model(model:BigramModel, filename:str):\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad de un modelo de bigramas sobre un corpus de prueba.  \n",
    "\n",
    "    La perplejidad mide la capacidad del modelo para predecir secuencias de palabras:  \n",
    "\n",
    "        PP = exp( - (1/T) * Σ log P(w_i | w_{i-1}) )  \n",
    "\n",
    "    Args:\n",
    "        model (BigramModel): Modelo de bigramas sobre el que se evalúa.  \n",
    "        filename (str): Nombre del archivo de prueba (.pickle) que contiene las sentencias.  \n",
    "\n",
    "    Returns:\n",
    "        float: Valor de la perplejidad.  \n",
    "                Devuelve `inf` si el corpus está vacío.  \n",
    "    \"\"\"\n",
    "    sentences = model._load_pickle(filename)\n",
    "    log_sum = 0.0\n",
    "    T = 0\n",
    "    for s in sentences:\n",
    "        for prev, cur in zip(s[:-1], s[1:]):\n",
    "            log_p = model.get_prob([prev, cur])  \n",
    "            log_sum += log_p\n",
    "            T += 1\n",
    "    return math.exp(-log_sum / T) if T else float(\"inf\")\n",
    "\n",
    "def perplexity_trigram_from_model(model:TrigramModel, filename:str):\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad de un modelo de trigramas sobre un corpus de prueba.  \n",
    "\n",
    "    La perplejidad se calcula como:  \n",
    "\n",
    "        PP = exp( - (1/T) * Σ log P(w_i | w_{i-2}, w_{i-1}) )  \n",
    "\n",
    "    Args:\n",
    "        model (TrigramModel): Modelo de trigramas sobre el que se evalúa.  \n",
    "        filename (str): Nombre del archivo de prueba (.pickle) que contiene las oraciones.  \n",
    "\n",
    "    Returns:\n",
    "        float: Valor de la perplejidad.  \n",
    "                Devuelve `inf` si el corpus está vacío.  \n",
    "    \"\"\"\n",
    "    sentences = model._load_pickle(filename)\n",
    "    log_sum = 0.0\n",
    "    T = 0\n",
    "    for s in sentences:\n",
    "        for prev_2, prev_1 ,cur in zip(s[:-2], s[1:-1], s[2:]):\n",
    "            log_p = model.get_prob([prev_2, prev_1, cur])  \n",
    "            log_sum += log_p\n",
    "            T += 1\n",
    "    return math.exp(-log_sum / T) if T else float(\"inf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4efde0",
   "metadata": {},
   "source": [
    "## Carga de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e051a7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20N_Erich_Carlos_unigram_model.pkl\n",
      "BAC_Erich_Carlos_unigram_model.pkl\n"
     ]
    }
   ],
   "source": [
    "modelo_de_unigramas_20N = UnigramModel(f\"20N_{GRUPO}_unigram_model.pkl\", file_is_training=False)\n",
    "modelo_de_unigramas_BAC = UnigramModel(f\"BAC_{GRUPO}_unigram_model.pkl\", file_is_training=False)\n",
    "\n",
    "modelo_de_bigramas_20N = BigramModel(f\"20N_{GRUPO}_bigram_model.pkl\", file_is_training=False)\n",
    "modelo_de_bigramas_BAC = BigramModel(f\"BAC_{GRUPO}_bigram_model.pkl\", file_is_training=False)\n",
    "\n",
    "modelo_de_trigramas_20N = TrigramModel(f\"20N_{GRUPO}_trigram_model.pkl\", file_is_training=False)\n",
    "modelo_de_trigramas_BAC = TrigramModel(f\"BAC_{GRUPO}_trigram_model.pkl\", file_is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebbc615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate \n",
    "\n",
    "stats = [\n",
    "    [\"BAC - Unigram\", perplexity_unigram(modelo_de_unigramas_BAC, f\"BAC_{GRUPO}_testing.pkl\")],\n",
    "    [\"20N - Unigram\", perplexity_unigram(modelo_de_unigramas_20N, f\"20N_{GRUPO}_testing.pkl\")],\n",
    "    [\"20N - Bigram\", perplexity_bigram_from_model(modelo_de_bigramas_20N, f\"20N_{GRUPO}_testing.pkl\")],\n",
    "    [\"BAC - Bigram\", perplexity_bigram_from_model(modelo_de_bigramas_BAC, f\"BAC_{GRUPO}_testing.pkl\")],\n",
    "    [\"20N - Trigram\", perplexity_trigram_from_model(modelo_de_trigramas_20N, f\"20N_{GRUPO}_testing.pkl\")],\n",
    "    [\"BAC - Trigram\", perplexity_trigram_from_model(modelo_de_trigramas_BAC, f\"BAC_{GRUPO}_testing.pkl\")],\n",
    "]\n",
    "\n",
    "print(tabulate(stats, headers=[\"Modelo\", \"Perplejidad\"], tablefmt=\"fancy_grid\", floatfmt=\".4f\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
