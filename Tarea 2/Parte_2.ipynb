{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6621d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from charset_normalizer import from_path\n",
    "from gensim.corpora import Dictionary\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "Se importan las librerias que se necesiten, \n",
    "si se quiere ejecutar el notebook, se recomienda crear la carpeta de data, y poner ahi los files como se describe\n",
    "\n",
    "\"\"\"\n",
    "ACTUAL_PATH = os.getcwd()\n",
    "# Donde esta el 20 News\n",
    "PATH_20N = os.path.join(ACTUAL_PATH, \"data/20news-18828\")\n",
    "# Donde se encuentra el BAC\n",
    "PATH_BAC = os.path.join(ACTUAL_PATH, \"data/BAC/blogs\")\n",
    "# Donde se van a guardar los files que se van obteniendo\n",
    "\n",
    "# IMPORTANTE: Los files de https://uniandes-my.sharepoint.com/:f:/g/personal/eg_soto_uniandes_edu_co/Ep4A2ReC4jNGpSyFcqflY_YBVJdekMnu7W755IMhpI33dw?e=9A6Ese, tienen que ir aca en esta direccion.\n",
    "\n",
    "PATH_FINAL_FILES = os.path.join(ACTUAL_PATH, \"data/final_files\")\n",
    "# Numero de grupo (realmente como no hay pues simplemente se pusimos nuestros nombres)\n",
    "GRUPO = \"Erich_Carlos\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717f9467",
   "metadata": {},
   "source": [
    "## Clases para los n gramas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "    \"\"\"\n",
    "    Modelo de unigramas,\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename: str, file_is_training=True):\n",
    "        \"\"\"Makes the diccionary that the model needs to work,\n",
    "        ge\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo a procesar\n",
    "            file_is_training (bool, optional):\n",
    "                Indica cómo manejar el archivo de entrada.\n",
    "                - Si es False, se carga el objeto ya procesado desde un archivo `.pickle`.\n",
    "                - Si es True, el archivo se procesa desde cero.\n",
    "\n",
    "        \"\"\"\n",
    "        print(filename)\n",
    "        file = self.get_pickle(filename)\n",
    "        if file_is_training:\n",
    "            self.word_counter_20N = {}\n",
    "            for sentence in file:\n",
    "                for word in sentence:\n",
    "                    self.word_counter_20N[word] = self.word_counter_20N.get(word, 0) + 1\n",
    "            self.total_words = sum(self.word_counter_20N.values())\n",
    "            self.V = len(self.word_counter_20N)\n",
    "        else:\n",
    "            self.word_counter_20N = file[\"word_counter_20N\"]\n",
    "            self.total_words = file[\"total_words\"]\n",
    "            self.V = file[\"V\"]\n",
    "\n",
    "        self.total_words = sum(self.word_counter_20N.values())\n",
    "        self.V = len(self.word_counter_20N)\n",
    "\n",
    "    def get_pickle(self, filename: str):\n",
    "        \"\"\"\n",
    "        Abre un file en formato .pickle,\n",
    "        dentro de PATH_FINAL_FILES y lo devuelve.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del file a abrir\n",
    "\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            sentences = pickle.load(f)\n",
    "        return sentences\n",
    "\n",
    "    def generate_unigrams(self, filename: str):\n",
    "        \"\"\"Genera los unigramas en un archivo (jsonl) es lo que se\n",
    "        espera\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for word in self.word_counter_20N.keys():\n",
    "                prob = self.get_prob(word)\n",
    "                f.write(json.dumps({\"word\": word, \"probability\": prob}) + \"\\n\")\n",
    "\n",
    "    def get_prob(self, word: str) -> float:\n",
    "        \"\"\"\n",
    "        Calcula la probabilidad de un unigrama.\n",
    "        Si la palabra existe en el vocabulario V, se devuelve su probabilidad.\n",
    "        En caso contrario, se asigna al token <UNK>.\n",
    "\n",
    "        Args:\n",
    "            word (str): Palabra a consultar.\n",
    "\n",
    "        Returns:\n",
    "            float: Probabilidad asociada a la palabra.\n",
    "        \"\"\"\n",
    "        if word.lower() in self.word_counter_20N.keys():\n",
    "            prob = self.word_counter_20N[word] / self.total_words\n",
    "        else:\n",
    "            prob = self.word_counter_20N[\"<UNK>\"] / self.total_words\n",
    "        return prob\n",
    "\n",
    "    def get_next_token(self) -> str:\n",
    "        \"\"\"Genera un token según las probabilidades unigramales.\"\"\"\n",
    "        probabilities = [self.get_prob([self.token_of(k)]) for k in range(self.V)]\n",
    "        probs = [math.exp(p) for p in probabilities]\n",
    "        index = random.choices(range(self.V), weights=probs, k=1)[0]\n",
    "        return self.token_of(index)\n",
    "\n",
    "    def generate_sentences(self, limit: int = 50) -> list[str]:\n",
    "        \"\"\"Genera una sentencia basado en unigramas\n",
    "\n",
    "        Args:\n",
    "            limit (int, optional): limite de palabras a predecir. Defaults to 50.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: sentencia en una lista de strings.\n",
    "        \"\"\"\n",
    "        sentence = [\"<s>\"]\n",
    "        for _ in range(limit):\n",
    "            token = self.get_next_token()\n",
    "            if token == \"</s>\":\n",
    "                break\n",
    "            sentence.append(token)\n",
    "        sentence.append(\"</s>\")\n",
    "        return \" \".join(sentence)\n",
    "\n",
    "    def save_model(self, filename: str):\n",
    "        \"\"\"\n",
    "        Guarda el modelo de unigramas entrenado en un archivo `.pickle`.\n",
    "\n",
    "        El archivo contendrá:\n",
    "        - word_counter_20N: Diccionario de conteos de palabras.\n",
    "        - total_words: Número total de palabras en el corpus.\n",
    "        - V: Tamaño del vocabulario.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"word_counter_20N\": self.word_counter_20N,\n",
    "            \"total_words\": self.total_words,\n",
    "            \"V\": self.V,\n",
    "        }\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(payload, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "class BigramModel:\n",
    "    \"\"\"Bigram Model\"\"\"\n",
    "\n",
    "    def __init__(self, filename: str, file_is_training=True):\n",
    "        \"\"\"\n",
    "        Inicializador del modelo de bigramas.\n",
    "\n",
    "        En este paso se construyen varias estructuras necesarias:\n",
    "\n",
    "        - dictionary: objeto que permite mapear palabras ↔ tokens.\n",
    "        - V: número total de tokens en el corpus (palabras + caracteres especiales).\n",
    "        - matrix: diccionario que guarda el conteo de ocurrencias de los bigramas\n",
    "            observados. (La matriz completa sería inviable de almacenar).\n",
    "        - row_sums: para agilizar el cálculo de probabilidades se guarda, para cada\n",
    "            token, la suma total de sus ocurrencias como primer elemento en un bigrama.\n",
    "            De esta forma, el denominador de la probabilidad condicional ya está\n",
    "            precomputado y no es necesario recalcularlo en cada consulta.\n",
    "        \"\"\"\n",
    "\n",
    "        data = self._load_pickle(filename)\n",
    "\n",
    "        if file_is_training:\n",
    "            self.dictionary = Dictionary(data)\n",
    "            self.V = len(self.dictionary)\n",
    "\n",
    "            self.matrix = {}\n",
    "            self.row_sums = {}\n",
    "\n",
    "            for sentence in data:\n",
    "                for i in range(len(sentence)):\n",
    "                    w_idx = self._word_index(sentence[i])\n",
    "                    if i < len(sentence) - 1:\n",
    "                        w_next_idx = self._word_index(sentence[i + 1])\n",
    "                        key = (w_idx, w_next_idx)\n",
    "                        self.matrix[key] = self.matrix.get(key, 0) + 1\n",
    "                        self.row_sums[w_idx] = self.row_sums.get(w_idx, 0) + 1\n",
    "        else:\n",
    "            self.dictionary = data[\"dictionary\"]\n",
    "            self.matrix = dict(data[\"matrix\"])\n",
    "            self.row_sums = dict(data[\"row_sums\"])\n",
    "\n",
    "            if not hasattr(self.dictionary, \"id2token\") or not self.dictionary.id2token:\n",
    "                self.dictionary.id2token = {\n",
    "                    i: t for t, i in self.dictionary.token2id.items()\n",
    "                }\n",
    "\n",
    "            self.V = len(self.dictionary.token2id)\n",
    "\n",
    "    def _word_index(self, word: str) -> int:\n",
    "        \"\"\"\n",
    "        Devuelve el ID asociado a un token.\n",
    "        Si el token no existe en el diccionario, se asigna el ID correspondiente de <UNK>.\n",
    "\n",
    "        Args:\n",
    "            word (str): Palabra o token cuyo ID se desea obtener.\n",
    "\n",
    "        Returns:\n",
    "            int: ID de la palabra o, en caso de no estar en el diccionario,\n",
    "                el ID de <UNK>.\n",
    "        \"\"\"\n",
    "        tid = self.dictionary.token2id.get(word)\n",
    "        if tid is None:\n",
    "            tid = self.dictionary.token2id[\"<UNK>\"]\n",
    "        return tid\n",
    "\n",
    "    def _load_pickle(self, filename: str):\n",
    "        \"\"\"Carga de un pickle\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del file\n",
    "\n",
    "        Returns:\n",
    "            _type_: Estructura que posea el pickle\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_model(self, filename: str):\n",
    "        \"\"\"Guarda el modelo, para no tener que\n",
    "        volver a recalcular.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del file en el cual se va a guardar el modelo\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"dictionary\": self.dictionary,\n",
    "            \"V\": self.V,\n",
    "            \"matrix\": dict(self.matrix),\n",
    "            \"row_sums\": dict(self.row_sums),\n",
    "        }\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(payload, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def token_of(self, idx: int) -> str:\n",
    "        \"\"\"\n",
    "        Devuelve el token asociado a un ID.\n",
    "\n",
    "        Args:\n",
    "            idx (int): ID del token.\n",
    "\n",
    "        Returns:\n",
    "            str: Token correspondiente o <UNK> si no existe.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.dictionary[idx]\n",
    "        except KeyError:\n",
    "            return \"<UNK>\"\n",
    "\n",
    "    def get_prob(self, words: list[str]) -> float:\n",
    "        \"\"\"Se obtiene la probabilidad de una lista palabras\n",
    "        [w1,w2]\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista de palabras sobre la que\n",
    "            se obtiene\n",
    "\n",
    "        Returns:\n",
    "            float: probabilidad de w1, w2\n",
    "        \"\"\"\n",
    "        m_i = self._word_index(words[0])\n",
    "        m_j = self._word_index(words[1])\n",
    "        c_bigram = self.matrix.get((m_i, m_j), 0)\n",
    "        row_sum = self.row_sums.get(m_i, 0)\n",
    "        return np.log((c_bigram + 1) / (row_sum + self.V))\n",
    "\n",
    "    def generate_bigrams(self, filename: str):\n",
    "        \"\"\"\n",
    "        Genera y guarda SOLO las probabilidades de los bigramas OBSERVADOS\n",
    "        (claves en self.matrix), incluyendo aquellos que involucren <UNK>.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida (.jsonl).\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for (i, j), _count in self.matrix.items():\n",
    "                w1 = self.token_of(i)\n",
    "                w2 = self.token_of(j)\n",
    "                prob = self.get_prob([w1, w2])\n",
    "                record = {\"w1\": w1, \"w2\": w2, \"probabilidad\": prob}\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    def get_next_token(self, words: list[str], top_k: int = 10) -> str:\n",
    "        \"\"\"\n",
    "        Predice el siguiente token a partir del modelo de bigramas.\n",
    "\n",
    "        A partir del token actual `words[0]`, calcula las probabilidades\n",
    "        de transición hacia todos los tokens del vocabulario y selecciona\n",
    "        aleatoriamente el siguiente token entre los `top_k` más probables.\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista de un solo token que sirve como contexto.\n",
    "            top_k (int): Número de candidatos más probables a considerar.\n",
    "\n",
    "        Returns:\n",
    "            str: El siguiente token predicho.\n",
    "        \"\"\"\n",
    "        prev = words[0]\n",
    "\n",
    "        probabilities = []\n",
    "        for k in range(self.V):\n",
    "            lp = self.get_prob([prev, self.token_of(k)])\n",
    "            probabilities.append((k, lp))\n",
    "\n",
    "        topk = sorted(probabilities, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "        indices, lps = zip(*topk)\n",
    "        probs = [math.exp(lp) for lp in lps]\n",
    "\n",
    "        chosen_idx = random.choices(indices, weights=probs, k=1)[0]\n",
    "        return self.token_of(chosen_idx)\n",
    "\n",
    "    def generate_sentences(\n",
    "        self, words: list[str], limit: int = 50, top_k: int = 10\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Genera una oración usando el modelo de bigramas.\n",
    "        Se detiene al alcanzar </s> o el límite de tokens.\n",
    "        \"\"\"\n",
    "        current = words[0]\n",
    "        out = []\n",
    "        out.append(words[0])\n",
    "\n",
    "        for _ in range(limit):\n",
    "            nxt = self.get_next_token([current], top_k=top_k)\n",
    "            if nxt == \"</s>\":\n",
    "                break\n",
    "            out.append(nxt)\n",
    "            current = nxt\n",
    "\n",
    "        return \" \".join(out)\n",
    "\n",
    "\n",
    "class TrigramModel:\n",
    "    \"\"\"\n",
    "    Inicializa el modelo de trigramas.\n",
    "\n",
    "    Si `file_is_training` es True, procesa los datos para construir el diccionario\n",
    "    y las estructuras necesarias para calcular probabilidades.\n",
    "    Si es False, carga el modelo previamente entrenado desde un archivo `.pickle`.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Nombre del archivo de entrada.\n",
    "        file_is_training (bool, optional):\n",
    "            - True: procesa los datos desde cero.\n",
    "            - False: carga un modelo ya procesado.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename, file_is_training=True):\n",
    "        data = self._load_pickle(filename)\n",
    "        if file_is_training:\n",
    "            self.dictionary = Dictionary(data)\n",
    "            self.dictionary.add_documents([[\"<UNK>\"]])\n",
    "            self.V = len(self.dictionary)\n",
    "            self.matrix_trigram = {}\n",
    "            self.pair_sums = {}\n",
    "            for sent in data:\n",
    "                ids = [self._word_index(w) for w in sent]\n",
    "                for t in range(len(ids) - 2):\n",
    "                    i, j, k = ids[t], ids[t + 1], ids[t + 2]\n",
    "                    key3 = (i, j, k)\n",
    "                    key2 = (i, j)\n",
    "                    self.matrix_trigram[key3] = self.matrix_trigram.get(key3, 0) + 1\n",
    "                    self.pair_sums[key2] = self.pair_sums.get(key2, 0) + 1\n",
    "        else:\n",
    "            self.dictionary = data[\"dictionary\"]\n",
    "            self.V = data[\"V\"]\n",
    "            self.matrix_trigram = dict(data[\"matrix_trigram\"])\n",
    "            self.pair_sums = dict(data[\"pair_sums\"])\n",
    "\n",
    "            if not hasattr(self.dictionary, \"id2token\") or not self.dictionary.id2token:\n",
    "                self.dictionary.id2token = {\n",
    "                    i: t for t, i in self.dictionary.token2id.items()\n",
    "                }\n",
    "\n",
    "            self.V = len(self.dictionary.token2id)\n",
    "\n",
    "    def _word_index(self, word: str) -> int:\n",
    "        \"\"\"\n",
    "        Devuelve el ID asociado a una palabra.\n",
    "\n",
    "        Si la palabra no existe en el diccionario, devuelve el ID de <UNK>.\n",
    "\n",
    "        Args:\n",
    "            word (str): Palabra a consultar.\n",
    "\n",
    "        Returns:\n",
    "            int: ID asociado a la palabra o al token <UNK>.\n",
    "        \"\"\"\n",
    "        tid = self.dictionary.token2id.get(word)\n",
    "        if tid is None:\n",
    "            tid = self.dictionary.token2id[\"<UNK>\"]\n",
    "        return tid\n",
    "\n",
    "    def _load_pickle(self, filename: str):\n",
    "        \"\"\"\n",
    "        Carga un archivo `.pickle` desde PATH_FINAL_FILES.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo a cargar.\n",
    "\n",
    "        Returns:\n",
    "            object: Contenido del pickle (corpus o modelo guardado).\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_model(self, filename: str):\n",
    "        \"\"\"\n",
    "        Guarda el modelo entrenado en un archivo `.pickle`.\n",
    "\n",
    "        El archivo incluye:\n",
    "        - Diccionario de tokens.\n",
    "        - Tamaño del vocabulario.\n",
    "        - Conteo de trigramas observados.\n",
    "        - Conteo de pares de tokens.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"dictionary\": self.dictionary,\n",
    "            \"V\": self.V,\n",
    "            \"matrix_trigram\": dict(self.matrix_trigram),\n",
    "            \"pair_sums\": dict(self.pair_sums),\n",
    "        }\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(payload, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def token_of(self, idx: int) -> str:\n",
    "        \"\"\"\n",
    "        Devuelve el token asociado a un ID.\n",
    "\n",
    "        Args:\n",
    "            idx (int): ID del token.\n",
    "\n",
    "        Returns:\n",
    "            str: Token correspondiente o <UNK> si no existe.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.dictionary[idx]\n",
    "        except KeyError:\n",
    "            return \"<UNK>\"\n",
    "\n",
    "    def get_prob(self, words: list[str]) -> float:\n",
    "        \"\"\"\n",
    "                Calcula la probabilidad de un trigrama.\n",
    "\n",
    "                Usa la fórmula:\n",
    "                    P(w_k | w_i, w_j) = (conteo(i, j, k)) / (conteo(i, j) + V)\n",
    "\n",
    "                con suavizado de Laplace.\n",
    "        top_k\n",
    "                Args:\n",
    "                    words (list[str]): Lista con tres tokens [w_i, w_j, w_k].\n",
    "\n",
    "                Returns:\n",
    "                    float: Probabilidad logarítmica del trigrama.\n",
    "        \"\"\"\n",
    "        i = self._word_index(words[0])\n",
    "        j = self._word_index(words[1])\n",
    "        k = self._word_index(words[2])\n",
    "        V = self.V\n",
    "        c_ijk = self.matrix_trigram.get((i, j, k), 0)\n",
    "        denom = self.pair_sums.get((i, j), 0)\n",
    "        return float(np.log((c_ijk + 1) / (denom + V)))\n",
    "\n",
    "    def generate_trigrams(self, filename: str):\n",
    "        \"\"\"\n",
    "        Guarda SOLO trigramas OBSERVADOS (claves de self.matrix_trigram),\n",
    "        incluyendo aquellos que involucren <UNK>.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida (.jsonl).\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for (i, j, k), _count in self.matrix_trigram.items():\n",
    "                w1, w2, w3 = self.token_of(i), self.token_of(j), self.token_of(k)\n",
    "                prob = self.get_prob([w1, w2, w3])\n",
    "                record = {\"w1\": w1, \"w2\": w2, \"w3\": w3, \"probabilidad\": prob}\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    def get_next_token(self, words: list[str]) -> str:\n",
    "        \"\"\"\n",
    "        Predice el siguiente token a partir del modelo de trigramas.\n",
    "\n",
    "        A partir del par actual `(words[0], words[1])`, calcula las probabilidades\n",
    "        de transición hacia todos los tokens del vocabulario y selecciona\n",
    "        aleatoriamente el siguiente token según las 10 probabilidades más altas.\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista de dos tokens que sirven como contexto.\n",
    "\n",
    "        Returns:\n",
    "            str: El siguiente token predicho.\n",
    "        \"\"\"\n",
    "        if (words[0], words[1]) not in self.pair_sums:\n",
    "            return self.token_of(random.randint(0, self.V - 1))\n",
    "\n",
    "        probabilities = []\n",
    "        for k in range(self.V):\n",
    "            p = self.get_prob([words[0], words[1], self.token_of(k)])\n",
    "            probabilities.append((k, p))\n",
    "\n",
    "        top10 = sorted(probabilities, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        indices, probs = zip(*top10)\n",
    "        probs = [math.exp(p) for p in probs]\n",
    "\n",
    "        chosen_idx = random.choices(indices, weights=probs, k=1)[0]\n",
    "        return self.token_of(chosen_idx)\n",
    "\n",
    "    def generate_sentences(self, words: list[str], limit=50) -> list[str]:\n",
    "        \"\"\"\n",
    "        Genera una oración utilizando un modelo de trigramas.\n",
    "\n",
    "        La generación comienza con dos palabras iniciales (`words[0]` y `words[1]`).\n",
    "        En cada paso se predice el siguiente token con `get_next_token`, se añade\n",
    "        a la oración y se actualiza el contexto.\n",
    "        El proceso se detiene al alcanzar el token de fin de secuencia `</s>`\n",
    "        o al llegar al número máximo de tokens (`limit`).\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista inicial con dos tokens de contexto.\n",
    "            limit (int, optional): Número máximo de tokens generados.\n",
    "                Por defecto 50.\n",
    "\n",
    "        Returns:\n",
    "            str: Oración generada.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        sentence = \" \".join(words)\n",
    "        predicted_token = self.get_next_token(words)\n",
    "        words[0] = words[1]\n",
    "        words[1] = predicted_token\n",
    "        while i != limit or predicted_token == \"<s>\":\n",
    "            predicted_token = self.get_next_token(words)\n",
    "            sentence += \" \" + predicted_token\n",
    "            words[0] = words[1]\n",
    "            words[1] = predicted_token\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b0bb5b",
   "metadata": {},
   "source": [
    "## Funciones para calcular perplejidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdde122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def perplexity_unigram(model: UnigramModel, filename: str) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad de un modelo de unigramas sobre un corpus de prueba.\n",
    "\n",
    "    La perplejidad mide qué tan bien el modelo predice un conjunto de oraciones.\n",
    "    Se calcula como:\n",
    "\n",
    "        PP = exp( - (1/T) * Σ log P(w_i) )\n",
    "\n",
    "    donde T es el número total de palabras en el corpus.\n",
    "\n",
    "    Args:\n",
    "        model (UnigramModel): Modelo de unigramas sobre el que se evalúa.\n",
    "        filename (str): Nombre del archivo de prueba (.pickle) que contiene las oraciones.\n",
    "\n",
    "    Returns:\n",
    "        float: Valor de la perplejidad.\n",
    "                Devuelve `inf` si alguna palabra tiene probabilidad 0 o si el corpus está vacío.\n",
    "    \"\"\"\n",
    "    sentences = model.get_pickle(filename)\n",
    "    log_sum = 0.0\n",
    "    T = 0\n",
    "    for s in sentences:\n",
    "        for w in s:\n",
    "            p = model.get_prob(w)\n",
    "            if p == 0.0:\n",
    "                return float(\"inf\")\n",
    "            log_sum += math.log(p)\n",
    "            T += 1\n",
    "    return math.exp(-log_sum / T) if T else float(\"inf\")\n",
    "\n",
    "\n",
    "def perplexity_bigram_from_model(model: BigramModel, filename: str):\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad de un modelo de bigramas sobre un corpus de prueba.\n",
    "\n",
    "    La perplejidad mide la capacidad del modelo para predecir secuencias de palabras:\n",
    "\n",
    "        PP = exp( - (1/T) * Σ log P(w_i | w_{i-1}) )\n",
    "\n",
    "    Args:\n",
    "        model (BigramModel): Modelo de bigramas sobre el que se evalúa.\n",
    "        filename (str): Nombre del archivo de prueba (.pickle) que contiene las sentencias.\n",
    "\n",
    "    Returns:\n",
    "        float: Valor de la perplejidad.\n",
    "                Devuelve `inf` si el corpus está vacío.\n",
    "    \"\"\"\n",
    "    sentences = model._load_pickle(filename)\n",
    "    log_sum = 0.0\n",
    "    T = 0\n",
    "    for s in sentences:\n",
    "        for prev, cur in zip(s[:-1], s[1:]):\n",
    "            log_p = model.get_prob([prev, cur])\n",
    "            log_sum += log_p\n",
    "            T += 1\n",
    "    return math.exp(-log_sum / T) if T else float(\"inf\")\n",
    "\n",
    "\n",
    "def perplexity_trigram_from_model(model: TrigramModel, filename: str):\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad de un modelo de trigramas sobre un corpus de prueba.\n",
    "\n",
    "    La perplejidad se calcula como:\n",
    "\n",
    "        PP = exp( - (1/T) * Σ log P(w_i | w_{i-2}, w_{i-1}) )\n",
    "\n",
    "    Args:\n",
    "        model (TrigramModel): Modelo de trigramas sobre el que se evalúa.\n",
    "        filename (str): Nombre del archivo de prueba (.pickle) que contiene las oraciones.\n",
    "\n",
    "    Returns:\n",
    "        float: Valor de la perplejidad.\n",
    "                Devuelve `inf` si el corpus está vacío.\n",
    "    \"\"\"\n",
    "    sentences = model._load_pickle(filename)\n",
    "    log_sum = 0.0\n",
    "    T = 0\n",
    "    for s in sentences:\n",
    "        for prev_2, prev_1, cur in zip(s[:-2], s[1:-1], s[2:]):\n",
    "            log_p = model.get_prob([prev_2, prev_1, cur])\n",
    "            log_sum += log_p\n",
    "            T += 1\n",
    "    return math.exp(-log_sum / T) if T else float(\"inf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4efde0",
   "metadata": {},
   "source": [
    "## Carga de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051a7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20N_Erich_Carlos_unigram_model.pkl\n",
      "BAC_Erich_Carlos_unigram_model.pkl\n"
     ]
    }
   ],
   "source": [
    "modelo_de_unigramas_20N = UnigramModel(\n",
    "    f\"20N_{GRUPO}_unigram_model.pkl\", file_is_training=False\n",
    ")\n",
    "modelo_de_unigramas_BAC = UnigramModel(\n",
    "    f\"BAC_{GRUPO}_unigram_model.pkl\", file_is_training=False\n",
    ")\n",
    "\n",
    "modelo_de_bigramas_20N = BigramModel(\n",
    "    f\"20N_{GRUPO}_bigram_model.pkl\", file_is_training=False\n",
    ")\n",
    "modelo_de_bigramas_BAC = BigramModel(\n",
    "    f\"BAC_{GRUPO}_bigram_model.pkl\", file_is_training=False\n",
    ")\n",
    "\n",
    "modelo_de_trigramas_20N = TrigramModel(\n",
    "    f\"20N_{GRUPO}_trigram_model.pkl\", file_is_training=False\n",
    ")\n",
    "modelo_de_trigramas_BAC = TrigramModel(\n",
    "    f\"BAC_{GRUPO}_trigram_model.pkl\", file_is_training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430be803",
   "metadata": {},
   "source": [
    "## V. Using the test dataset, calculate the perplexity of each of the language models. Report the results obtained. If you experience variable overflow, use probabilities in log space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebbc615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════╤═══════════════╕\n",
      "│ Modelo        │   Perplejidad │\n",
      "╞═══════════════╪═══════════════╡\n",
      "│ BAC - Unigram │      538.0352 │\n",
      "├───────────────┼───────────────┤\n",
      "│ 20N - Unigram │      725.0064 │\n",
      "├───────────────┼───────────────┤\n",
      "│ 20N - Bigram  │     1474.5914 │\n",
      "├───────────────┼───────────────┤\n",
      "│ BAC - Bigram  │      512.2996 │\n",
      "├───────────────┼───────────────┤\n",
      "│ 20N - Trigram │    10611.2138 │\n",
      "├───────────────┼───────────────┤\n",
      "│ BAC - Trigram │     6125.9944 │\n",
      "╘═══════════════╧═══════════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "stats = [\n",
    "    [\n",
    "        \"BAC - Unigram\",\n",
    "        perplexity_unigram(modelo_de_unigramas_BAC, f\"BAC_{GRUPO}_testing.pkl\"),\n",
    "    ],\n",
    "    [\n",
    "        \"20N - Unigram\",\n",
    "        perplexity_unigram(modelo_de_unigramas_20N, f\"20N_{GRUPO}_testing.pkl\"),\n",
    "    ],\n",
    "    [\n",
    "        \"20N - Bigram\",\n",
    "        perplexity_bigram_from_model(\n",
    "            modelo_de_bigramas_20N, f\"20N_{GRUPO}_testing.pkl\"\n",
    "        ),\n",
    "    ],\n",
    "    [\n",
    "        \"BAC - Bigram\",\n",
    "        perplexity_bigram_from_model(\n",
    "            modelo_de_bigramas_BAC, f\"BAC_{GRUPO}_testing.pkl\"\n",
    "        ),\n",
    "    ],\n",
    "    [\n",
    "        \"20N - Trigram\",\n",
    "        perplexity_trigram_from_model(\n",
    "            modelo_de_trigramas_20N, f\"20N_{GRUPO}_testing.pkl\"\n",
    "        ),\n",
    "    ],\n",
    "    [\n",
    "        \"BAC - Trigram\",\n",
    "        perplexity_trigram_from_model(\n",
    "            modelo_de_trigramas_BAC, f\"BAC_{GRUPO}_testing.pkl\"\n",
    "        ),\n",
    "    ],\n",
    "]\n",
    "\n",
    "print(\n",
    "    tabulate(\n",
    "        stats, headers=[\"Modelo\", \"Perplejidad\"], tablefmt=\"fancy_grid\", floatfmt=\".4f\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9780859",
   "metadata": {},
   "source": [
    "## VI. Using your best language model, build a method/function that automatically generates sentences by receiving the first word of a sentence as input. Take different tests and document them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d008ba7",
   "metadata": {},
   "source": [
    "Para esta sección, se decidió experimentar seleccionando los k tokens con mayor probabilidad y, a partir de ellos, determinar el token a predecir. Además, se estableció un límite de 30 palabras por sentencia generada. Estos experimentos se ejecutarán con valores de k iguales a 1, 2, 3, 4 y 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5952101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase 1: do you can t know what i m not to the same time .\n",
      "Frase 2: do you can t know what i m not to the same time .\n",
      "Frase 3: do you can t know what i m not to the same time .\n",
      "Frase 4: do you can t know what i m not to the same time .\n",
      "Frase 5: do you can t know what i m not to the same time .\n",
      "Frase 6: do you can t know what i m not to the same time .\n",
      "Frase 7: do you can t know what i m not to the same time .\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    sentence = modelo_de_bigramas_BAC.generate_sentences([\"do\"], limit=30, top_k=1)\n",
    "    print(f\"Frase {i+1}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18cac12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase 1: do you can t have to be a little bit of the first time to the first time to the first time .\n",
      "Frase 2: do you re not to the same time to the same time .\n",
      "Frase 3: do you re going to the first , i m not a lot .\n",
      "Frase 4: do . . . . .\n",
      "Frase 5: do you re not a little bit of my life . .\n",
      "Frase 6: do .\n",
      "Frase 7: do you can t know what i m not a lot of the first time .\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    sentence = modelo_de_bigramas_BAC.generate_sentences([\"do\"], limit=30, top_k=2)\n",
    "    print(f\"Frase {i+1}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa540cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase 1: do you re not to be able to the same .\n",
      "Frase 2: do .\n",
      "Frase 3: do you re not to be a lot of the same time .\n",
      "Frase 4: do .\n",
      "Frase 5: do . .\n",
      "Frase 6: do you re going to be the first , and the way to be the way . .\n",
      "Frase 7: do you can t know .\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    sentence = modelo_de_bigramas_BAC.generate_sentences([\"do\"], limit=30, top_k=3)\n",
    "    print(f\"Frase {i+1}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28dfb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase 1: do you can be in my own .\n",
      "Frase 2: do not the first time .\n",
      "Frase 3: do it s the way i have a lot of the world . . . .\n",
      "Frase 4: do .\n",
      "Frase 5: do you can t know .\n",
      "Frase 6: do you know .\n",
      "Frase 7: do not to the world . . .\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    sentence = modelo_de_bigramas_BAC.generate_sentences([\"do\"], limit=30, top_k=4)\n",
    "    print(f\"Frase {i+1}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553fb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase 1: do you know what the way to be able to get the way i am i have the way .\n",
      "Frase 2: do .\n",
      "Frase 3: do not .\n",
      "Frase 4: do not a lot to be a little while i m not sure i m sure i am i can t have been a few months , the way to do\n",
      "Frase 5: do you can see it was a few minutes . . . .\n",
      "Frase 6: do .\n",
      "Frase 7: do you are a bit .\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    sentence = modelo_de_bigramas_BAC.generate_sentences([\"do\"], limit=30, top_k=5)\n",
    "    print(f\"Frase {i+1}: {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b86388",
   "metadata": {},
   "source": [
    "Si siempre se selecciona únicamente la palabra con la probabilidad más alta, las predicciones tienden a ser repetitivas y poco interesantes, ya que muchas frases terminan de la misma manera. Al ampliar la selección a las dos opciones más probables, aún persiste cierta repetitividad, aunque las frases comienzan a mostrar mayor variación en sus finales. A medida que se incrementa el valor de top-k, las oraciones adquieren un espacio más amplio para generar palabras distintas. Sin embargo, cuando se consideran todas las probabilidades (como ya se probó en un experimento), el resultado es incoherente y las frases pierden sentido. Por lo tanto, el rango óptimo no consiste en tomar solo la primera opción, pero tampoco en incluir todas las posibilidades."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
