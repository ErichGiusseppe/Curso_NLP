{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cfe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/erich/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from charset_normalizer import from_path\n",
    "from gensim.corpora import Dictionary\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "Se importan las librerias que se necesiten, \n",
    "si se quiere ejecutar el notebook, se recomienda crear la carpeta de data, y poner ahi los files como se describe\n",
    "\n",
    "\"\"\"\n",
    "ACTUAL_PATH = os.getcwd()\n",
    "# Donde esta el 20 News\n",
    "PATH_20N = os.path.join(ACTUAL_PATH, \"data/20news-18828\")\n",
    "# Donde se encuentra el BAC\n",
    "PATH_BAC = os.path.join(ACTUAL_PATH, \"data/BAC/blogs\")\n",
    "# Donde se van a guardar los files que se van obteniendo\n",
    "\n",
    "# IMPORTANTE: Los files de https://uniandes-my.sharepoint.com/:f:/g/personal/eg_soto_uniandes_edu_co/Ep4A2ReC4jNGpSyFcqflY_YBVJdekMnu7W755IMhpI33dw?e=9A6Ese, tienen que ir aca en esta direccion.\n",
    "PATH_FINAL_FILES = os.path.join(ACTUAL_PATH, \"data/final_files\")\n",
    "# Numero de grupo (realmente como no hay pues simplemente se pusimos nuestros nombres)\n",
    "GRUPO = \"Erich_Carlos\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d444fb",
   "metadata": {},
   "source": [
    "## I. Read the files and build two large consolidate files that are the union of all the documents in 20N and BAC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb246615",
   "metadata": {},
   "source": [
    "### UPLOAD_FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c7d5b",
   "metadata": {},
   "source": [
    "#### UPLOADING 20N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "789767b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Para esto se define en que formato y donde se quiere el archivo completo de 20N\"\"\"\n",
    "\n",
    "NEW_20N_FILE = os.path.join(PATH_FINAL_FILES, \"20N.jsonl\")\n",
    "\n",
    "mayor_folders_20N = os.listdir(PATH_20N)\n",
    "dictionary = {}\n",
    "\"\"\"\n",
    "Para cada archivo disponible en 20N,  \n",
    "se generan registros con:  \n",
    "- el ID del archivo  \n",
    "- el tema del archivo  \n",
    "- el texto del contenido  \n",
    "\n",
    "Todo se guarda en formato JSONL por cuestiones de formato.\n",
    "\"\"\"\n",
    "with open(NEW_20N_FILE, \"w\", encoding=\"utf-8\", errors=\"replace\") as f_n:\n",
    "    for folder in mayor_folders_20N:\n",
    "        minor_files_path = os.path.join(PATH_20N, folder)\n",
    "        minor_files = os.listdir(minor_files_path)\n",
    "        for file in minor_files:\n",
    "            file_path = os.path.join(minor_files_path, file)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "                text = f.read().strip()\n",
    "\n",
    "            record = {\"id\": file, \"theme\": folder, \"text\": text}\n",
    "            unit = folder + file\n",
    "            if file in dictionary.keys():\n",
    "                dictionary[unit] += 1\n",
    "            else:\n",
    "                dictionary[unit] = 1\n",
    "            f_n.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff5851",
   "metadata": {},
   "source": [
    "#### UPLOADING BAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55eae228",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m post \u001b[38;5;241m=\u001b[39m post\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</post>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m record \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_num\u001b[39m\u001b[38;5;124m\"\u001b[39m: post_num, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: post}\n\u001b[0;32m---> 24\u001b[0m \u001b[43mf_n\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m post_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Para cada archivo disponible en BAC,\n",
    "se generan registros con:\n",
    "- el ID del archivo\n",
    "- el tema del archivo\n",
    "- el texto del contenido\n",
    "\n",
    "Todo se guarda en formato JSONL por cuestiones de formato.\n",
    "\"\"\"\n",
    "\n",
    "NEW_BAC_FILE = os.path.join(PATH_FINAL_FILES, \"BAC.jsonl\")\n",
    "mayor_folders_BAC = os.listdir(PATH_BAC)\n",
    "with open(NEW_BAC_FILE, \"w\", encoding=\"utf-8\", errors=\"replace\") as f_n:\n",
    "    for file in mayor_folders_BAC:\n",
    "        post_num = 0\n",
    "        file_path = os.path.join(PATH_BAC, file)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            text = f.read().strip()\n",
    "            text = re.sub(r\"</?Blog>\", \"\", text)\n",
    "        post_list = text.split(\"<post>\")\n",
    "        for post in post_list:\n",
    "            post = post.strip().replace(\"</post>\", \"\")\n",
    "            record = {\"id\": file, \"post_num\": post_num, \"text\": post}\n",
    "            f_n.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "            post_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5688cb7",
   "metadata": {},
   "source": [
    "## II. & III. \n",
    "### Tokenize by sentence \n",
    "### & \n",
    "### Select 80% of the resulting sentences -random without replacement- to build the N-gram model and the remaining 20% for evaluation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a489088",
   "metadata": {},
   "source": [
    "### Funciones utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbbcfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"procesador de sentencias\n",
    "\n",
    "    Args:\n",
    "        text (str): sentencia a procesar\n",
    "\n",
    "    Returns:\n",
    "        list[str]: lista de palabras y simbolos a dejar.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,!?]\", \" \", text, flags=re.I | re.A | re.MULTILINE)\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\d+\", \"NUM\", text)\n",
    "    text = \"<s> \" + text + \" </s>\"\n",
    "    text = text.strip().split()\n",
    "    return text\n",
    "\n",
    "\n",
    "def divide_sentences(text: str) -> list[str]:\n",
    "    \"\"\"Divide los textos en sentencias\n",
    "\n",
    "    Args:\n",
    "        text (str): texto completo\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Lista completa de sentencias\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bea1e5",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4530297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome!',\n",
       " 'As soon as we can, Jon and I are going to try to get the class list from the registrar to invite the whole class, but in the meantime, tell everyone you know in YLS04 to check it out.',\n",
       " \"We'd conceived of this place as a way for the class to keep in touch, share legal experiences (working, clerking, keeping ourselves out of jail, etc.)\",\n",
       " 'and express opinions on everything and anything.',\n",
       " 'Basically, we want to entertain ourselves and each other.',\n",
       " \"Obviously, it will become whatever we all want it to be, and I can't wait to see what bizarre form it takes.\",\n",
       " \"If you'd like to post but haven't gotten a log-in, e-mail Jon or me, and we'll put it up for you or arrange a log-in.\",\n",
       " \"There are no rules per se, but as a general rule, if you're posting something that has to do with your work, obviously be aware of whatever privileges/ethical guidelines apply (the secrecy of a judge's chambers, attorney-client privilege, general theories of slander: i.e.\",\n",
       " '\"Tony Kronman smokes crack\" might cause us some problems).',\n",
       " 'Remember,  anyone  can see this blog (and I am sure the rest of the world is just dying to tune in to hear what YLS04 has to say), so be cautious.',\n",
       " 'But not if it means holding back something entertaining, of course.',\n",
       " 'First order of business -- what should we call it?',\n",
       " 'We were thinking of it as a virtual \"Wall\" -- with fewer (only slightly) personal attacks and more fun, but couldn\\'t think of a really good name.',\n",
       " \"So let's hear some suggestions.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Welcome! As soon as we can, Jon and I are going to try to get the class list from the registrar to invite the whole class, but in the meantime, tell everyone you know in YLS04 to check it out.   We'd conceived of this place as a way for the class to keep in touch, share legal experiences (working, clerking, keeping ourselves out of jail, etc.) and express opinions on everything and anything.  Basically, we want to entertain ourselves and each other.  Obviously, it will become whatever we all want it to be, and I can't wait to see what bizarre form it takes.  If you'd like to post but haven't gotten a log-in, e-mail Jon or me, and we'll put it up for you or arrange a log-in.  There are no rules per se, but as a general rule, if you're posting something that has to do with your work, obviously be aware of whatever privileges/ethical guidelines apply (the secrecy of a judge's chambers, attorney-client privilege, general theories of slander: i.e. \"Tony Kronman smokes crack\" might cause us some problems).  Remember,  anyone  can see this blog (and I am sure the rest of the world is just dying to tune in to hear what YLS04 has to say), so be cautious.  But not if it means holding back something entertaining, of course.  First order of business -- what should we call it?  We were thinking of it as a virtual \"Wall\" -- with fewer (only slightly) personal attacks and more fun, but couldn't think of a really good name.  So let's hear some suggestions.\"\"\"\n",
    "sentences = divide_sentences(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea6892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'welcome', '!', '</s>'], ['<s>', 'as', 'soon', 'as', 'we', 'can', ',', 'jon', 'and', 'i', 'are', 'going', 'to', 'try', 'to', 'get', 'the', 'class', 'list', 'from', 'the', 'registrar', 'to', 'invite', 'the', 'whole', 'class', ',', 'but', 'in', 'the', 'meantime', ',', 'tell', 'everyone', 'you', 'know', 'in', 'ylsNUM', 'to', 'check', 'it', 'out', '.', '</s>'], ['<s>', 'we', 'd', 'conceived', 'of', 'this', 'place', 'as', 'a', 'way', 'for', 'the', 'class', 'to', 'keep', 'in', 'touch', ',', 'share', 'legal', 'experiences', 'working', ',', 'clerking', ',', 'keeping', 'ourselves', 'out', 'of', 'jail', ',', 'etc', '.', '</s>'], ['<s>', 'and', 'express', 'opinions', 'on', 'everything', 'and', 'anything', '.', '</s>'], ['<s>', 'basically', ',', 'we', 'want', 'to', 'entertain', 'ourselves', 'and', 'each', 'other', '.', '</s>'], ['<s>', 'obviously', ',', 'it', 'will', 'become', 'whatever', 'we', 'all', 'want', 'it', 'to', 'be', ',', 'and', 'i', 'can', 't', 'wait', 'to', 'see', 'what', 'bizarre', 'form', 'it', 'takes', '.', '</s>'], ['<s>', 'if', 'you', 'd', 'like', 'to', 'post', 'but', 'haven', 't', 'gotten', 'a', 'log', 'in', ',', 'e', 'mail', 'jon', 'or', 'me', ',', 'and', 'we', 'll', 'put', 'it', 'up', 'for', 'you', 'or', 'arrange', 'a', 'log', 'in', '.', '</s>'], ['<s>', 'there', 'are', 'no', 'rules', 'per', 'se', ',', 'but', 'as', 'a', 'general', 'rule', ',', 'if', 'you', 're', 'posting', 'something', 'that', 'has', 'to', 'do', 'with', 'your', 'work', ',', 'obviously', 'be', 'aware', 'of', 'whatever', 'privileges', 'ethical', 'guidelines', 'apply', 'the', 'secrecy', 'of', 'a', 'judge', 's', 'chambers', ',', 'attorney', 'client', 'privilege', ',', 'general', 'theories', 'of', 'slander', 'i', '.', 'e', '.', '</s>'], ['<s>', 'tony', 'kronman', 'smokes', 'crack', 'might', 'cause', 'us', 'some', 'problems', '.', '</s>'], ['<s>', 'remember', ',', 'anyone', 'can', 'see', 'this', 'blog', 'and', 'i', 'am', 'sure', 'the', 'rest', 'of', 'the', 'world', 'is', 'just', 'dying', 'to', 'tune', 'in', 'to', 'hear', 'what', 'ylsNUM', 'has', 'to', 'say', ',', 'so', 'be', 'cautious', '.', '</s>'], ['<s>', 'but', 'not', 'if', 'it', 'means', 'holding', 'back', 'something', 'entertaining', ',', 'of', 'course', '.', '</s>'], ['<s>', 'first', 'order', 'of', 'business', 'what', 'should', 'we', 'call', 'it', '?', '</s>'], ['<s>', 'we', 'were', 'thinking', 'of', 'it', 'as', 'a', 'virtual', 'wall', 'with', 'fewer', 'only', 'slightly', 'personal', 'attacks', 'and', 'more', 'fun', ',', 'but', 'couldn', 't', 'think', 'of', 'a', 'really', 'good', 'name', '.', '</s>'], ['<s>', 'so', 'let', 's', 'hear', 'some', 'suggestions', '.', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "processed = [preprocess_text(s) for s in sentences]\n",
    "\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32526084",
   "metadata": {},
   "source": [
    "### Tokenizacion por sentencia y preprocesamiento de las mismas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7253a",
   "metadata": {},
   "source": [
    "Para cada archivo y para cada texto contenido en él:  \n",
    "1. Se divide el texto en oraciones y se normaliza (minúsculas, \n",
    "   manejo de números, limpieza de símbolos innecesarios, etc.).  \n",
    "2. Se agregan las etiquetas `<s>` y `</s>` para marcar el inicio y fin de cada oración.  \n",
    "3. Se construye un diccionario de tokens.  \n",
    "4. Todas las palabras con frecuencia igual a 1 se reemplazan por el token <unk>, \n",
    "   con el fin de reducir ruido y manejar vocabulario raro o desconocido.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ded1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(data, filename: str):\n",
    "    \"\"\"funcion para guardar una estructura en un .pickle\n",
    "\n",
    "    Args:\n",
    "        data (_type_): Estructura a guardar\n",
    "        filename (str): Nombre del archivo a guardar\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f6782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Diccionario para contar palabras y asi reemplazar despues por <UNK>\"\"\"\n",
    "\n",
    "word_count = {}\n",
    "with open(os.path.join(PATH_FINAL_FILES, \"20N.jsonl\"), \"r\") as f:\n",
    "    for line in f:\n",
    "        line = json.loads(line)\n",
    "        for word in preprocess_text(line[\"text\"]):\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "word_count = {word: count for word, count in word_count.items() if count == 1}\n",
    "\n",
    "\"\"\"\n",
    "Se itera sobre los textos de 20N, dividiendo en oraciones y aplicando preprocesamiento.  \n",
    "Cada oración se añade a una lista para su posterior uso.  \n",
    "Luego, se realiza una partición aleatoria en conjuntos de entrenamiento y prueba,  \n",
    "los cuales finalmente se guardan en archivos.\n",
    "\"\"\"\n",
    "sentencias = []\n",
    "with open(os.path.join(PATH_FINAL_FILES, \"20N.jsonl\"), \"r\") as f:\n",
    "    for line in f:\n",
    "        line = json.loads(line)\n",
    "        sentences = divide_sentences(line[\"text\"])\n",
    "        for sentence in sentences:\n",
    "            pre_process = preprocess_text(sentence)\n",
    "            tokens = [\n",
    "                word if word not in word_count else \"<UNK>\" for word in pre_process\n",
    "            ]\n",
    "            sentencias.append(tokens)\n",
    "            # print(sentence, pre_process)\n",
    "random.seed(42)\n",
    "random.shuffle(sentencias)\n",
    "index_to_split = int(0.8 * len(sentencias))\n",
    "train_sentences = sentencias[:index_to_split]\n",
    "test_sentences = sentencias[index_to_split:]\n",
    "save_pickle(train_sentences, f\"20N_{GRUPO}_training.pkl\")\n",
    "save_pickle(test_sentences, f\"20N_{GRUPO}_testing.pkl\")\n",
    "\n",
    "\"\"\" Se eliminan los archivos que no se necesitan por performance, (mi pc se ponia lenta).\n",
    "\"\"\"\n",
    "del sentencias\n",
    "del train_sentences\n",
    "del test_sentences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea5c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_lineas(path):\n",
    "    c = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for _ in f:\n",
    "            c += 1\n",
    "    return c\n",
    "\n",
    "\n",
    "file_path = os.path.join(PATH_FINAL_FILES, \"BAC.jsonl\")\n",
    "total_lineas = contar_lineas(file_path)\n",
    "\n",
    "\"\"\" Diccionario para contar palabras y asi reemplazar despues por <UNK>\n",
    "\"\"\"\n",
    "word_count = {}\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, total=total_lineas, desc=\"Contando palabras\", unit=\"línea\"):\n",
    "        line = json.loads(line)\n",
    "        for word in preprocess_text(line[\"text\"]):\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "\n",
    "word_count = {word: count for word, count in word_count.items() if count == 1}\n",
    "\"\"\"\n",
    "Se itera sobre los textos de BAC, dividiendo en oraciones y aplicando preprocesamiento.  \n",
    "Cada oración se añade a una lista para su posterior uso.  \n",
    "Luego, se realiza una partición aleatoria en conjuntos de entrenamiento y prueba,  \n",
    "los cuales finalmente se guardan en archivos.\n",
    "\"\"\"\n",
    "sentencias = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, total=total_lineas, desc=\"Procesando oraciones\", unit=\"línea\"):\n",
    "        line = json.loads(line)\n",
    "        sentences = divide_sentences(line[\"text\"])\n",
    "        for sentence in sentences:\n",
    "            pre_process = preprocess_text(sentence)\n",
    "            tokens = [\n",
    "                word if word not in word_count else \"<UNK>\" for word in pre_process\n",
    "            ]\n",
    "            sentencias.append(tokens)\n",
    "            # print(sentence, pre_process)\n",
    "random.seed(42)\n",
    "random.shuffle(sentencias)\n",
    "split_idx = int(0.8 * len(sentencias))\n",
    "train_sentences = sentencias[:split_idx]\n",
    "test_sentences = sentencias[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97145739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_pickle(train_sentences, f\"BAC_{GRUPO}_training.pkl\")\n",
    "save_pickle(test_sentences, f\"BAC_{GRUPO}_testing.pkl\")\n",
    "\n",
    "\"\"\" Se eliminan los archivos que no se necesitan por performance, (mi pc se ponia lenta).\n",
    "\"\"\"\n",
    "del sentencias\n",
    "del train_sentences\n",
    "del test_sentences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67566b05",
   "metadata": {},
   "source": [
    "## IV. Calcular N Gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64322fb",
   "metadata": {},
   "source": [
    "### Calculo de N-Gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7aff28",
   "metadata": {},
   "source": [
    "#### Clases de los N-Gramas, (Unigramas, Bigramas y Trigramas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964e068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "    \"\"\"\n",
    "    Modelo de unigramas,\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename: str, file_is_training=True):\n",
    "        \"\"\"Makes the diccionary that the model needs to work,\n",
    "        ge\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo a procesar\n",
    "            file_is_training (bool, optional):\n",
    "                Indica cómo manejar el archivo de entrada.\n",
    "                - Si es False, se carga el objeto ya procesado desde un archivo `.pickle`.\n",
    "                - Si es True, el archivo se procesa desde cero.\n",
    "\n",
    "        \"\"\"\n",
    "        print(filename)\n",
    "        file = self.get_pickle(filename)\n",
    "        if file_is_training:\n",
    "            self.word_counter_20N = {}\n",
    "            for sentence in file:\n",
    "                for word in sentence:\n",
    "                    self.word_counter_20N[word] = self.word_counter_20N.get(word, 0) + 1\n",
    "            self.total_words = sum(self.word_counter_20N.values())\n",
    "            self.V = len(self.word_counter_20N)\n",
    "        else:\n",
    "            self.word_counter_20N = file[\"word_counter_20N\"]\n",
    "            self.total_words = file[\"total_words\"]\n",
    "            self.V = file[\"V\"]\n",
    "\n",
    "        self.total_words = sum(self.word_counter_20N.values())\n",
    "        self.V = len(self.word_counter_20N)\n",
    "\n",
    "    def get_pickle(self, filename: str):\n",
    "        \"\"\"\n",
    "        Abre un file en formato .pickle,\n",
    "        dentro de PATH_FINAL_FILES y lo devuelve.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del file a abrir\n",
    "\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            sentences = pickle.load(f)\n",
    "        return sentences\n",
    "\n",
    "    def generate_unigrams(self, filename: str):\n",
    "        \"\"\"Genera los unigramas en un archivo (jsonl) es lo que se\n",
    "        espera\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for word in self.word_counter_20N.keys():\n",
    "                prob = self.get_prob(word)\n",
    "                f.write(json.dumps({\"word\": word, \"probability\": prob}) + \"\\n\")\n",
    "\n",
    "    def get_prob(self, word: str) -> float:\n",
    "        \"\"\"\n",
    "        Calcula la probabilidad de un unigrama.\n",
    "        Si la palabra existe en el vocabulario V, se devuelve su probabilidad.\n",
    "        En caso contrario, se asigna al token <UNK>.\n",
    "\n",
    "        Args:\n",
    "            word (str): Palabra a consultar.\n",
    "\n",
    "        Returns:\n",
    "            float: Probabilidad asociada a la palabra.\n",
    "        \"\"\"\n",
    "        if word.lower() in self.word_counter_20N.keys():\n",
    "            prob = self.word_counter_20N[word] / self.total_words\n",
    "        else:\n",
    "            prob = self.word_counter_20N[\"<UNK>\"] / self.total_words\n",
    "        return prob\n",
    "\n",
    "    def get_next_token(self) -> str:\n",
    "        \"\"\"Genera un token según las probabilidades unigramales.\"\"\"\n",
    "        probabilities = [self.get_prob([self.token_of(k)]) for k in range(self.V)]\n",
    "        probs = [math.exp(p) for p in probabilities]\n",
    "        index = random.choices(range(self.V), weights=probs, k=1)[0]\n",
    "        return self.token_of(index)\n",
    "\n",
    "    def generate_sentences(self, limit: int = 50) -> list[str]:\n",
    "        \"\"\"Genera una sentencia basado en unigramas\n",
    "\n",
    "        Args:\n",
    "            limit (int, optional): limite de palabras a predecir. Defaults to 50.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: sentencia en una lista de strings.\n",
    "        \"\"\"\n",
    "        sentence = [\"<s>\"]\n",
    "        for _ in range(limit):\n",
    "            token = self.get_next_token()\n",
    "            if token == \"</s>\":\n",
    "                break\n",
    "            sentence.append(token)\n",
    "        sentence.append(\"</s>\")\n",
    "        return \" \".join(sentence)\n",
    "\n",
    "    def save_model(self, filename: str):\n",
    "        \"\"\"\n",
    "        Guarda el modelo de unigramas entrenado en un archivo `.pickle`.\n",
    "\n",
    "        El archivo contendrá:\n",
    "        - word_counter_20N: Diccionario de conteos de palabras.\n",
    "        - total_words: Número total de palabras en el corpus.\n",
    "        - V: Tamaño del vocabulario.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"word_counter_20N\": self.word_counter_20N,\n",
    "            \"total_words\": self.total_words,\n",
    "            \"V\": self.V,\n",
    "        }\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(payload, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "class BigramModel:\n",
    "    \"\"\"Bigram Model\"\"\"\n",
    "\n",
    "    def __init__(self, filename: str, file_is_training=True):\n",
    "        \"\"\"\n",
    "        Inicializador del modelo de bigramas.\n",
    "\n",
    "        En este paso se construyen varias estructuras necesarias:\n",
    "\n",
    "        - dictionary: objeto que permite mapear palabras ↔ tokens.\n",
    "        - V: número total de tokens en el corpus (palabras + caracteres especiales).\n",
    "        - matrix: diccionario que guarda el conteo de ocurrencias de los bigramas\n",
    "            observados. (La matriz completa sería inviable de almacenar).\n",
    "        - row_sums: para agilizar el cálculo de probabilidades se guarda, para cada\n",
    "            token, la suma total de sus ocurrencias como primer elemento en un bigrama.\n",
    "            De esta forma, el denominador de la probabilidad condicional ya está\n",
    "            precomputado y no es necesario recalcularlo en cada consulta.\n",
    "        \"\"\"\n",
    "\n",
    "        data = self._load_pickle(filename)\n",
    "\n",
    "        if file_is_training:\n",
    "            self.dictionary = Dictionary(data)\n",
    "            self.V = len(self.dictionary)\n",
    "\n",
    "            self.matrix = {}\n",
    "            self.row_sums = {}\n",
    "\n",
    "            for sentence in data:\n",
    "                for i in range(len(sentence)):\n",
    "                    w_idx = self._word_index(sentence[i])\n",
    "                    if i < len(sentence) - 1:\n",
    "                        w_next_idx = self._word_index(sentence[i + 1])\n",
    "                        key = (w_idx, w_next_idx)\n",
    "                        self.matrix[key] = self.matrix.get(key, 0) + 1\n",
    "                        self.row_sums[w_idx] = self.row_sums.get(w_idx, 0) + 1\n",
    "        else:\n",
    "            self.dictionary = data[\"dictionary\"]\n",
    "            self.matrix = dict(data[\"matrix\"])\n",
    "            self.row_sums = dict(data[\"row_sums\"])\n",
    "\n",
    "            if not hasattr(self.dictionary, \"id2token\") or not self.dictionary.id2token:\n",
    "                self.dictionary.id2token = {\n",
    "                    i: t for t, i in self.dictionary.token2id.items()\n",
    "                }\n",
    "\n",
    "            self.V = len(self.dictionary.token2id)\n",
    "\n",
    "    def _word_index(self, word: str) -> int:\n",
    "        \"\"\"\n",
    "        Devuelve el ID asociado a un token.\n",
    "        Si el token no existe en el diccionario, se asigna el ID correspondiente de <UNK>.\n",
    "\n",
    "        Args:\n",
    "            word (str): Palabra o token cuyo ID se desea obtener.\n",
    "\n",
    "        Returns:\n",
    "            int: ID de la palabra o, en caso de no estar en el diccionario,\n",
    "                el ID de <UNK>.\n",
    "        \"\"\"\n",
    "        tid = self.dictionary.token2id.get(word)\n",
    "        if tid is None:\n",
    "            tid = self.dictionary.token2id[\"<UNK>\"]\n",
    "        return tid\n",
    "\n",
    "    def _load_pickle(self, filename: str):\n",
    "        \"\"\"Carga de un pickle\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del file\n",
    "\n",
    "        Returns:\n",
    "            _type_: Estructura que posea el pickle\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_model(self, filename: str):\n",
    "        \"\"\"Guarda el modelo, para no tener que\n",
    "        volver a recalcular.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del file en el cual se va a guardar el modelo\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"dictionary\": self.dictionary,\n",
    "            \"V\": self.V,\n",
    "            \"matrix\": dict(self.matrix),\n",
    "            \"row_sums\": dict(self.row_sums),\n",
    "        }\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(payload, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def token_of(self, idx: int) -> str:\n",
    "        \"\"\"\n",
    "        Devuelve el token asociado a un ID.\n",
    "\n",
    "        Args:\n",
    "            idx (int): ID del token.\n",
    "\n",
    "        Returns:\n",
    "            str: Token correspondiente o <UNK> si no existe.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.dictionary[idx]\n",
    "        except KeyError:\n",
    "            return \"<UNK>\"\n",
    "\n",
    "    def get_prob(self, words: list[str]) -> float:\n",
    "        \"\"\"Se obtiene la probabilidad de una lista palabras\n",
    "        [w1,w2]\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista de palabras sobre la que\n",
    "            se obtiene\n",
    "\n",
    "        Returns:\n",
    "            float: probabilidad de w1, w2\n",
    "        \"\"\"\n",
    "        m_i = self._word_index(words[0])\n",
    "        m_j = self._word_index(words[1])\n",
    "        c_bigram = self.matrix.get((m_i, m_j), 0)\n",
    "        row_sum = self.row_sums.get(m_i, 0)\n",
    "        return np.log((c_bigram + 1) / (row_sum + self.V))\n",
    "\n",
    "    def generate_bigrams(self, filename: str):\n",
    "        \"\"\"\n",
    "        Genera y guarda SOLO las probabilidades de los bigramas OBSERVADOS\n",
    "        (claves en self.matrix), incluyendo aquellos que involucren <UNK>.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida (.jsonl).\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for (i, j), _count in self.matrix.items():\n",
    "                w1 = self.token_of(i)\n",
    "                w2 = self.token_of(j)\n",
    "                prob = self.get_prob([w1, w2])\n",
    "                record = {\"w1\": w1, \"w2\": w2, \"probabilidad\": prob}\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    def get_next_token(self, words: list[str], top_k: int = 10) -> str:\n",
    "        \"\"\"\n",
    "        Predice el siguiente token a partir del modelo de bigramas.\n",
    "\n",
    "        A partir del token actual `words[0]`, calcula las probabilidades\n",
    "        de transición hacia todos los tokens del vocabulario y selecciona\n",
    "        aleatoriamente el siguiente token entre los `top_k` más probables.\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista de un solo token que sirve como contexto.\n",
    "            top_k (int): Número de candidatos más probables a considerar.\n",
    "\n",
    "        Returns:\n",
    "            str: El siguiente token predicho.\n",
    "        \"\"\"\n",
    "        prev = words[0]\n",
    "\n",
    "        probabilities = []\n",
    "        for k in range(self.V):\n",
    "            lp = self.get_prob([prev, self.token_of(k)])\n",
    "            probabilities.append((k, lp))\n",
    "\n",
    "        topk = sorted(probabilities, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "        indices, lps = zip(*topk)\n",
    "        probs = [math.exp(lp) for lp in lps]\n",
    "\n",
    "        chosen_idx = random.choices(indices, weights=probs, k=1)[0]\n",
    "        return self.token_of(chosen_idx)\n",
    "\n",
    "    def generate_sentences(\n",
    "        self, words: list[str], limit: int = 50, top_k: int = 10\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Genera una oración usando el modelo de bigramas.\n",
    "        Se detiene al alcanzar </s> o el límite de tokens.\n",
    "        \"\"\"\n",
    "        current = words[0]\n",
    "        out = []\n",
    "        out.append(words[0])\n",
    "\n",
    "        for _ in range(limit):\n",
    "            nxt = self.get_next_token([current], top_k=top_k)\n",
    "            if nxt == \"</s>\":\n",
    "                break\n",
    "            out.append(nxt)\n",
    "            current = nxt\n",
    "\n",
    "        return \" \".join(out)\n",
    "\n",
    "\n",
    "class TrigramModel:\n",
    "    \"\"\"\n",
    "    Inicializa el modelo de trigramas.\n",
    "\n",
    "    Si `file_is_training` es True, procesa los datos para construir el diccionario\n",
    "    y las estructuras necesarias para calcular probabilidades.\n",
    "    Si es False, carga el modelo previamente entrenado desde un archivo `.pickle`.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Nombre del archivo de entrada.\n",
    "        file_is_training (bool, optional):\n",
    "            - True: procesa los datos desde cero.\n",
    "            - False: carga un modelo ya procesado.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename, file_is_training=True):\n",
    "        data = self._load_pickle(filename)\n",
    "        if file_is_training:\n",
    "            self.dictionary = Dictionary(data)\n",
    "            self.dictionary.add_documents([[\"<UNK>\"]])\n",
    "            self.V = len(self.dictionary)\n",
    "            self.matrix_trigram = {}\n",
    "            self.pair_sums = {}\n",
    "            for sent in data:\n",
    "                ids = [self._word_index(w) for w in sent]\n",
    "                for t in range(len(ids) - 2):\n",
    "                    i, j, k = ids[t], ids[t + 1], ids[t + 2]\n",
    "                    key3 = (i, j, k)\n",
    "                    key2 = (i, j)\n",
    "                    self.matrix_trigram[key3] = self.matrix_trigram.get(key3, 0) + 1\n",
    "                    self.pair_sums[key2] = self.pair_sums.get(key2, 0) + 1\n",
    "        else:\n",
    "            self.dictionary = data[\"dictionary\"]\n",
    "            self.V = data[\"V\"]\n",
    "            self.matrix_trigram = dict(data[\"matrix_trigram\"])\n",
    "            self.pair_sums = dict(data[\"pair_sums\"])\n",
    "\n",
    "            if not hasattr(self.dictionary, \"id2token\") or not self.dictionary.id2token:\n",
    "                self.dictionary.id2token = {\n",
    "                    i: t for t, i in self.dictionary.token2id.items()\n",
    "                }\n",
    "\n",
    "            self.V = len(self.dictionary.token2id)\n",
    "\n",
    "    def _word_index(self, word: str) -> int:\n",
    "        \"\"\"\n",
    "        Devuelve el ID asociado a una palabra.\n",
    "\n",
    "        Si la palabra no existe en el diccionario, devuelve el ID de <UNK>.\n",
    "\n",
    "        Args:\n",
    "            word (str): Palabra a consultar.\n",
    "\n",
    "        Returns:\n",
    "            int: ID asociado a la palabra o al token <UNK>.\n",
    "        \"\"\"\n",
    "        tid = self.dictionary.token2id.get(word)\n",
    "        if tid is None:\n",
    "            tid = self.dictionary.token2id[\"<UNK>\"]\n",
    "        return tid\n",
    "\n",
    "    def _load_pickle(self, filename: str):\n",
    "        \"\"\"\n",
    "        Carga un archivo `.pickle` desde PATH_FINAL_FILES.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo a cargar.\n",
    "\n",
    "        Returns:\n",
    "            object: Contenido del pickle (corpus o modelo guardado).\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_model(self, filename: str):\n",
    "        \"\"\"\n",
    "        Guarda el modelo entrenado en un archivo `.pickle`.\n",
    "\n",
    "        El archivo incluye:\n",
    "        - Diccionario de tokens.\n",
    "        - Tamaño del vocabulario.\n",
    "        - Conteo de trigramas observados.\n",
    "        - Conteo de pares de tokens.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"dictionary\": self.dictionary,\n",
    "            \"V\": self.V,\n",
    "            \"matrix_trigram\": dict(self.matrix_trigram),\n",
    "            \"pair_sums\": dict(self.pair_sums),\n",
    "        }\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(payload, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def token_of(self, idx: int) -> str:\n",
    "        \"\"\"\n",
    "        Devuelve el token asociado a un ID.\n",
    "\n",
    "        Args:\n",
    "            idx (int): ID del token.\n",
    "\n",
    "        Returns:\n",
    "            str: Token correspondiente o <UNK> si no existe.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.dictionary[idx]\n",
    "        except KeyError:\n",
    "            return \"<UNK>\"\n",
    "\n",
    "    def get_prob(self, words: list[str]) -> float:\n",
    "        \"\"\"\n",
    "                Calcula la probabilidad de un trigrama.\n",
    "\n",
    "                Usa la fórmula:\n",
    "                    P(w_k | w_i, w_j) = (conteo(i, j, k)) / (conteo(i, j) + V)\n",
    "\n",
    "                con suavizado de Laplace.\n",
    "        top_k\n",
    "                Args:\n",
    "                    words (list[str]): Lista con tres tokens [w_i, w_j, w_k].\n",
    "\n",
    "                Returns:\n",
    "                    float: Probabilidad logarítmica del trigrama.\n",
    "        \"\"\"\n",
    "        i = self._word_index(words[0])\n",
    "        j = self._word_index(words[1])\n",
    "        k = self._word_index(words[2])\n",
    "        V = self.V\n",
    "        c_ijk = self.matrix_trigram.get((i, j, k), 0)\n",
    "        denom = self.pair_sums.get((i, j), 0)\n",
    "        return float(np.log((c_ijk + 1) / (denom + V)))\n",
    "\n",
    "    def generate_trigrams(self, filename: str):\n",
    "        \"\"\"\n",
    "        Guarda SOLO trigramas OBSERVADOS (claves de self.matrix_trigram),\n",
    "        incluyendo aquellos que involucren <UNK>.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida (.jsonl).\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for (i, j, k), _count in self.matrix_trigram.items():\n",
    "                w1, w2, w3 = self.token_of(i), self.token_of(j), self.token_of(k)\n",
    "                prob = self.get_prob([w1, w2, w3])\n",
    "                record = {\"w1\": w1, \"w2\": w2, \"w3\": w3, \"probabilidad\": prob}\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    def get_next_token(self, words: list[str]) -> str:\n",
    "        \"\"\"\n",
    "        Predice el siguiente token a partir del modelo de trigramas.\n",
    "\n",
    "        A partir del par actual `(words[0], words[1])`, calcula las probabilidades\n",
    "        de transición hacia todos los tokens del vocabulario y selecciona\n",
    "        aleatoriamente el siguiente token según las 10 probabilidades más altas.\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista de dos tokens que sirven como contexto.\n",
    "\n",
    "        Returns:\n",
    "            str: El siguiente token predicho.\n",
    "        \"\"\"\n",
    "        if (words[0], words[1]) not in self.pair_sums:\n",
    "            return self.token_of(random.randint(0, self.V - 1))\n",
    "\n",
    "        probabilities = []\n",
    "        for k in range(self.V):\n",
    "            p = self.get_prob([words[0], words[1], self.token_of(k)])\n",
    "            probabilities.append((k, p))\n",
    "\n",
    "        top10 = sorted(probabilities, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        indices, probs = zip(*top10)\n",
    "        probs = [math.exp(p) for p in probs]\n",
    "\n",
    "        chosen_idx = random.choices(indices, weights=probs, k=1)[0]\n",
    "        return self.token_of(chosen_idx)\n",
    "\n",
    "    def generate_sentences(self, words: list[str], limit=50) -> list[str]:\n",
    "        \"\"\"\n",
    "        Genera una oración utilizando un modelo de trigramas.\n",
    "\n",
    "        La generación comienza con dos palabras iniciales (`words[0]` y `words[1]`).\n",
    "        En cada paso se predice el siguiente token con `get_next_token`, se añade\n",
    "        a la oración y se actualiza el contexto.\n",
    "        El proceso se detiene al alcanzar el token de fin de secuencia `</s>`\n",
    "        o al llegar al número máximo de tokens (`limit`).\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista inicial con dos tokens de contexto.\n",
    "            limit (int, optional): Número máximo de tokens generados.\n",
    "                Por defecto 50.\n",
    "\n",
    "        Returns:\n",
    "            str: Oración generada.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        sentence = \" \".join(words)\n",
    "        predicted_token = self.get_next_token(words)\n",
    "        words[0] = words[1]\n",
    "        words[1] = predicted_token\n",
    "        while i != limit or predicted_token == \"<s>\":\n",
    "            predicted_token = self.get_next_token(words)\n",
    "            sentence += \" \" + predicted_token\n",
    "            words[0] = words[1]\n",
    "            words[1] = predicted_token\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2dd947",
   "metadata": {},
   "source": [
    "#### Entrenamiento de los N-Gramas, creacion de los archivos con las probabilidades y guardado de los archivos que contienen las estructuras de los N-Gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aba375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickle(filename: str) -> list[str]:\n",
    "    \"\"\"Carga un .pickle guardado\n",
    "\n",
    "    Args:\n",
    "        filename (str): nombre del archivo\n",
    "\n",
    "    Returns:\n",
    "        Any: devuelve lo que sea se guardara en el .pickle\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        sentences = pickle.load(f)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a466712f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la construcción del modelo de unigramas para el corpus 20N...\n",
      "20N_Erich_Carlos_training.pkl\n",
      "   • Procesando y generando la distribución de unigramas (20N).\n",
      "   • Distribución de unigramas generada satisfactoriamente (20N).\n",
      "   • Modelo de unigramas almacenado correctamente (20N).\n",
      "✓ Proceso completado: unigramas del corpus 20N.\n",
      "\n",
      "Iniciando la construcción del modelo de unigramas para el corpus BAC...\n",
      "BAC_Erich_Carlos_training.pkl\n",
      "   • Procesando y generando la distribución de unigramas (BAC).\n",
      "   • Distribución de unigramas generada satisfactoriamente (BAC).\n",
      "   • Modelo de unigramas almacenado correctamente (BAC).\n",
      "✓ Proceso completado: unigramas del corpus BAC.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Genera los archivos con las probabilidades\n",
    "# Unigramas\n",
    "\n",
    "print(\"Iniciando la construcción del modelo de unigramas para el corpus 20N...\")\n",
    "modelo_de_unigramas_20N = UnigramModel(f\"20N_{GRUPO}_training.pkl\")\n",
    "print(\"   • Procesando y generando la distribución de unigramas (20N).\")\n",
    "modelo_de_unigramas_20N.generate_unigrams(f\"20N_{GRUPO}_unigrams.jsonl\")\n",
    "print(\"   • Distribución de unigramas generada satisfactoriamente (20N).\")\n",
    "modelo_de_unigramas_20N.save_model(f\"20N_{GRUPO}_unigram_model.pkl\")\n",
    "print(\"   • Modelo de unigramas almacenado correctamente (20N).\")\n",
    "del modelo_de_unigramas_20N\n",
    "gc.collect()\n",
    "print(\"✓ Proceso completado: unigramas del corpus 20N.\\n\")\n",
    "\n",
    "print(\"Iniciando la construcción del modelo de unigramas para el corpus BAC...\")\n",
    "modelo_de_unigramas_BAC = UnigramModel(f\"BAC_{GRUPO}_training.pkl\")\n",
    "print(\"   • Procesando y generando la distribución de unigramas (BAC).\")\n",
    "modelo_de_unigramas_BAC.generate_unigrams(f\"BAC_{GRUPO}_unigrams.jsonl\")\n",
    "print(\"   • Distribución de unigramas generada satisfactoriamente (BAC).\")\n",
    "modelo_de_unigramas_BAC.save_model(f\"BAC_{GRUPO}_unigram_model.pkl\")\n",
    "print(\"   • Modelo de unigramas almacenado correctamente (BAC).\")\n",
    "del modelo_de_unigramas_BAC\n",
    "gc.collect()\n",
    "print(\"✓ Proceso completado: unigramas del corpus BAC.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d54e40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la construcción del modelo de bigramas para el corpus 20N...\n",
      "   • Procesando y generando la distribución de bigramas (20N).\n",
      "   • Distribución de bigramas generada satisfactoriamente (20N).\n",
      "   • Modelo de bigramas almacenado correctamente (20N).\n",
      "✓ Proceso completado: bigramas del corpus 20N.\n",
      "\n",
      "Iniciando la construcción del modelo de bigramas para el corpus BAC...\n",
      "   • Procesando y generando la distribución de bigramas (BAC).\n",
      "   • Distribución de bigramas generada satisfactoriamente (BAC).\n",
      "   • Modelo de bigramas almacenado correctamente (BAC).\n",
      "✓ Proceso completado: bigramas del corpus BAC.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bigramas\n",
    "print(\"Iniciando la construcción del modelo de bigramas para el corpus 20N...\")\n",
    "modelo_de_bigramas_20N = BigramModel(f\"20N_{GRUPO}_training.pkl\")\n",
    "print(\"   • Procesando y generando la distribución de bigramas (20N).\")\n",
    "modelo_de_bigramas_20N.generate_bigrams(f\"20N_{GRUPO}_bigrams.jsonl\")\n",
    "print(\"   • Distribución de bigramas generada satisfactoriamente (20N).\")\n",
    "modelo_de_bigramas_20N.save_model(f\"20N_{GRUPO}_bigram_model.pkl\")\n",
    "print(\"   • Modelo de bigramas almacenado correctamente (20N).\")\n",
    "del modelo_de_bigramas_20N\n",
    "gc.collect()\n",
    "print(\"✓ Proceso completado: bigramas del corpus 20N.\\n\")\n",
    "\n",
    "print(\"Iniciando la construcción del modelo de bigramas para el corpus BAC...\")\n",
    "modelo_de_bigramas_BAC = BigramModel(f\"BAC_{GRUPO}_training.pkl\")\n",
    "print(\"   • Procesando y generando la distribución de bigramas (BAC).\")\n",
    "modelo_de_bigramas_BAC.generate_bigrams(f\"BAC_{GRUPO}_bigrams.jsonl\")\n",
    "print(\"   • Distribución de bigramas generada satisfactoriamente (BAC).\")\n",
    "modelo_de_bigramas_BAC.save_model(f\"BAC_{GRUPO}_bigram_model.pkl\")\n",
    "print(\"   • Modelo de bigramas almacenado correctamente (BAC).\")\n",
    "del modelo_de_bigramas_BAC\n",
    "gc.collect()\n",
    "print(\"✓ Proceso completado: bigramas del corpus BAC.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2215561c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la construcción del modelo de trigramas para el corpus 20N...\n",
      "   • Procesando y generando la distribución de trigramas (20N).\n",
      "   • Distribución de trigramas generada satisfactoriamente (20N).\n",
      "   • Modelo de trigramas almacenado correctamente (20N).\n",
      "✓ Proceso completado: trigramas del corpus 20N.\n",
      "\n",
      "Iniciando la construcción del modelo de trigramas para el corpus BAC...\n",
      "   • Procesando y generando la distribución de trigramas (BAC).\n",
      "   • Distribución de trigramas generada satisfactoriamente (BAC).\n",
      "   • Modelo de trigramas almacenado correctamente (BAC).\n",
      "✓ Proceso completado: trigramas del corpus BAC.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trigramas\n",
    "print(\"Iniciando la construcción del modelo de trigramas para el corpus 20N...\")\n",
    "modelo_de_trigramas_20N = TrigramModel(f\"20N_{GRUPO}_training.pkl\")\n",
    "print(\"   • Procesando y generando la distribución de trigramas (20N).\")\n",
    "modelo_de_trigramas_20N.generate_trigrams(f\"20N_{GRUPO}_trigrams.jsonl\")\n",
    "print(\"   • Distribución de trigramas generada satisfactoriamente (20N).\")\n",
    "modelo_de_trigramas_20N.save_model(f\"20N_{GRUPO}_trigram_model.pkl\")\n",
    "print(\"   • Modelo de trigramas almacenado correctamente (20N).\")\n",
    "del modelo_de_trigramas_20N\n",
    "gc.collect()\n",
    "print(\"✓ Proceso completado: trigramas del corpus 20N.\\n\")\n",
    "\n",
    "print(\"Iniciando la construcción del modelo de trigramas para el corpus BAC...\")\n",
    "modelo_de_trigramas_BAC = TrigramModel(f\"BAC_{GRUPO}_training.pkl\")\n",
    "print(\"   • Procesando y generando la distribución de trigramas (BAC).\")\n",
    "modelo_de_trigramas_BAC.generate_trigrams(f\"BAC_{GRUPO}_trigrams.jsonl\")\n",
    "print(\"   • Distribución de trigramas generada satisfactoriamente (BAC).\")\n",
    "modelo_de_trigramas_BAC.save_model(f\"BAC_{GRUPO}_trigram_model.pkl\")\n",
    "print(\"   • Modelo de trigramas almacenado correctamente (BAC).\")\n",
    "del modelo_de_trigramas_BAC\n",
    "gc.collect()\n",
    "print(\"✓ Proceso completado: trigramas del corpus BAC.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
