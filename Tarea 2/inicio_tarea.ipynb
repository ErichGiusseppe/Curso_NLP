{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d1cfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from charset_normalizer import from_path\n",
    "from gensim.corpora import Dictionary\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"\n",
    "Se importan las librerias que se necesiten, \n",
    "si se quiere ejecutar el notebook, se recomienda crear la carpeta de data, y poner ahi los files como se describe\n",
    "\n",
    "\"\"\"\n",
    "ACTUAL_PATH = os.getcwd()\n",
    "# Donde esta el 20 News\n",
    "PATH_20N = os.path.join(ACTUAL_PATH, \"data/20news-18828\")\n",
    "# Donde se encuentra el BAC\n",
    "PATH_BAC = os.path.join(ACTUAL_PATH, \"data/BAC/blogs\")\n",
    "# Donde se van a guardar los files que se van obteniendo\n",
    "PATH_FINAL_FILES = os.path.join(ACTUAL_PATH, \"data/final_files\")\n",
    "# Numero de grupo (realmente como no hay pues simplemente se pusimos nuestros nombres)\n",
    "GRUPO = \"Erich_Carlos\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d444fb",
   "metadata": {},
   "source": [
    "## I. Read the files and build two large consolidate files that are the union of all the documents in 20N and BAC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb246615",
   "metadata": {},
   "source": [
    "### UPLOAD_FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c7d5b",
   "metadata": {},
   "source": [
    "#### UPLOADING 20N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "789767b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Para esto se define en que formato y donde se quiere el archivo completo de 20N\n",
    "\"\"\"\n",
    "NEW_20N_FILE = os.path.join(PATH_FINAL_FILES, \"20N.jsonl\")\n",
    "\n",
    "mayor_folders_20N = os.listdir(PATH_20N)\n",
    "dictionary = {}\n",
    "\"\"\"\n",
    "Para cada archivo disponible en 20N,  \n",
    "se generan registros con:  \n",
    "- el ID del archivo  \n",
    "- el tema del archivo  \n",
    "- el texto del contenido  \n",
    "\n",
    "Todo se guarda en formato JSONL por cuestiones de formato.\n",
    "\"\"\"\n",
    "with open(NEW_20N_FILE, \"w\", encoding=\"utf-8\", errors=\"replace\") as f_n:\n",
    "    for folder in mayor_folders_20N:\n",
    "        minor_files_path = os.path.join(PATH_20N, folder)\n",
    "        minor_files = os.listdir(minor_files_path)\n",
    "        for file in minor_files:\n",
    "            file_path = os.path.join(minor_files_path, file)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "                text = f.read().strip()\n",
    "\n",
    "            record = {\"id\": file, \"theme\": folder, \"text\": text}\n",
    "            unit = folder + file\n",
    "            if file in dictionary.keys():\n",
    "                dictionary[unit] += 1\n",
    "            else:\n",
    "                dictionary[unit] = 1\n",
    "            f_n.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff5851",
   "metadata": {},
   "source": [
    "#### UPLOADING BAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55eae228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Para cada archivo disponible en BAC,  \n",
    "se generan registros con:  \n",
    "- el ID del archivo  \n",
    "- el tema del archivo  \n",
    "- el texto del contenido  \n",
    "\n",
    "Todo se guarda en formato JSONL por cuestiones de formato.\n",
    "\"\"\"\n",
    "NEW_BAC_FILE = os.path.join(PATH_FINAL_FILES, \"BAC.jsonl\")\n",
    "mayor_folders_BAC = os.listdir(PATH_BAC)\n",
    "with open(NEW_BAC_FILE, \"w\", encoding=\"utf-8\", errors=\"replace\") as f_n:\n",
    "    for file in mayor_folders_BAC:\n",
    "        post_num = 0\n",
    "        file_path = os.path.join(PATH_BAC, file)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            text = f.read().strip()\n",
    "            text = re.sub(r\"</?Blog>\", \"\", text)\n",
    "        post_list = text.split(\"<post>\")\n",
    "        for post in post_list:\n",
    "            post = post.strip().replace(\"</post>\", \"\")\n",
    "            record = {\"id\": file, \"post_num\": post_num,\"text\": post}\n",
    "            f_n.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "            post_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5688cb7",
   "metadata": {},
   "source": [
    "## II. Tokenize by sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a489088",
   "metadata": {},
   "source": [
    "### Funciones utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbbbcfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"sentencia\n",
    "\n",
    "    Args:\n",
    "        text (str): sentencia a procesar\n",
    "\n",
    "    Returns:\n",
    "        list[str]: lista de palabras y simbolos a dejar.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,!?]\", \" \", text, flags=re.I | re.A | re.MULTILINE)\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\d+\", \"NUM\", text)\n",
    "    text = \"<s> \" + text + \" </s>\"\n",
    "    text = text.strip().split()\n",
    "    return text\n",
    "\n",
    "\n",
    "def divide_sentences(text: str) -> list[str]:\n",
    "    \"\"\"Divide los textos en sentencias\n",
    "\n",
    "    Args:\n",
    "        text (str): texto completo\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Lista completa de sentencias\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bea1e5",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4530297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is sentence one.', 'Here is another!', 'And number 123.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is sentence one. Here is another! And number 123.\"\n",
    "sentences = divide_sentences(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aea6892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'this', 'is', 'sentence', 'one', '.', '</s>'], ['<s>', 'here', 'is', 'another', '!', '</s>'], ['<s>', 'and', 'number', 'NUM', '.', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "processed = [preprocess_text(s) for s in sentences]\n",
    "\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32526084",
   "metadata": {},
   "source": [
    "### Tokenizing_full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7253a",
   "metadata": {},
   "source": [
    "Para cada archivo y para cada texto contenido en él:  \n",
    "1. Se divide el texto en oraciones y se normaliza (minúsculas, \n",
    "   manejo de números, limpieza de símbolos innecesarios, etc.).  \n",
    "2. Se agregan las etiquetas `<s>` y `</s>` para marcar el inicio y fin de cada oración.  \n",
    "3. Se construye un diccionario de tokens.  \n",
    "4. Todas las palabras con frecuencia igual a 1 se reemplazan por el token <unk>, \n",
    "   con el fin de reducir ruido y manejar vocabulario raro o desconocido.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ded1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(data, filename:str):\n",
    "    \"\"\"funcion para guardar una estructura en un .pickle\n",
    "\n",
    "    Args:\n",
    "        data (_type_): Estructura a guardar\n",
    "        filename (str): Nombre del archivo a guardar\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f6782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\" Diccionario para contar palabras y asi reemplazar despues por <UNK>\n",
    "\"\"\"\n",
    "word_count = {}\n",
    "with open(os.path.join(PATH_FINAL_FILES, \"20N.jsonl\"), \"r\") as f:\n",
    "    for line in f:\n",
    "        line = json.loads(line)\n",
    "        for word in preprocess_text(line[\"text\"]):\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "word_count = {word: count for word, count in word_count.items() if count == 1}\n",
    "\n",
    "\"\"\"\n",
    "Se itera sobre los textos de 20N, dividiendo en oraciones y aplicando preprocesamiento.  \n",
    "Cada oración se añade a una lista para su posterior uso.  \n",
    "Luego, se realiza una partición aleatoria en conjuntos de entrenamiento y prueba,  \n",
    "los cuales finalmente se guardan en archivos.\n",
    "\"\"\"\n",
    "sentencias = []\n",
    "with open(os.path.join(PATH_FINAL_FILES, \"20N.jsonl\"), \"r\") as f:\n",
    "    for line in f:\n",
    "        line = json.loads(line)\n",
    "        sentences = divide_sentences(line[\"text\"])\n",
    "        for sentence in sentences:\n",
    "            pre_process = preprocess_text(sentence)\n",
    "            tokens = [word if word not in word_count else \"<UNK>\" for word in pre_process]\n",
    "            sentencias.append(tokens)\n",
    "            # print(sentence, pre_process)\n",
    "random.seed(42)\n",
    "random.shuffle(sentencias)\n",
    "index_to_split = int(0.8 * len(sentencias))\n",
    "train_sentences = sentencias[:index_to_split]\n",
    "test_sentences = sentencias[index_to_split:]\n",
    "save_pickle(train_sentences, f\"20N_{GRUPO}_training.pkl\")\n",
    "save_pickle(test_sentences, f\"20N_{GRUPO}_testing.pkl\")\n",
    "\n",
    "\"\"\" Se eliminan los archivos que no se necesitan por performance, (mi pc se ponia lenta).\n",
    "\"\"\"\n",
    "del sentencias\n",
    "del train_sentences\n",
    "del test_sentences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea5c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_lineas(path):\n",
    "    c = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for _ in f:\n",
    "            c += 1\n",
    "    return c\n",
    "\n",
    "file_path = os.path.join(PATH_FINAL_FILES, \"BAC.jsonl\")\n",
    "total_lineas = contar_lineas(file_path)\n",
    "\n",
    "\"\"\" Diccionario para contar palabras y asi reemplazar despues por <UNK>\n",
    "\"\"\"\n",
    "word_count = {}\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, total=total_lineas, desc=\"Contando palabras\", unit=\"línea\"):\n",
    "        line = json.loads(line)\n",
    "        for word in preprocess_text(line[\"text\"]):\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "\n",
    "word_count = {word: count for word, count in word_count.items() if count == 1}\n",
    "\"\"\"\n",
    "Se itera sobre los textos de BAC, dividiendo en oraciones y aplicando preprocesamiento.  \n",
    "Cada oración se añade a una lista para su posterior uso.  \n",
    "Luego, se realiza una partición aleatoria en conjuntos de entrenamiento y prueba,  \n",
    "los cuales finalmente se guardan en archivos.\n",
    "\"\"\"\n",
    "sentencias = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, total=total_lineas, desc=\"Procesando oraciones\", unit=\"línea\"): \n",
    "        line = json.loads(line)\n",
    "        sentences = divide_sentences(line[\"text\"])\n",
    "        for sentence in sentences:\n",
    "            pre_process = preprocess_text(sentence)\n",
    "            tokens = [word if word not in word_count else \"<UNK>\" for word in pre_process]\n",
    "            sentencias.append(tokens)\n",
    "            # print(sentence, pre_process)\n",
    "random.seed(42)\n",
    "random.shuffle(sentencias)\n",
    "split_idx = int(0.8 * len(sentencias))\n",
    "train_sentences = sentencias[:split_idx]\n",
    "test_sentences = sentencias[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97145739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_pickle(train_sentences, f\"BAC_{GRUPO}_training.pkl\")\n",
    "save_pickle(test_sentences, f\"BAC_{GRUPO}_testing.pkl\")\n",
    "\n",
    "\"\"\" Se eliminan los archivos que no se necesitan por performance, (mi pc se ponia lenta).\n",
    "\"\"\"\n",
    "del sentencias\n",
    "del train_sentences\n",
    "del test_sentences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67566b05",
   "metadata": {},
   "source": [
    "## IV. Calcular N Gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64322fb",
   "metadata": {},
   "source": [
    "### Calcular Unigramas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7aff28",
   "metadata": {},
   "source": [
    "#### 20N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964e068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "    \"\"\" \n",
    "    Modelo de unigramas, \n",
    "    \"\"\"\n",
    "    def __init__(self, filename: str, file_is_training = True):\n",
    "        \"\"\"Makes the diccionary that the model needs to work,\n",
    "        ge\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo a procesar\n",
    "            file_is_training (bool, optional): \n",
    "                Indica cómo manejar el archivo de entrada.  \n",
    "                - Si es False, se carga el objeto ya procesado desde un archivo `.pickle`.  \n",
    "                - Si es True, el archivo se procesa desde cero.  \n",
    "\n",
    "        \"\"\"\n",
    "        print(filename)\n",
    "        file = self.get_pickle(filename)\n",
    "        if file_is_training:\n",
    "            self.word_counter_20N = {}\n",
    "            for sentence in file:\n",
    "                for word in sentence:\n",
    "                    self.word_counter_20N[word] = self.word_counter_20N.get(word, 0) + 1\n",
    "        else:\n",
    "            self.word_counter_20N = file\n",
    "        self.total_words = sum(self.word_counter_20N.values())\n",
    "        self.V = len(self.word_counter_20N)\n",
    "    \n",
    "    def get_pickle(self, filename: str):\n",
    "        \"\"\"\n",
    "        Abre un file en formato .pickle, \n",
    "        dentro de PATH_FINAL_FILES y lo devuelve.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del file a abrir\n",
    "\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            sentences = pickle.load(f)\n",
    "        return sentences\n",
    "    def generate_unigrams(self, filename: str):\n",
    "        \"\"\"Genera los unigramas en un archivo (jsonl) es lo que se \n",
    "        espera\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for word in self.word_counter_20N.keys():\n",
    "                prob = self.get_prob(word)\n",
    "                f.write(json.dumps({\"word\": word, \"probability\": prob}) + \"\\n\")\n",
    "    def get_prob(self, word: str) -> float:\n",
    "        \"\"\"\n",
    "        Calcula la probabilidad de un unigrama.  \n",
    "        Si la palabra existe en el vocabulario V, se devuelve su probabilidad.  \n",
    "        En caso contrario, se asigna al token <UNK>.  \n",
    "\n",
    "        Args:\n",
    "            word (str): Palabra a consultar.  \n",
    "\n",
    "        Returns:\n",
    "            float: Probabilidad asociada a la palabra.  \n",
    "        \"\"\"\n",
    "        if word.lower() in self.word_counter_20N.keys():\n",
    "            prob = self.word_counter_20N[word] / self.total_words\n",
    "        else:\n",
    "            prob = self.word_counter_20N[\"<UNK>\"] / self.total_words\n",
    "        return prob\n",
    "    def get_next_token(self) -> str:\n",
    "        \"\"\" Genera un token según las probabilidades unigramales.\"\"\"\n",
    "        probabilities = [self.get_prob([self.token_of(k)]) for k in range(self.V)]\n",
    "        probs = [math.exp(p) for p in probabilities]\n",
    "        index = random.choices(range(self.V), weights=probs, k=1)[0]\n",
    "        return self.token_of(index)\n",
    "    \n",
    "    def generate_sentences(self, limit:int= 50) -> list[str]:\n",
    "        \"\"\" Genera una sentencia basado en unigramas\n",
    "\n",
    "        Args:\n",
    "            limit (int, optional): limite de palabras a predecir. Defaults to 50.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: sentencia en una lista de strings.\n",
    "        \"\"\"\n",
    "        sentence = [\"<s>\"]\n",
    "        for _ in range(limit):\n",
    "            token = self.get_next_token()\n",
    "            if token == \"</s>\":\n",
    "                break\n",
    "            sentence.append(token)    \n",
    "    def generate_bigrams(self, filename: str):\n",
    "        \"\"\"\n",
    "        Genera y guarda las probabilidades de todos los bigramas observados y no observados,\n",
    "        escribiéndolos progresivamente en un archivo JSONL legible.\n",
    "\n",
    "        Para cada par (i, j) en la matriz `self.V`, se obtiene el token correspondiente \n",
    "        mediante `self.token_of`, se calcula su probabilidad con `self.get_prob([w1, w2])` \n",
    "        y se escribe directamente en el archivo.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida (.jsonl) donde se guardarán \n",
    "                            las probabilidades de los bigramas.\n",
    "\n",
    "        Efectos:\n",
    "            Crea un archivo JSONL en PATH_FINAL_FILES/filename donde cada línea \n",
    "            tiene la forma:\n",
    "            {\"w1\": \"...\", \"w2\": \"...\", \"probabilidad\": ...}\n",
    "        \"\"\"\n",
    "\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            iterator = tqdm(\n",
    "                [(i, j) for i in range(self.V) for j in range(self.V)],\n",
    "                desc=\"Generando bigramas\",\n",
    "                unit=\"bigrama\",\n",
    "                total=self.V * self.V,\n",
    "                leave=True,\n",
    "                file=sys.stdout\n",
    "            )\n",
    "\n",
    "            for i, j in iterator:\n",
    "                w1 = self.token_of(i)\n",
    "                w2 = self.token_of(j)\n",
    "                prob = self.get_prob([w1, w2])\n",
    "\n",
    "                record = {\"w1\": w1, \"w2\": w2, \"probabilidad\": prob}\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "        sentence.append(\"</s>\")\n",
    "        return \" \".join(sentence)\n",
    "    def save_model(self, filename: str):\n",
    "        \"\"\"\n",
    "        Guarda el modelo de unigramas entrenado en un archivo `.pickle`.\n",
    "\n",
    "        El archivo contendrá:\n",
    "        - word_counter_20N: Diccionario de conteos de palabras.\n",
    "        - total_words: Número total de palabras en el corpus.\n",
    "        - V: Tamaño del vocabulario.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"word_counter_20N\": self.word_counter_20N,\n",
    "            \"total_words\": self.total_words,\n",
    "            \"V\": self.V\n",
    "        }\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(payload, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "class BigramModel:\n",
    "    \"\"\"Bigram Model\n",
    "    \"\"\"\n",
    "    def __init__(self, filename:str, file_is_training=True):\n",
    "        \"\"\"\n",
    "        Inicializador del modelo de bigramas.\n",
    "\n",
    "        En este paso se construyen varias estructuras necesarias:\n",
    "\n",
    "        - dictionary: objeto que permite mapear palabras ↔ tokens.\n",
    "        - V: número total de tokens en el corpus (palabras + caracteres especiales).\n",
    "        - matrix: diccionario que guarda el conteo de ocurrencias de los bigramas \n",
    "            observados. (La matriz completa sería inviable de almacenar).\n",
    "        - row_sums: para agilizar el cálculo de probabilidades se guarda, para cada \n",
    "            token, la suma total de sus ocurrencias como primer elemento en un bigrama. \n",
    "            De esta forma, el denominador de la probabilidad condicional ya está \n",
    "            precomputado y no es necesario recalcularlo en cada consulta.\n",
    "        \"\"\"\n",
    "\n",
    "        data = self._load_pickle(filename)\n",
    "\n",
    "        if file_is_training:\n",
    "            self.dictionary = Dictionary(data)\n",
    "            self.V = len(self.dictionary)\n",
    "\n",
    "            self.matrix = {}                  \n",
    "            self.row_sums = {}            \n",
    "\n",
    "            for sentence in data:\n",
    "                for i in range(len(sentence)):\n",
    "                    w_idx = self._word_index(sentence[i])\n",
    "                    if i < len(sentence) - 1:\n",
    "                        w_next_idx = self._word_index(sentence[i + 1])\n",
    "                        key = (w_idx, w_next_idx)\n",
    "                        self.matrix[key] = self.matrix.get(key, 0) + 1\n",
    "                        self.row_sums[w_idx] = self.row_sums.get(w_idx, 0) + 1\n",
    "\n",
    "        else:\n",
    "            self.dictionary = data[\"dictionary\"]\n",
    "            self.V = data[\"V\"]\n",
    "            self.matrix = dict(data[\"matrix\"])\n",
    "            self.row_sums = dict(data[\"row_sums\"])\n",
    "\n",
    "    def _word_index(self, word: str) -> int:\n",
    "        \"\"\"\n",
    "        Devuelve el ID asociado a un token. \n",
    "        Si el token no existe en el diccionario, se asigna el ID correspondiente de <UNK>.\n",
    "\n",
    "        Args:\n",
    "            word (str): Palabra o token cuyo ID se desea obtener.\n",
    "\n",
    "        Returns:\n",
    "            int: ID de la palabra o, en caso de no estar en el diccionario, \n",
    "                el ID de <UNK>.\n",
    "        \"\"\"\n",
    "        tid = self.dictionary.token2id.get(word)\n",
    "        if tid is None:\n",
    "            tid = self.dictionary.token2id[\"<UNK>\"]\n",
    "        return tid\n",
    "\n",
    "    def _load_pickle(self, filename: str):\n",
    "        \"\"\"Carga de un pickle\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del file\n",
    "\n",
    "        Returns:\n",
    "            _type_: Estructura que posea el pickle\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_model(self, filename: str):\n",
    "        \"\"\"Guarda el modelo, para no tener que\n",
    "        volver a recalcular.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del file en el cual se va a guardar el modelo\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"dictionary\": self.dictionary,\n",
    "            \"V\": self.V,\n",
    "            \"matrix\": dict(self.matrix),\n",
    "            \"row_sums\": dict(self.row_sums)\n",
    "        }\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(payload, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def token_of(self, idx: int) -> str:\n",
    "        \"\"\"Obtiene el token asociado a una ID\n",
    "\n",
    "        Args:\n",
    "            idx (int): ID del token\n",
    "\n",
    "        Returns:\n",
    "            str: Token asociado a la ID\n",
    "        \"\"\"\n",
    "        return self.dictionary.id2token.get(idx, \"<UNK>\")\n",
    "\n",
    "    def get_prob(self, words: list[str]) -> float:\n",
    "        \"\"\"Se obtiene la probabilidad de una lista palabras\n",
    "        [w1,w2]\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista de palabras sobre la que \n",
    "            se obtiene \n",
    "\n",
    "        Returns:\n",
    "            float: probabilidad de w1, w2\n",
    "        \"\"\"\n",
    "        m_i = self._word_index(words[0])\n",
    "        m_j = self._word_index(words[1])\n",
    "        c_bigram = self.matrix.get((m_i, m_j), 0)\n",
    "        row_sum  = self.row_sums.get(m_i, 0)\n",
    "        return np.log((c_bigram + 1) / (row_sum + self.V))\n",
    "\n",
    "    def generate_bigrams(self, filename: str):\n",
    "        \"\"\"\n",
    "        Genera y guarda SOLO las probabilidades de los bigramas OBSERVADOS\n",
    "        (claves en self.matrix), incluyendo aquellos que involucren <UNK>.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida (.jsonl).\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for (i, j), _count in self.matrix.items():\n",
    "                w1 = self.token_of(i)\n",
    "                w2 = self.token_of(j)\n",
    "                prob = self.get_prob([w1, w2])\n",
    "                record = {\"w1\": w1, \"w2\": w2, \"probabilidad\": prob}\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "    def get_next_token(self, words:list[str]) -> str:\n",
    "        \"\"\"\n",
    "        Predice el siguiente token a partir del modelo de bigramas.\n",
    "\n",
    "        A partir del token actual (`words[0]`), se calculan las probabilidades\n",
    "        de transición hacia todos los tokens del vocabulario. Luego, se elige \n",
    "        aleatoriamente un token ponderado por dichas probabilidades.\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista que contiene el token actual en la primera posición.\n",
    "\n",
    "        Returns:\n",
    "            str: El siguiente token predicho.\n",
    "        \"\"\"\n",
    "        probabilities = []\n",
    "        for k in range(self.V):\n",
    "            probabilities.append(self.get_prob([words[0], self.token_of(k)]))\n",
    "        probs = [math.exp(p) for p in probabilities] \n",
    "        index = random.choices(range(self.V), weights=probs, k=1)[0]\n",
    "        return self.token_of(index)\n",
    "    \n",
    "    def generate_sentences(self, words:list[str], limit= 50) -> list[str]:\n",
    "        \"\"\"\n",
    "        Genera una oración utilizando un modelo de bigramas.\n",
    "\n",
    "        La generación inicia a partir del token en `words[0]`. En cada paso, \n",
    "        se predice el siguiente token con `get_next_token`, se agrega a la \n",
    "        oración y se actualiza el contexto. El proceso termina al alcanzar el \n",
    "        token de fin de secuencia `</s>` o al superar el límite de tokens.\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista inicial con el token de partida en la \n",
    "                primera posición.\n",
    "            limit (int, optional): Número máximo de tokens a generar. \n",
    "                Por defecto 50.\n",
    "\n",
    "        Returns:\n",
    "            str: Oración generada.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        sentence = words[0]\n",
    "        predicted_token = self.get_next_token(words)\n",
    "        words[0] = predicted_token\n",
    "        while i != limit or predicted_token == \"</s>\":\n",
    "            predicted_token = self.get_next_token(words)\n",
    "            sentence += \" \" + predicted_token\n",
    "            words[0] = predicted_token\n",
    "            i+=1\n",
    "        return sentence\n",
    "    \n",
    "class TrigramModel:\n",
    "    \"\"\"\n",
    "    Inicializa el modelo de trigramas.\n",
    "\n",
    "    Si `file_is_training` es True, procesa los datos para construir el diccionario \n",
    "    y las estructuras necesarias para calcular probabilidades.  \n",
    "    Si es False, carga el modelo previamente entrenado desde un archivo `.pickle`.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Nombre del archivo de entrada.\n",
    "        file_is_training (bool, optional): \n",
    "            - True: procesa los datos desde cero.  \n",
    "            - False: carga un modelo ya procesado.  \n",
    "    \"\"\"\n",
    "    def __init__(self, filename, file_is_training=True):\n",
    "        data = self._load_pickle(filename)\n",
    "        if file_is_training:\n",
    "            self.dictionary = Dictionary(data)\n",
    "            self.dictionary.add_documents([[\"<UNK>\"]])\n",
    "            self.V = len(self.dictionary)\n",
    "            self.matrix_trigram = {}      \n",
    "            self.pair_sums = {}           \n",
    "            for sent in data:\n",
    "                ids = [self._word_index(w) for w in sent]\n",
    "                for t in range(len(ids) - 2):\n",
    "                    i, j, k = ids[t], ids[t+1], ids[t+2]\n",
    "                    key3 = (i, j, k)\n",
    "                    key2 = (i, j)\n",
    "                    self.matrix_trigram[key3] = self.matrix_trigram.get(key3, 0) + 1\n",
    "                    self.pair_sums[key2] = self.pair_sums.get(key2, 0) + 1\n",
    "        else:\n",
    "            self.dictionary     = data[\"dictionary\"]\n",
    "            self.V              = data[\"V\"]\n",
    "            self.matrix_trigram = dict(data[\"matrix_trigram\"])\n",
    "            self.pair_sums      = dict(data[\"pair_sums\"])\n",
    "\n",
    "    def _word_index(self, word: str) -> int:\n",
    "        \"\"\"\n",
    "        Devuelve el ID asociado a una palabra.\n",
    "\n",
    "        Si la palabra no existe en el diccionario, devuelve el ID de <UNK>.\n",
    "\n",
    "        Args:\n",
    "            word (str): Palabra a consultar.\n",
    "\n",
    "        Returns:\n",
    "            int: ID asociado a la palabra o al token <UNK>.\n",
    "        \"\"\"\n",
    "        tid = self.dictionary.token2id.get(word)\n",
    "        if tid is None:\n",
    "            tid = self.dictionary.token2id[\"<UNK>\"]\n",
    "        return tid\n",
    "\n",
    "    def _load_pickle(self, filename: str):\n",
    "        \"\"\"\n",
    "        Carga un archivo `.pickle` desde PATH_FINAL_FILES.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo a cargar.\n",
    "\n",
    "        Returns:\n",
    "            object: Contenido del pickle (corpus o modelo guardado).\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_model(self, filename: str):\n",
    "        \"\"\"\n",
    "        Guarda el modelo entrenado en un archivo `.pickle`.\n",
    "\n",
    "        El archivo incluye:\n",
    "        - Diccionario de tokens.\n",
    "        - Tamaño del vocabulario.\n",
    "        - Conteo de trigramas observados.\n",
    "        - Conteo de pares de tokens.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"dictionary\": self.dictionary,\n",
    "            \"V\": self.V,\n",
    "            \"matrix_trigram\": dict(self.matrix_trigram),\n",
    "            \"pair_sums\": dict(self.pair_sums),\n",
    "        }\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(payload, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def token_of(self, idx: int) -> str:\n",
    "        \"\"\"\n",
    "        Devuelve el token asociado a un ID.\n",
    "\n",
    "        Args:\n",
    "            idx (int): ID del token.\n",
    "\n",
    "        Returns:\n",
    "            str: Token correspondiente o <UNK> si no existe.\n",
    "        \"\"\"\n",
    "        return self.dictionary.id2token.get(idx, \"<UNK>\")\n",
    "\n",
    "    def get_prob(self, words: list[str]) -> float:\n",
    "        \"\"\"\n",
    "        Calcula la probabilidad de un trigrama.\n",
    "\n",
    "        Usa la fórmula:\n",
    "            P(w_k | w_i, w_j) = (conteo(i, j, k)) / (conteo(i, j) + V)\n",
    "\n",
    "        con suavizado de Laplace.\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista con tres tokens [w_i, w_j, w_k].\n",
    "\n",
    "        Returns:\n",
    "            float: Probabilidad logarítmica del trigrama.\n",
    "        \"\"\"\n",
    "        i = self._word_index(words[0])\n",
    "        j = self._word_index(words[1])\n",
    "        k = self._word_index(words[2])\n",
    "        V = self.V\n",
    "        c_ijk = self.matrix_trigram.get((i, j, k), 0)\n",
    "        denom = self.pair_sums.get((i, j), 0)\n",
    "        return float(np.log((c_ijk + 1) / (denom + V)))\n",
    "    def generate_trigrams(self, filename: str):\n",
    "        \"\"\"\n",
    "        Guarda SOLO trigramas OBSERVADOS (claves de self.matrix_trigram),\n",
    "        incluyendo aquellos que involucren <UNK>.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo de salida (.jsonl).\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for (i, j, k), _count in self.matrix_trigram.items():\n",
    "                w1, w2, w3 = self.token_of(i), self.token_of(j), self.token_of(k)\n",
    "                prob = self.get_prob([w1, w2, w3])\n",
    "                record = {\"w1\": w1, \"w2\": w2, \"w3\": w3, \"probabilidad\": prob}\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    def get_next_token(self, words:list[str]) -> str:\n",
    "        \"\"\"\n",
    "        Predice el siguiente token a partir del modelo de trigramas.\n",
    "\n",
    "        A partir del par actual `(words[0], words[1])`, calcula las probabilidades\n",
    "        de transición hacia todos los tokens del vocabulario y selecciona \n",
    "        aleatoriamente el siguiente token según dichas probabilidades.\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista de dos tokens que sirven como contexto.\n",
    "\n",
    "        Returns:\n",
    "            str: El siguiente token predicho.\n",
    "        \"\"\"\n",
    "        probabilities = []\n",
    "        if (words[0], words[1]) in self.pair_sums:\n",
    "            for k in range(self.V):\n",
    "                probabilities.append(self.get_prob([words[0], words[1], self.token_of(k)]))\n",
    "        probs = [math.exp(p) for p in probabilities] \n",
    "        return random.choices(range(self.V), weights=probs, k=1)[0]\n",
    "    def generate_sentences(self, words:list[str], limit= 50) -> list[str]:\n",
    "        \"\"\"\n",
    "        Genera una oración utilizando un modelo de trigramas.\n",
    "\n",
    "        La generación comienza con dos palabras iniciales (`words[0]` y `words[1]`).  \n",
    "        En cada paso se predice el siguiente token con `get_next_token`, se añade \n",
    "        a la oración y se actualiza el contexto.  \n",
    "        El proceso se detiene al alcanzar el token de fin de secuencia `</s>` \n",
    "        o al llegar al número máximo de tokens (`limit`).\n",
    "\n",
    "        Args:\n",
    "            words (list[str]): Lista inicial con dos tokens de contexto.\n",
    "            limit (int, optional): Número máximo de tokens generados. \n",
    "                Por defecto 50.\n",
    "\n",
    "        Returns:\n",
    "            str: Oración generada.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        sentence = \" \".join(words)\n",
    "        predicted_token = self.get_next_token(words)\n",
    "        words[0] = words[1]\n",
    "        words[1] = predicted_token\n",
    "        while i != limit or predicted_token == \"<s>\":\n",
    "            predicted_token = self.get_next_token(words)\n",
    "            sentence += \" \" + predicted_token\n",
    "            words[0] = words[1]\n",
    "            words[1] = predicted_token\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aba375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickle(filename: str) -> list[str]:\n",
    "    \"\"\"Carga un .pickle guardado\n",
    "\n",
    "    Args:\n",
    "        filename (str): nombre del archivo\n",
    "\n",
    "    Returns:\n",
    "        Any: devuelve lo que sea se guardara en el .pickle \n",
    "    \"\"\"\n",
    "    filepath = os.path.join(PATH_FINAL_FILES, filename)\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        sentences = pickle.load(f)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78001d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def perplexity_unigram(model: UnigramModel, filename:str) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad de un modelo de unigramas sobre un corpus de prueba.  \n",
    "\n",
    "    La perplejidad mide qué tan bien el modelo predice un conjunto de oraciones.  \n",
    "    Se calcula como:  \n",
    "\n",
    "        PP = exp( - (1/T) * Σ log P(w_i) )  \n",
    "\n",
    "    donde T es el número total de palabras en el corpus.  \n",
    "\n",
    "    Args:\n",
    "        model (UnigramModel): Modelo de unigramas sobre el que se evalúa.  \n",
    "        filename (str): Nombre del archivo de prueba (.pickle) que contiene las oraciones.  \n",
    "\n",
    "    Returns:\n",
    "        float: Valor de la perplejidad.  \n",
    "                Devuelve `inf` si alguna palabra tiene probabilidad 0 o si el corpus está vacío.  \n",
    "    \"\"\"\n",
    "    sentences = model.get_pickle(filename)\n",
    "    log_sum = 0.0\n",
    "    T = 0\n",
    "    for s in sentences:\n",
    "        for w in s:\n",
    "            p = model.get_prob(w)\n",
    "            if p == 0.0:\n",
    "                return float(\"inf\")\n",
    "            log_sum += math.log(p)\n",
    "            T += 1\n",
    "    return math.exp(-log_sum / T) if T else float(\"inf\")\n",
    "\n",
    "def perplexity_bigram_from_model(model:BigramModel, filename:str):\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad de un modelo de bigramas sobre un corpus de prueba.  \n",
    "\n",
    "    La perplejidad mide la capacidad del modelo para predecir secuencias de palabras:  \n",
    "\n",
    "        PP = exp( - (1/T) * Σ log P(w_i | w_{i-1}) )  \n",
    "\n",
    "    Args:\n",
    "        model (BigramModel): Modelo de bigramas sobre el que se evalúa.  \n",
    "        filename (str): Nombre del archivo de prueba (.pickle) que contiene las sentencias.  \n",
    "\n",
    "    Returns:\n",
    "        float: Valor de la perplejidad.  \n",
    "                Devuelve `inf` si el corpus está vacío.  \n",
    "    \"\"\"\n",
    "    sentences = model._load_pickle(filename)\n",
    "    log_sum = 0.0\n",
    "    T = 0\n",
    "    for s in sentences:\n",
    "        for prev, cur in zip(s[:-1], s[1:]):\n",
    "            log_p = model.get_prob([prev, cur])  \n",
    "            log_sum += log_p\n",
    "            T += 1\n",
    "    return math.exp(-log_sum / T) if T else float(\"inf\")\n",
    "\n",
    "def perplexity_trigram_from_model(model:TrigramModel, filename:str):\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad de un modelo de trigramas sobre un corpus de prueba.  \n",
    "\n",
    "    La perplejidad se calcula como:  \n",
    "\n",
    "        PP = exp( - (1/T) * Σ log P(w_i | w_{i-2}, w_{i-1}) )  \n",
    "\n",
    "    Args:\n",
    "        model (TrigramModel): Modelo de trigramas sobre el que se evalúa.  \n",
    "        filename (str): Nombre del archivo de prueba (.pickle) que contiene las oraciones.  \n",
    "\n",
    "    Returns:\n",
    "        float: Valor de la perplejidad.  \n",
    "                Devuelve `inf` si el corpus está vacío.  \n",
    "    \"\"\"\n",
    "    sentences = model._load_pickle(filename)\n",
    "    log_sum = 0.0\n",
    "    T = 0\n",
    "    for s in sentences:\n",
    "        for prev_2, prev_1 ,cur in zip(s[:-2], s[1:-1], s[2:]):\n",
    "            print(prev_2, prev_1 ,cur )\n",
    "            log_p = model.get_prob([prev_2, prev_1, cur])  \n",
    "            log_sum += log_p\n",
    "            T += 1\n",
    "    return math.exp(-log_sum / T) if T else float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a466712f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la construcción del modelo de unigramas para el corpus 20N...\n",
      "20N_Erich_Carlos_training.pkl\n",
      "   • Procesando y generando la distribución de unigramas (20N).\n",
      "   • Distribución de unigramas generada satisfactoriamente (20N).\n",
      "   • Modelo de unigramas almacenado correctamente (20N).\n",
      "✓ Proceso completado: unigramas del corpus 20N.\n",
      "\n",
      "Iniciando la construcción del modelo de unigramas para el corpus BAC...\n",
      "BAC_Erich_Carlos_training.pkl\n",
      "   • Procesando y generando la distribución de unigramas (BAC).\n",
      "   • Distribución de unigramas generada satisfactoriamente (BAC).\n",
      "   • Modelo de unigramas almacenado correctamente (BAC).\n",
      "✓ Proceso completado: unigramas del corpus BAC.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Genera los archivos con las probabilidades\n",
    "# Unigramas\n",
    "\n",
    "print(\"Iniciando la construcción del modelo de unigramas para el corpus 20N...\")\n",
    "modelo_de_unigramas_20N = UnigramModel(f\"20N_{GRUPO}_training.pkl\")\n",
    "print(\"   • Procesando y generando la distribución de unigramas (20N).\")\n",
    "modelo_de_unigramas_20N.generate_unigrams(f\"20N_{GRUPO}_unigrams.jsonl\")\n",
    "print(\"   • Distribución de unigramas generada satisfactoriamente (20N).\")\n",
    "modelo_de_unigramas_20N.save_model(f\"20N_{GRUPO}_unigram_model.pkl\")\n",
    "print(\"   • Modelo de unigramas almacenado correctamente (20N).\")\n",
    "del modelo_de_unigramas_20N\n",
    "gc.collect()\n",
    "print(\"✓ Proceso completado: unigramas del corpus 20N.\\n\")\n",
    "\n",
    "print(\"Iniciando la construcción del modelo de unigramas para el corpus BAC...\")\n",
    "modelo_de_unigramas_BAC = UnigramModel(f\"BAC_{GRUPO}_training.pkl\")\n",
    "print(\"   • Procesando y generando la distribución de unigramas (BAC).\")\n",
    "modelo_de_unigramas_BAC.generate_unigrams(f\"BAC_{GRUPO}_unigrams.jsonl\")\n",
    "print(\"   • Distribución de unigramas generada satisfactoriamente (BAC).\")\n",
    "modelo_de_unigramas_BAC.save_model(f\"BAC_{GRUPO}_unigram_model.pkl\")\n",
    "print(\"   • Modelo de unigramas almacenado correctamente (BAC).\")\n",
    "del modelo_de_unigramas_BAC\n",
    "gc.collect()\n",
    "print(\"✓ Proceso completado: unigramas del corpus BAC.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d54e40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la construcción del modelo de bigramas para el corpus 20N...\n",
      "   • Procesando y generando la distribución de bigramas (20N).\n",
      "   • Distribución de bigramas generada satisfactoriamente (20N).\n",
      "   • Modelo de bigramas almacenado correctamente (20N).\n",
      "✓ Proceso completado: bigramas del corpus 20N.\n",
      "\n",
      "Iniciando la construcción del modelo de bigramas para el corpus BAC...\n",
      "   • Procesando y generando la distribución de bigramas (BAC).\n",
      "   • Distribución de bigramas generada satisfactoriamente (BAC).\n",
      "   • Modelo de bigramas almacenado correctamente (BAC).\n",
      "✓ Proceso completado: bigramas del corpus BAC.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bigramas\n",
    "print(\"Iniciando la construcción del modelo de bigramas para el corpus 20N...\")\n",
    "modelo_de_bigramas_20N = BigramModel(f\"20N_{GRUPO}_training.pkl\")\n",
    "print(\"   • Procesando y generando la distribución de bigramas (20N).\")\n",
    "modelo_de_bigramas_20N.generate_bigrams(f\"20N_{GRUPO}_bigrams.jsonl\")\n",
    "print(\"   • Distribución de bigramas generada satisfactoriamente (20N).\")\n",
    "modelo_de_bigramas_20N.save_model(f\"20N_{GRUPO}_bigram_model.pkl\")\n",
    "print(\"   • Modelo de bigramas almacenado correctamente (20N).\")\n",
    "del modelo_de_bigramas_20N\n",
    "gc.collect()\n",
    "print(\"✓ Proceso completado: bigramas del corpus 20N.\\n\")\n",
    "\n",
    "print(\"Iniciando la construcción del modelo de bigramas para el corpus BAC...\")\n",
    "modelo_de_bigramas_BAC = BigramModel(f\"BAC_{GRUPO}_training.pkl\")\n",
    "print(\"   • Procesando y generando la distribución de bigramas (BAC).\")\n",
    "modelo_de_bigramas_BAC.generate_bigrams(f\"BAC_{GRUPO}_bigrams.jsonl\")\n",
    "print(\"   • Distribución de bigramas generada satisfactoriamente (BAC).\")\n",
    "modelo_de_bigramas_BAC.save_model(f\"BAC_{GRUPO}_bigram_model.pkl\")\n",
    "print(\"   • Modelo de bigramas almacenado correctamente (BAC).\")\n",
    "del modelo_de_bigramas_BAC\n",
    "gc.collect()\n",
    "print(\"✓ Proceso completado: bigramas del corpus BAC.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2215561c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la construcción del modelo de trigramas para el corpus 20N...\n",
      "   • Procesando y generando la distribución de trigramas (20N).\n",
      "   • Distribución de trigramas generada satisfactoriamente (20N).\n",
      "   • Modelo de trigramas almacenado correctamente (20N).\n",
      "✓ Proceso completado: trigramas del corpus 20N.\n",
      "\n",
      "Iniciando la construcción del modelo de trigramas para el corpus BAC...\n"
     ]
    }
   ],
   "source": [
    "# Trigramas\n",
    "print(\"Iniciando la construcción del modelo de trigramas para el corpus 20N...\")\n",
    "modelo_de_trigramas_20N = TrigramModel(f\"20N_{GRUPO}_training.pkl\")\n",
    "print(\"   • Procesando y generando la distribución de trigramas (20N).\")\n",
    "modelo_de_trigramas_20N.generate_trigrams(f\"20N_{GRUPO}_trigrams.jsonl\")\n",
    "print(\"   • Distribución de trigramas generada satisfactoriamente (20N).\")\n",
    "modelo_de_trigramas_20N.save_model(f\"20N_{GRUPO}_trigram_model.pkl\")\n",
    "print(\"   • Modelo de trigramas almacenado correctamente (20N).\")\n",
    "del modelo_de_trigramas_20N\n",
    "gc.collect()\n",
    "print(\"✓ Proceso completado: trigramas del corpus 20N.\\n\")\n",
    "\n",
    "print(\"Iniciando la construcción del modelo de trigramas para el corpus BAC...\")\n",
    "modelo_de_trigramas_BAC = TrigramModel(f\"BAC_{GRUPO}_training.pkl\")\n",
    "print(\"   • Procesando y generando la distribución de trigramas (BAC).\")\n",
    "modelo_de_trigramas_BAC.generate_trigrams(f\"BAC_{GRUPO}_trigrams.jsonl\")\n",
    "print(\"   • Distribución de trigramas generada satisfactoriamente (BAC).\")\n",
    "modelo_de_trigramas_BAC.save_model(f\"BAC_{GRUPO}_trigram_model.pkl\")\n",
    "print(\"   • Modelo de trigramas almacenado correctamente (BAC).\")\n",
    "del modelo_de_trigramas_BAC\n",
    "gc.collect()\n",
    "print(\"✓ Proceso completado: trigramas del corpus BAC.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af89474",
   "metadata": {},
   "source": [
    "## Carga de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda845f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_model_20N = UnigramModel(f\"20N_{GRUPO}_unigram_model.pkl\", file_is_training=False)\n",
    "unigram_model_BAC = UnigramModel(f\"BAC_{GRUPO}_unigram_model.pkl\", file_is_training=False)\n",
    "\n",
    "bigram_model_20N = BigramModel(f\"20N_{GRUPO}_bigram_model.pkl\", file_is_training=False)\n",
    "bigram_model_BAC = BigramModel(f\"BAC_{GRUPO}_bigram_model.pkl\", file_is_training=False)\n",
    "\n",
    "trigram_model_20N = TrigramModel(f\"20N_{GRUPO}_trigram_model.pkl\", file_is_training=False)\n",
    "trigram_model_BAC = TrigramModel(f\"BAC_{GRUPO}_trigram_model.pkl\", file_is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51922b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate \n",
    "\n",
    "stats = [\n",
    "    [\"BAC - Unigram\", perplexity_unigram(modelo_de_unigramas_BAC, f\"BAC_{GRUPO}_testing.pkl\")],\n",
    "    [\"20N - Unigram\", perplexity_unigram(modelo_de_unigramas_20N, f\"20N_{GRUPO}_testing.pkl\")],\n",
    "    [\"20N - Bigram\", perplexity_bigram_from_model(modelo_de_bigramas_20N, f\"20N_{GRUPO}_testing.pkl\")],\n",
    "    [\"BAC - Bigram\", perplexity_bigram_from_model(modelo_de_bigramas_BAC, f\"BAC_{GRUPO}_testing.pkl\")],\n",
    "    [\"20N - Trigram\", perplexity_trigram_from_model(modelo_de_trigramas_20N, f\"20N_{GRUPO}_testing.pkl\")],\n",
    "    [\"BAC - Trigram\", perplexity_trigram_from_model(modelo_de_trigramas_BAC, f\"BAC_{GRUPO}_testing.pkl\")],\n",
    "]\n",
    "\n",
    "print(tabulate(stats, headers=[\"Modelo\", \"Perplejidad\"], tablefmt=\"fancy_grid\", floatfmt=\".4f\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
