{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89bf0ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Acompañar al estudiante como pareja de estudio en la comprensión y preparación de las lecturas antes de clase. Durante la clase, liderar la conversación grupal de los estudiantes guiándolos en el cruce de las lecturas con diversos tipos de materiales escritos y audiovisuales, para entretejer los temas del curso desde lo espacial y lo biográfico con tres enfoques: histórico, analítico y metodológico.',\n",
       " 'deprecated (reemplazado por Negam)',\n",
       " 'deprecated (reemplazado por GramGuia)',\n",
       " 'deprecated (reemplazado por SantosGPT)',\n",
       " 'Apoyar el aprendizaje reflexivo y ético del estudiante, facilitando la reconstrucción y análisis de momentos significativos de su práctica clínica (Psicología). El asistente ayuda a organizar ideas, profundizar en la comprensión de la interacción terapéutica y preparar al estudiante para una participación activa y fundamentada en la supervisión.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_excel(\"Catalogo GPTS.xlsx\")\n",
    "corpus = df[\"Propósito\"].astype(str).tolist()\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7de38193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/erich/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/erich/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/erich/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')  \n",
    "nltk.download('omw-1.4')  \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# remove stopwords, punctuation, and normalize the corpus\n",
    "stop = set(stopwords.words('spanish'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "clean_corpus = [clean(doc).split() for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c41a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Creating document-term matrix \n",
    "dictionary = corpora.Dictionary(clean_corpus)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in clean_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "929fd335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.405*\"asistente\" + 0.332*\"estudiantes\" + 0.202*\"propósito\"'), (1, '0.241*\"crítico\" + -0.194*\"rol\" + 0.157*\"análisis\"'), (2, '-0.158*\"gram\" + 0.136*\"análisis\" + 0.128*\"amazonía\"'), (3, '0.179*\"académico\" + 0.165*\"profesora\" + 0.164*\"voz\"'), (4, '0.212*\"estudiantes\" + -0.151*\"claro\" + -0.143*\"ético\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "# LSA model\n",
    "lsa = LsiModel(doc_term_matrix, num_topics=5, id2word = dictionary)\n",
    "\n",
    "# LSA model\n",
    "print(lsa.print_topics(num_topics=5, num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc1a90e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.019*\"asistente\" + 0.010*\"estudiantes\" + 0.009*\"aprendizaje\"'), (1, '0.011*\"comprensión\" + 0.011*\"estudiantes\" + 0.011*\"asistente\"'), (2, '0.022*\"estudiantes\" + 0.017*\"nan\" + 0.014*\"asistente\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "lda = LdaModel(corpus=doc_term_matrix, id2word=dictionary, num_topics=3, passes=15)\n",
    "print(lda.print_topics(num_topics=5, num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26802698",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_alpha \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_stop]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# corpus de df[\"Propósito\"]\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m texts \u001b[38;5;241m=\u001b[39m [preprocess(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPropósito\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# diccionario y matriz\u001b[39;00m\n\u001b[1;32m     17\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary(texts)\n",
      "Cell \u001b[0;32mIn[24], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_alpha \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_stop]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# corpus de df[\"Propósito\"]\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPropósito\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# diccionario y matriz\u001b[39;00m\n\u001b[1;32m     17\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary(texts)\n",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess\u001b[39m(doc):\n\u001b[0;32m---> 10\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m())\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_alpha \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_stop]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# cargar modelo de spacy en español (pip install spacy && python -m spacy download es_core_news_sm)\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def preprocess(doc):\n",
    "    doc = nlp(doc.lower())\n",
    "    return [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "# corpus de df[\"Propósito\"]\n",
    "texts = [preprocess(doc) for doc in df[\"Propósito\"]]\n",
    "\n",
    "# diccionario y matriz\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "doc_term_matrix = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ad6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "lda = LdaModel(corpus=doc_term_matrix, id2word=dictionary, num_topics=5, passes=15)\n",
    "for idx, topic in lda.print_topics(num_topics=5, num_words=5):\n",
    "    print(f\"Tópico {idx}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd55021",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for idx, topic in lsa.print_topics(num_topics=5, num_words=3):\n",
    "    words = [w.split(\"*\")[1].strip().replace('\"', \"\") for w in topic.split(\"+\")]\n",
    "    labels[idx] = \", \".join(words)\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557a2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas de tópicos:\n",
      "{0: 'estudi, curs, analisis, clinic', 1: 'estudi, asistent, gui, econom', 2: 'asistent, estudi, critic, proposit', 3: 'estudi, curs, proces, equip', 4: 'nan, aprendizaj, activ, asistent'}\n",
      "                                           Propósito  Tópico  \\\n",
      "0  Acompañar al estudiante como pareja de estudio...       1   \n",
      "1                 deprecated (reemplazado por Negam)       4   \n",
      "2              deprecated (reemplazado por GramGuia)       4   \n",
      "3             deprecated (reemplazado por SantosGPT)       4   \n",
      "4  Apoyar el aprendizaje reflexivo y ético del es...       4   \n",
      "\n",
      "                           Etiqueta  \n",
      "0     estudi, asistent, gui, econom  \n",
      "1  nan, aprendizaj, activ, asistent  \n",
      "2  nan, aprendizaj, activ, asistent  \n",
      "3  nan, aprendizaj, activ, asistent  \n",
      "4  nan, aprendizaj, activ, asistent  \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_excel(\"Catalogo GPTS.xlsx\")\n",
    "corpus = df[\"Propósito\"].astype(str).tolist()\n",
    "corpus[:5]\n",
    "# --- Preprocesamiento ---\n",
    "# Descargar recursos NLTK si no están aún\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "stop = set(stopwords.words(\"spanish\"))\n",
    "exclude = set(string.punctuation)\n",
    "stemmer = SpanishStemmer()\n",
    "\n",
    "def clean(doc):\n",
    "    if not isinstance(doc, str):  # evita errores con NaN o floats\n",
    "        return []\n",
    "    \n",
    "    doc = doc.lower()\n",
    "    stop_free = \" \".join([i for i in doc.split() if i not in stop])\n",
    "    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(stemmer.stem(word) for word in punc_free.split())\n",
    "    return normalized.split()\n",
    "\n",
    "# Corpus de descripciones\n",
    "corpus_raw = df[\"Propósito\"].astype(str).tolist()\n",
    "clean_corpus = [clean(doc) for doc in corpus_raw]\n",
    "\n",
    "# --- Diccionario y matriz documento-término ---\n",
    "dictionary = corpora.Dictionary(clean_corpus)\n",
    "doc_term_matrix = [dictionary.doc2bow(text) for text in clean_corpus]\n",
    "\n",
    "# --- Modelo LDA ---\n",
    "lda = LdaModel(corpus=doc_term_matrix, id2word=dictionary, num_topics=5, passes=15, random_state=42)\n",
    "\n",
    "# --- Obtener etiquetas de los tópicos ---\n",
    "labels = {}\n",
    "for idx, topic in lda.print_topics(num_topics=5, num_words=4):\n",
    "    words = [w.split(\"*\")[1].replace('\"', '').strip() for w in topic.split(\"+\")]\n",
    "    labels[idx] = \", \".join(words)\n",
    "\n",
    "print(\"Etiquetas de tópicos:\")\n",
    "print(labels)\n",
    "\n",
    "# --- Asignar tópico dominante a cada documento ---\n",
    "doc_topics = [max(lda[doc], key=lambda x: x[1])[0] for doc in doc_term_matrix]\n",
    "df[\"Tópico\"] = doc_topics\n",
    "df[\"Etiqueta\"] = df[\"Tópico\"].map(labels)\n",
    "\n",
    "# Ver resultados\n",
    "print(df[[\"Propósito\", \"Tópico\", \"Etiqueta\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cfb6d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listo. Archivo guardado: Catalogo GPTS - Etiquetado.xlsx\n",
      "                                           Propósito          Etiqueta_1  \\\n",
      "0  Acompañar al estudiante como pareja de estudio...               Clase   \n",
      "1                 deprecated (reemplazado por Negam)          Deprecated   \n",
      "2              deprecated (reemplazado por GramGuia)          Deprecated   \n",
      "3             deprecated (reemplazado por SantosGPT)          Deprecated   \n",
      "4  Apoyar el aprendizaje reflexivo y ético del es...  Apoyar aprendizaje   \n",
      "5  deprecated (reemplazado por prompt portatil de...          Deprecated   \n",
      "6  El asistente Comunidad sitúa al estudiante en ...           Comunidad   \n",
      "7  Apoyar a los grupos de estudiantes, durante la...      Identificación   \n",
      "8  Apoyar a los estudiantes en el desarrollo de u...           Analítico   \n",
      "9  Retroalimentador de historias clínicas, enfoca...             Clínico   \n",
      "\n",
      "           Etiqueta_2        Etiqueta_3              Etiqueta_4  \\\n",
      "0           Analítico      Metodológico            Conversación   \n",
      "1         Reemplazado             Negam  Deprecated reemplazado   \n",
      "2         Reemplazado          Gramguia  Deprecated reemplazado   \n",
      "3         Reemplazado         Santosgpt  Deprecated reemplazado   \n",
      "4      Reconstrucción             Ético              Estudiante   \n",
      "5         Reemplazado         Políticas         Prompt portatil   \n",
      "6             Víctima    Rol integrante               Conflicto   \n",
      "7     Caracterización   Latinoamericano           Investigación   \n",
      "8                Área           Trabajo                 Énfasis   \n",
      "9  Historias clínicas  Rotación clínica        Enfocado mejorar   \n",
      "\n",
      "              Etiqueta_5  \n",
      "0             Estudiante  \n",
      "1      Reemplazado negam  \n",
      "2   Reemplazado gramguia  \n",
      "3  Reemplazado santosgpt  \n",
      "4               Práctica  \n",
      "5          Restricciones  \n",
      "6      Víctimas directas  \n",
      "7               Contexto  \n",
      "8             Desarrollo  \n",
      "9        Quinto semestre  \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# --- NLP: spaCy (lemmatización) ---\n",
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "except OSError:\n",
    "    # Si el modelo no está instalado: ejecuta en tu terminal:\n",
    "    #   python -m spacy download es_core_news_sm\n",
    "    raise RuntimeError(\"Falta el modelo 'es_core_news_sm'. Instálalo y vuelve a ejecutar.\")\n",
    "\n",
    "# --- Keyphrase & TF-IDF ---\n",
    "import yake\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# -------------------------\n",
    "# Utilidades\n",
    "# -------------------------\n",
    "SPANISH_STOP = nlp.Defaults.stop_words\n",
    "\n",
    "VALID_POS = {\"NOUN\", \"PROPN\", \"ADJ\", \"VERB\"}  # Ajusta si quieres más/menos clases\n",
    "PUNCT_RE = re.compile(r\"[^\\wáéíóúñüÁÉÍÓÚÑÜ#@]+\")\n",
    "\n",
    "def normalize_space(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def spacy_lemmatize(text: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Devuelve:\n",
    "      - 'lemmas': lista de lemas filtrados por POS/stopwords\n",
    "      - 'noun_chunks': frases nominales normalizadas (texto superficial, no lemas)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if pd.isna(text) else str(text)\n",
    "    doc = nlp(text.lower())\n",
    "\n",
    "    lemmas = []\n",
    "    for tok in doc:\n",
    "        if (tok.is_alpha or tok.text.isalnum()) and tok.pos_ in VALID_POS and tok.lemma_ not in SPANISH_STOP:\n",
    "            # Normaliza tokens y elimina puntuación rara\n",
    "            t = PUNCT_RE.sub(\" \", tok.lemma_)\n",
    "            t = normalize_space(t)\n",
    "            if t:\n",
    "                lemmas.append(t)\n",
    "\n",
    "    # noun chunks (frases nominales)\n",
    "    chunks = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        c = chunk.text.lower()\n",
    "        c = PUNCT_RE.sub(\" \", c)\n",
    "        c = \" \".join([t for t in c.split() if t not in SPANISH_STOP])\n",
    "        c = normalize_space(c)\n",
    "        if len(c) > 2:\n",
    "            chunks.append(c)\n",
    "\n",
    "    return {\"lemmas\": lemmas, \"noun_chunks\": chunks}\n",
    "\n",
    "def jaccard(a: str, b: str) -> float:\n",
    "    sa, sb = set(a.split()), set(b.split())\n",
    "    if not sa or not sb:\n",
    "        return 0.0\n",
    "    inter = len(sa & sb)\n",
    "    union = len(sa | sb)\n",
    "    return inter / union if union else 0.0\n",
    "\n",
    "def mmr_select(cands: List[str], scores: List[float], top_n: int = 5, lambda_mult: float = 0.7) -> List[str]:\n",
    "    \"\"\"\n",
    "    Maximal Marginal Relevance para diversificar etiquetas.\n",
    "    \"\"\"\n",
    "    if not cands:\n",
    "        return []\n",
    "    selected = []\n",
    "    cand_idx = list(range(len(cands)))\n",
    "    # arranque: mejor score\n",
    "    best = max(cand_idx, key=lambda i: scores[i])\n",
    "    selected.append(best)\n",
    "    cand_idx.remove(best)\n",
    "\n",
    "    while len(selected) < min(top_n, len(cands)):\n",
    "        def utility(i):\n",
    "            sim_to_sel = 0.0\n",
    "            if selected:\n",
    "                sim_to_sel = max(jaccard(cands[i], cands[j]) for j in selected)\n",
    "            return lambda_mult * scores[i] - (1 - lambda_mult) * sim_to_sel\n",
    "\n",
    "        next_i = max(cand_idx, key=utility)\n",
    "        selected.append(next_i)\n",
    "        cand_idx.remove(next_i)\n",
    "\n",
    "    return [cands[i] for i in selected]\n",
    "\n",
    "# -------------------------\n",
    "# Cargar datos\n",
    "# -------------------------\n",
    "df = pd.read_excel(\"Catalogo GPTS.xlsx\")\n",
    "if \"Propósito\" not in df.columns:\n",
    "    raise ValueError(\"No se encontró la columna 'Propósito' en el Excel.\")\n",
    "\n",
    "texts = df[\"Propósito\"].astype(str).fillna(\"\")\n",
    "\n",
    "# -------------------------\n",
    "# Preprocesamiento\n",
    "# -------------------------\n",
    "proc = [spacy_lemmatize(t) for t in texts]\n",
    "lemmas_docs = [\" \".join(p[\"lemmas\"]) for p in proc]  # texto lematizado para TF-IDF\n",
    "noun_chunks_docs = [p[\"noun_chunks\"] for p in proc]\n",
    "\n",
    "# -------------------------\n",
    "# TF-IDF sobre lemas (1-3gramas)\n",
    "# -------------------------\n",
    "tfidf = TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 3), min_df=2)  # min_df=2 reduce ruido\n",
    "X = tfidf.fit_transform(lemmas_docs)\n",
    "vocab = tfidf.get_feature_names_out()\n",
    "\n",
    "def top_tfidf_terms(row_idx: int, top_k: int = 30) -> List[Tuple[str, float]]:\n",
    "    row = X[row_idx]\n",
    "    if row.nnz == 0:\n",
    "        return []\n",
    "    coo = row.tocoo()\n",
    "    pairs = list(zip(coo.col, coo.data))\n",
    "    pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    top = pairs[:top_k]\n",
    "    return [(vocab[c], float(v)) for c, v in top]\n",
    "\n",
    "# -------------------------\n",
    "# YAKE por documento\n",
    "# -------------------------\n",
    "kw_extractor = yake.KeywordExtractor(\n",
    "    lan=\"es\", n=3,  # hasta trigramas\n",
    "    dedupLim=0.9,  # evita duplicados muy cercanos\n",
    "    windowsSize=1,\n",
    "    top=30,  # extrae bastantes; luego combinamos\n",
    ")\n",
    "\n",
    "def yake_terms(text: str) -> List[Tuple[str, float]]:\n",
    "    # YAKE devuelve score \"menor es mejor\"; invertiremos más adelante\n",
    "    kws = kw_extractor.extract_keywords(text)\n",
    "    # Normaliza textos\n",
    "    cleaned = []\n",
    "    for key, score in kws:\n",
    "        k = key.lower()\n",
    "        k = PUNCT_RE.sub(\" \", k)\n",
    "        k = \" \".join(w for w in k.split() if w not in SPANISH_STOP)\n",
    "        k = normalize_space(k)\n",
    "        if len(k) > 2:\n",
    "            cleaned.append((k, float(score)))\n",
    "    # Elimina duplicados conservando el mejor score\n",
    "    uniq: Dict[str, float] = {}\n",
    "    for k, s in cleaned:\n",
    "        if k not in uniq or s < uniq[k]:\n",
    "            uniq[k] = s\n",
    "    return list(uniq.items())\n",
    "\n",
    "# -------------------------\n",
    "# Fusión de candidatos y selección de 5 etiquetas\n",
    "# -------------------------\n",
    "all_etiquetas = []\n",
    "for i, original_text in enumerate(texts):\n",
    "    # candidatos: noun chunks + YAKE + TF-IDF\n",
    "    tfidf_list = top_tfidf_terms(i, top_k=30)\n",
    "    yake_list = yake_terms(original_text)\n",
    "\n",
    "    cand_set = {}\n",
    "    # Añade noun chunks con score base muy bajo (serán reponderados)\n",
    "    for c in noun_chunks_docs[i]:\n",
    "        cand_set[c] = {\"tfidf\": 0.0, \"yake\": None}\n",
    "    # Añade TF-IDF\n",
    "    for term, val in tfidf_list:\n",
    "        cand_set[term] = cand_set.get(term, {\"tfidf\": 0.0, \"yake\": None})\n",
    "        cand_set[term][\"tfidf\"] = max(cand_set[term][\"tfidf\"], val)\n",
    "    # Añade YAKE\n",
    "    for term, yscore in yake_list:\n",
    "        cand_set[term] = cand_set.get(term, {\"tfidf\": 0.0, \"yake\": None})\n",
    "        # YAKE: menor es mejor -> invertimos\n",
    "        cand_set[term][\"yake\"] = yscore\n",
    "\n",
    "    if not cand_set:\n",
    "        all_etiquetas.append([\"sin etiqueta\"])\n",
    "        continue\n",
    "\n",
    "    # Prepara vectores de puntuación\n",
    "    cands = list(cand_set.keys())\n",
    "    tfidf_scores = [cand_set[c][\"tfidf\"] for c in cands]\n",
    "    yake_scores_raw = [cand_set[c][\"yake\"] if cand_set[c][\"yake\"] is not None else None for c in cands]\n",
    "\n",
    "    # Normaliza TF-IDF 0..1\n",
    "    if max(tfidf_scores) > 0:\n",
    "        tfidf_norm = minmax_scale(tfidf_scores)\n",
    "    else:\n",
    "        tfidf_norm = [0.0] * len(cands)\n",
    "\n",
    "    # Normaliza YAKE a 0..1 con \"mayor es mejor\":\n",
    "    #   - invertimos: score_inv = 1 / (1 + raw)\n",
    "    #   - minmax sobre los que tienen valor\n",
    "    inv = []\n",
    "    for s in yake_scores_raw:\n",
    "        inv.append(None if s is None else 1.0 / (1.0 + s))\n",
    "\n",
    "    # Rellena None con 0\n",
    "    inv_filled = [0.0 if v is None else v for v in inv]\n",
    "    if any(v > 0 for v in inv_filled):\n",
    "        yake_norm = minmax_scale(inv_filled)\n",
    "    else:\n",
    "        yake_norm = [0.0] * len(cands)\n",
    "\n",
    "    # Combina con media armónica (premia altos en ambas)\n",
    "    combined = []\n",
    "    for t, y in zip(tfidf_norm, yake_norm):\n",
    "        if t == 0 and y == 0:\n",
    "            combined.append(0.0)\n",
    "        elif t == 0 or y == 0:\n",
    "            combined.append(0.5 * max(t, y))\n",
    "        else:\n",
    "            combined.append(2 * (t * y) / (t + y))\n",
    "\n",
    "    # Selección diversa (5 etiquetas)\n",
    "    selected = mmr_select(cands, combined, top_n=5, lambda_mult=0.72)\n",
    "\n",
    "    # Pulido final de etiquetas (titular-style)\n",
    "    def pretty(tag: str) -> str:\n",
    "        # Quita dobles espacios, recorta, y capitaliza primera letra\n",
    "        tag = normalize_space(tag)\n",
    "        return tag if tag.startswith(\"#\") or tag.startswith(\"@\") else tag.capitalize()\n",
    "\n",
    "    selected = [pretty(s) for s in selected]\n",
    "    all_etiquetas.append(selected)\n",
    "\n",
    "# -------------------------\n",
    "# Salida\n",
    "# -------------------------\n",
    "df[\"Etiqueta_1\"] = [etq[0] if len(etq) > 0 else \"\" for etq in all_etiquetas]\n",
    "df[\"Etiqueta_2\"] = [etq[1] if len(etq) > 1 else \"\" for etq in all_etiquetas]\n",
    "df[\"Etiqueta_3\"] = [etq[2] if len(etq) > 2 else \"\" for etq in all_etiquetas]\n",
    "df[\"Etiqueta_4\"] = [etq[3] if len(etq) > 3 else \"\" for etq in all_etiquetas]\n",
    "df[\"Etiqueta_5\"] = [etq[4] if len(etq) > 4 else \"\" for etq in all_etiquetas]\n",
    "df[\"Etiquetas\"] = [\", \".join(etq) for etq in all_etiquetas]\n",
    "\n",
    "# Guarda un nuevo Excel\n",
    "out_path = \"Catalogo GPTS - Etiquetado.xlsx\"\n",
    "df.to_excel(out_path, index=False)\n",
    "print(f\"Listo. Archivo guardado: {out_path}\")\n",
    "print(df[[\"Propósito\", \"Etiqueta_1\", \"Etiqueta_2\", \"Etiqueta_3\", \"Etiqueta_4\", \"Etiqueta_5\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a53195d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using node v22.17.0 (npm v10.9.2)\n",
      "Collecting yake\n",
      "  Downloading yake-0.6.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click>=6.0 in /home/erich/dev/Curso_NLP/.venv/lib/python3.10/site-packages (from yake) (8.2.1)\n",
      "Collecting jellyfish (from yake)\n",
      "  Downloading jellyfish-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: networkx in /home/erich/dev/Curso_NLP/.venv/lib/python3.10/site-packages (from yake) (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /home/erich/dev/Curso_NLP/.venv/lib/python3.10/site-packages (from yake) (1.26.4)\n",
      "Collecting segtok (from yake)\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: tabulate in /home/erich/dev/Curso_NLP/.venv/lib/python3.10/site-packages (from yake) (0.9.0)\n",
      "Requirement already satisfied: regex in /home/erich/dev/Curso_NLP/.venv/lib/python3.10/site-packages (from segtok->yake) (2025.7.34)\n",
      "Downloading yake-0.6.0-py3-none-any.whl (80 kB)\n",
      "Downloading jellyfish-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (357 kB)\n",
      "Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: segtok, jellyfish, yake\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [yake]\n",
      "\u001b[1A\u001b[2KSuccessfully installed jellyfish-1.2.0 segtok-1.5.11 yake-0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yake"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
