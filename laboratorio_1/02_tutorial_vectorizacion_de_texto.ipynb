{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wq8_dqAU4-Q"
   },
   "source": [
    "# Introducción\n",
    "\n",
    "## Descripción\n",
    "Con este notebook de Python se busca introducir la aplicación de técnicas de representación de texto como la bolsa de palabras (BOW por sus siglas en inglés). La representación del texto consiste en transformar el texto (sequencia de símbolos) a una representación más apropiada que permita el uso de distintos algoritmos para así abordar tareas de NLP. Si se quieren utilizar técnicas de machine learning clásico, la etapa de representación del texto implica la extración de características del texto expresadas como un vector del mismo tamaño para todos los documentos con valores númericos.\n",
    "\n",
    "Las técnicas de repsentación de texto que se verán, son:\n",
    "* Bolsa de palabras.\n",
    "* Matriz tf-idf.\n",
    "\n",
    "En este tutorial también se mostrará como hacer uso de la librería de Python [scikit-learn](https://scikit-learn.org/stable/index.html) para realizar la extracción de características del texto.\n",
    "\n",
    "**Los objetivos de aprendizaje son**:\n",
    "\n",
    "1. Explorar técnicas de representación de texto.\n",
    "2. Hacer uso de librerías para realizar extracción de características del texto.\n",
    "\n",
    "## Metodología\n",
    "Este notebook será un tutorial para aprender a instalar y usar **scikit-learn** con el fin de aplicar distintas técnicas de extracción de características del texto. Adicionalmente, se referirá al estudiante a la documentación de la librería y de los métodos vistos para que pueda ampliar la información sobre su uso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RA7wbVV8U4-S"
   },
   "source": [
    "# Instalación de la librería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wl9lD9dOU4-S"
   },
   "outputs": [],
   "source": [
    "# Para instalar localmente, descomente la siguiente línea\n",
    "# pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MbcN_cnU4-T"
   },
   "source": [
    "Importación de los módulos a usar en el tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qjrRak-BU4-T"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kq0oqhJsU4-T"
   },
   "source": [
    "# Representación de bolsa de palabras (BOW)\n",
    "\n",
    "La librería scikit-learn nos permite realizar de manera sencilla la transformación del texto a una representación de bolsa de palabras. Esta representación consiste en representar cada documento como un conjunto que registra la ocurrencia de cada una de las palabras del vocabulario en el documento.\n",
    "\n",
    "Para realizar esta transformación, harémos uso de la clase [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), la cual realiza esta vectorización con una interfaz sencilla, pero fácilmente ajustable a nuestras necesidades específicas.\n",
    "\n",
    "El caso más simple de uso de CountVectorizer, se compone de dos pasos. El primero, es la inicialización de la clase, y el segundo la transformación del corpus de documentos en los vectores de ocurrencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hRQ47PqpU4-T",
    "outputId": "e47b1515-10d7-45eb-cb10-78cf2991fb0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: ['is' 'test' 'the' 'this']\n",
      "Dimensiones de la matriz X: (3, 4)\n",
      "Contenido de la matriz X:\n",
      " [[2 1 1 1]\n",
      " [1 1 3 1]\n",
      " [1 1 1 1]]\n",
      "Cantidad de documentos: 3\n",
      "Cantidad de palabras: 4\n",
      "Cantidad de ocurrencias: 15\n",
      "Tokens en el documento [[5]\n",
      " [6]\n",
      " [4]]\n",
      "Ocurrencias por token [[4 3 5 3]]\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X = count_vect.fit_transform(['This is is the test 1', 'This is the the the test 2', 'This is the test 3'])\n",
    "\n",
    "print('Vocabulario:', count_vect.get_feature_names_out())\n",
    "\n",
    "print('Dimensiones de la matriz X:', X.shape)\n",
    "\n",
    "print('Contenido de la matriz X:\\n', X.toarray())\n",
    "\n",
    "print('Cantidad de documentos:', X.shape[0])\n",
    "print('Cantidad de palabras:', X.shape[1])\n",
    "print('Cantidad de ocurrencias:', X.sum())\n",
    "\n",
    "# Ocurrencias por documento\n",
    "print('Tokens en el documento', X.sum(axis=1))\n",
    "\n",
    "# Ocurrencias por palabra\n",
    "print('Ocurrencias por token', X.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdZLdK_5U4-U"
   },
   "source": [
    "Con los valores por defecto CountVectorizer realiza un procesamiento del texto y una transformación que se compone de los siguientes pasos:\n",
    "\n",
    "1. Se realiza una tokenización del texto en donde solo se conservan las secuencias de caracteres de 2 o más caracteres alfanuméricos.\n",
    "2. Se realiza una normalización de los tokens en donde se transforman a minúsculas.\n",
    "3. Se contruye el vocabulario compuesto de todos los tokens presentes en el corpus.\n",
    "4. Se construye la matriz de ocurrencia de términos con los documentos como filas, y el índice de los tokens del vocabulario como columnas.\n",
    "\n",
    "Nota: La matriz resultado es un objeto tipo `scipy.sparse` para hacer un uso eficiente de la memoria, ya que al representar cada documento con un vector de tamaño del vocabulario, estos son muy grandes y están compuestos principalmente por ceros (dispersos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3QXpkyaU4-U"
   },
   "source": [
    "Para explorar la representación vectorial de cada documento, puede ver el vector mediante el índice del documento en el corpus. Por ejemplo, a continuación se muestra el vector correspondiente al primer documento del corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFgUbOI5U4-V",
    "outputId": "c25946f2-9cd3-446a-e2ce-d1597cc9bb5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 1, 1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxEWyNWXWAkz"
   },
   "source": [
    "Reflexione un momento en la razón por la cual este vector tiene un \"2\" en una de sus dimensiones. ¿Que palabra se repite dos veces en este documento?, ¿Volviendo al vector del vocabulario en que posición/dimensión se encuntra dicha palabra?, ¿Coincide con la dimensión donde apararece el 2 en el documento?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDdisgURU4-V"
   },
   "source": [
    "También puede observar facilmente los términos del vocabulario presentes en el primer documento usando las posiciones diferentes de zero del vector que representa el documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bxhVY3vgU4-V",
    "outputId": "0681efc6-bc9f-4719-ba36-15d8eec191b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['this', 'is', 'the', 'test'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names_out()[X[0].nonzero()[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDAhv6ilX_A_"
   },
   "source": [
    "En este ejercicio el vocabulario es extremandamente pequeño, solo 4 tokens/palabras, y es por eso que los vectores que representan a los documentos no son dispersos. En un escenario real con vocabularios compuestos por miles de palabras distintas esto no va a ocurrir, y la mayor parte de las posiciones resultantes del vector que representa a un documento van a ser cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_e3qB9fU4-W"
   },
   "source": [
    "## Controlar el procesamiento de texto\n",
    "\n",
    "En algunas situaciones, puede ser útil el poder configurar la etapa de procesamiento de texto de acuerdo con nuestras necesidades. Para lograrlo, podemos pasar distintos parámetros en la incialización de CountVectorizar que nos permiten ajustar la tokenización del texto, y las técnicas de normalización que queremos aplicar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4gW5x04U4-W"
   },
   "source": [
    "### Transformación a minúsculas\n",
    "\n",
    "La transformación a minúsculas se hace por defecto, pero se puede desactivar mediante el argumento **lowercase**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peX14gy6U4-W",
    "outputId": "91f79140-ed77-47dc-e452-f8498f9cc9ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hola', 'cómo', 'estás'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(lowercase=False)\n",
    "sample = count_vect.fit_transform(['Hola, ¿cómo estás?'])\n",
    "count_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24nxpokxU4-W"
   },
   "source": [
    "### Remover acentos\n",
    "\n",
    "Si se quieren eliminar la acentuación de los tokens, podemos pasar como argumento a **strip_accents** el valor de `unicode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyQ3yvCjU4-W",
    "outputId": "ee5794f4-1d2a-433e-8ab4-40e811f604e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['como', 'estas', 'hola'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(strip_accents='unicode')\n",
    "sample = count_vect.fit_transform(['Hola, ¿cómo estás?'])\n",
    "count_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bxvx3-WcU4-X"
   },
   "source": [
    "### Remover palabras de parada\n",
    "Para remover las palabras de parada del corpus, podemos pasar un listado con las palabras que queremos ignorar en el argumento **stop_words**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTonQ94uU4-X",
    "outputId": "bd19adf0-5119-43bc-d09c-d0034819c78c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cómo', 'hola'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "count_vect = CountVectorizer(stop_words=stops)\n",
    "sample = count_vect.fit_transform(['Hola, ¿cómo estás?'])\n",
    "count_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5kArQ4SU4-X"
   },
   "source": [
    "### Modificar la tokenización\n",
    "\n",
    "Si queremos definir los tokens según criterios diferentes a los usados por defecto, podemos pasar una función en donde definamos la tokenización o podemos pasar un patrón de expresiones regulares con nuestras reglas de tokenización.\n",
    "\n",
    "Vea en el ejemplo siguiente como los números que contienen comas o puntos se covierten en varios tokens, solo se convierte en un token las palabras unidas por un guíon, las siglas se ignoran, al igual que los signos de puntuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2ZM4tS4U4-X",
    "outputId": "7ca5b7e3-0389-4be5-f76a-aa4b4a0ab57b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hola', 'cómo', 'estás', '234', '56', '233', '123', 'test',\n",
       "       'mi_nombre'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "sample = count_vect.fit_transform(['Hola, ¿cómo estás? 1,234.56, 1,233,123 test-test U.S.A. mi_nombre'])\n",
    "count_vect.get_feature_names_out()[sample[0].nonzero()[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rB6HvCTSU4-X"
   },
   "source": [
    "A continuación definimos nuestra propia regla de tokenización por medio de una expresión regular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUzrX8xHU4-X",
    "outputId": "ffd9e9e1-bdb7-4ee8-cd3c-c68dcc4093b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hola', ',', '¿', 'cómo', 'estás', '?', '1,234.56', '1,233,123',\n",
       "       'test-test', 'u.s.a.', 'mi_nombre'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = r\"\"\"\n",
    "(?x) # Permite agregar comentarios y espacios en la expresión\n",
    "(?:(?:\\d+,?)+(?:\\.\\d+)?) # números con comas y puntos como decimales\n",
    "| (?:[^\\W\\d_]\\.)+ # Palabras con puntos (abreviaciones)\n",
    "| \\w+(?:-\\w+)* # Palabras, acepta guiones intermedios (test-test)\n",
    "| ['\\\"¡¿.,!?]+ # Acepta signos de puntuación\n",
    "\"\"\"\n",
    "count_vect = CountVectorizer(token_pattern=regex)\n",
    "sample = count_vect.fit_transform(['Hola, ¿cómo estás? 1,234.56, 1,233,123 test-test U.S.A. mi_nombre'])\n",
    "count_vect.get_feature_names_out()[sample[0].nonzero()[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CDMo6BhU4-X"
   },
   "source": [
    "## Controlando la creación del vocabulario\n",
    "\n",
    "Para crear una representación de bolsa de palabras, primero se realiza la creación del vocabulario. Por defecto, el vocabulario contiene todos los tokens presentes en el corpus. Sin embargo, **scikit-learn** nos permite controlar como se crea el vocabulario, definir que palabras ignorar, o definir el vocabulario a utilizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeYilvbQU4-Y"
   },
   "source": [
    "En este ejemplo, vea como podemos predefinir el vocabulario con el cual se va a crear la representación de bolsa de palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TF6qxLgYU4-Y",
    "outputId": "0d72cbbf-8587-45bb-fffe-5e526d03f249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario creado: ['hola' 'como' 'estas' 'test']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 0, 0, 2],\n",
       "       [0, 0, 0, 3]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = { # Los índices (valores) deben ser únicos e ir de 0 a N-1, donde N es la cantidad de tokens en el vocabulario\n",
    "    'hola': 0,\n",
    "    'como': 1,\n",
    "    'estas': 2,\n",
    "    'test': 3,\n",
    "}\n",
    "\n",
    "count_vect = CountVectorizer(vocabulary=vocab)\n",
    "sample = count_vect.fit_transform(['Hola, ¿cómo estás?', 'este es el test test 2', 'test test test 3'])\n",
    "\n",
    "print('Vocabulario creado:', count_vect.get_feature_names_out())\n",
    "sample.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSHvnx61U4-Y"
   },
   "source": [
    "Nótese que las palabras pasadas en el vocabulario deben coincidir exactamente con las palabras tokenizadas y normalizadas de los documentos. Por ejemplo, en este caso las palabras del vocabulario \"como\" y \"estas\" tienen cero ocurrencias en los documentos debido a que en el vocabulario hay ausencia de las tildes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvJhOs7tU4-Y"
   },
   "source": [
    "## Limite el tamaño del vocabulario\n",
    "\n",
    "Para limitar el tamaño del vocabulario, mediante el argumento **max_features** puede definir la cantidad máxima de palabras presentes en el vocabulario, así el vocabulario solo conservará los tokens con mayor ocurrencia en el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQGwo01gU4-Y",
    "outputId": "8280bf85-dae3-402a-e578-546a2f2782ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario creado: ['cómo' 'el' 'test']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 2],\n",
       "       [0, 0, 3]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(max_features=3)\n",
    "sample = count_vect.fit_transform(['Hola, ¿cómo estás?', 'este es el test test 2', 'test test test 3'])\n",
    "\n",
    "print('Vocabulario creado:', count_vect.get_feature_names_out())\n",
    "sample.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmwyD1AhU4-Y"
   },
   "source": [
    "Adicionalmente, puede ignorar del vocabulario las palabras o tokens que están presentes en casi todos los documentos del corpus. Esto sería el equivalente a sleccionar palabras de parada de acuerdo a su corpus, es decir, palabras o tokens que al estar presentes en casi todos los documentos poseen un bajo poder discriminativo, al no conllevar información relevante para diferenciar los documentos.\n",
    "\n",
    "Para lograr esto puede definir el argumento **max_df**. En el vocabulario solo se agregarán los tokens con una frecuencia menor a max_df en el corpus, donde max_df puede ser una proporción del total de documentos o un valor absoluto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4od_qa7U4-Y",
    "outputId": "ab6fa03c-77df-42ed-b287-6bc30e880a3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario creado: ['cómo' 'el' 'es' 'este' 'estás' 'hola' 'otro']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(max_df=0.7)\n",
    "sample = count_vect.fit_transform(['Hola, ¿cómo estás?', 'este es el test test 2', 'test test test 3', 'Este es otro test'])\n",
    "\n",
    "print('Vocabulario creado:', count_vect.get_feature_names_out())\n",
    "sample.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kNnrKFdU4-Z"
   },
   "source": [
    "La palabra test que se repite en $3/4=0.75$ de los documentos, por lo tanto no se agrega al vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0jWDmL0U4-Z"
   },
   "source": [
    "También se puede ignorar palabras con una frecuencia por documento menor a un valor determinado, esta técnica puede ser útil para eliminar palabras poco representativas del corpus que podrían ser vistas como outliers o errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nAqPKi9U4-Z",
    "outputId": "dd9bea15-7034-4638-f0ac-8ea83dc95207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario creado: ['es' 'este' 'test']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [1, 1, 2],\n",
       "       [0, 0, 3],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(min_df=0.3)\n",
    "sample = count_vect.fit_transform(['Hola, ¿cómo estás?', 'este es el test test 2', 'test test test 3', 'Este es otro test'])\n",
    "\n",
    "print('Vocabulario creado:', count_vect.get_feature_names_out())\n",
    "sample.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8czJquyU4-Z"
   },
   "source": [
    "Para este caso, las palabras con presencia en solo uno de los 4 documentos se ignoran en la ceación del vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6g2yW4MjU4-Z"
   },
   "source": [
    "# Matriz tf-idf\n",
    "\n",
    "Esta representación de texto es una variante de la representación en bolsa de palabras, en donde en vez de registrar la ocurrencia se registra el score tf-idf, el cual es una función que pondera la ocurrencia del termino con el poder discriminativo del mismo.\n",
    "\n",
    "En **scikit-learn** podemos aplicar esta representación de manera muy similar a CountVectorizer con la clase [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSeeFGHZU4-Z",
    "outputId": "8f011d95-f577-469a-948a-b01196813133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario creado: ['cómo' 'el' 'es' 'este' 'estás' 'hola' 'otro' 'test']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.57735027, 0.        , 0.        , 0.        , 0.57735027,\n",
       "        0.57735027, 0.        , 0.        ],\n",
       "       [0.        , 0.50814302, 0.40062579, 0.40062579, 0.        ,\n",
       "        0.        , 0.        , 0.64868222],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.        , 0.4842629 , 0.4842629 , 0.        ,\n",
       "        0.        , 0.61422608, 0.39205255]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['Hola, ¿cómo estás?', 'este es el test test 2', 'test test test 3', 'Este es otro test']\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "sample = tfidf_vect.fit_transform(corpus)\n",
    "\n",
    "print('Vocabulario creado:', count_vect.get_feature_names_out())\n",
    "sample.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6P0y2mqU4-a"
   },
   "source": [
    "Los scores tf-idf de **scikit-learn** se calculan de la siguiente manera:\n",
    "\n",
    "$\\text{tf-idf} = \\text{tf} * \\text{idf}$, en donde **tf** es la ocurrencia del término en el corpus.\n",
    "\n",
    "El **idf** se obtiene así:\n",
    "\n",
    "$\\text{idf} =  \\ln({1 + n\\over 1 + df}) + 1$, donde **n** es el número de documentos en el corpus, y **df** es el número de documentos en el corpus que contienen el término.\n",
    "\n",
    "Finalmente, se normaliza cada vector de documento, de modo que cada vector tenga una norma de una unidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugz-hI6gU4-a",
    "outputId": "5db4b93f-d054-4578-87ba-01f5870b472d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57735027, 0.        , 0.        , 0.        , 0.57735027,\n",
       "        0.57735027, 0.        , 0.        ],\n",
       "       [0.        , 0.50814302, 0.40062579, 0.40062579, 0.        ,\n",
       "        0.        , 0.        , 0.64868222],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.        , 0.4842629 , 0.4842629 , 0.        ,\n",
       "        0.        , 0.61422608, 0.39205255]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = CountVectorizer().fit_transform(corpus)\n",
    "tf = counts.toarray()\n",
    "df = np.where(tf > 0, 1, 0)\n",
    "\n",
    "idf = np.log((1 + df.shape[0]) / (1 + df.sum(axis=0))) + 1\n",
    "\n",
    "tfidf = tf * idf\n",
    "tfidf /= np.linalg.norm(tfidf, axis=1, keepdims=True)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4J3Yh96HU4-l"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
