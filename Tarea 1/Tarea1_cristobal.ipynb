{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26829,
     "status": "ok",
     "timestamp": 1756160547056,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "HebImrKX3YT5",
    "outputId": "afd32e1d-d0e7-47d1-b02e-cab88a6ee89e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using node v22.17.0 (npm v10.9.2)\n",
      "Collecting KafNafParserPy\n",
      "  Downloading KafNafParserPy-1.896.tar.gz (37 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting lxml (from KafNafParserPy)\n",
      "  Downloading lxml-6.0.1-cp310-cp310-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Downloading lxml-6.0.1-cp310-cp310-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: KafNafParserPy\n",
      "  Building wheel for KafNafParserPy (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for KafNafParserPy: filename=kafnafparserpy-1.896-py3-none-any.whl size=51545 sha256=60e33d389168699ec56f1d504f8cf6172118c21e753af522e859470a5528385c\n",
      "  Stored in directory: /home/erich/.cache/pip/wheels/a3/41/21/8d1958c241de897a3db26742c12141c5593be1981373523e67\n",
      "Successfully built KafNafParserPy\n",
      "Installing collected packages: lxml, KafNafParserPy\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [KafNafParserPy]\n",
      "\u001b[1A\u001b[2KSuccessfully installed KafNafParserPy-1.896 lxml-6.0.1\n",
      "Now using node v22.17.0 (npm v10.9.2)\n",
      "Requirement already satisfied: nltk in /home/erich/dev/Curso_NLP/.venv/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/erich/dev/Curso_NLP/.venv/lib/python3.10/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/erich/dev/Curso_NLP/.venv/lib/python3.10/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/erich/dev/Curso_NLP/.venv/lib/python3.10/site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in /home/erich/dev/Curso_NLP/.venv/lib/python3.10/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install KafNafParserPy\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 10335,
     "status": "ok",
     "timestamp": 1756160557395,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "NZxjyx-FBK22"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from KafNafParserPy import KafNafParser\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LMsk8ay2L9e"
   },
   "source": [
    "# Implementación métricas de evaluación de IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756160557400,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "5fuh4wGkmnsE"
   },
   "outputs": [],
   "source": [
    "def precision(query_relevance: list):\n",
    "  return sum(query_relevance)/len(query_relevance)\n",
    "\n",
    "def precision_at_k(query_relevance: list, k: int):\n",
    "  return sum(query_relevance[:k])/k\n",
    "\n",
    "def recall_at_k(query_relevance: list, relevant_docs: int, k: int):\n",
    "  return sum(query_relevance[:k])/relevant_docs\n",
    "\n",
    "def average_precision(query_relevance: list):\n",
    "  relevant_docs = sum(query_relevance)\n",
    "  found_relevant = 0\n",
    "  k = 0\n",
    "  acc_precision = 0\n",
    "  while found_relevant < relevant_docs:\n",
    "    if query_relevance[k] == 0:\n",
    "      k+=1\n",
    "      continue\n",
    "    acc_precision += precision(query_relevance[:k+1])\n",
    "    found_relevant += 1\n",
    "    k += 1\n",
    "  return acc_precision/relevant_docs\n",
    "\n",
    "def mean_average_precision(queries_relevance: list[list]):\n",
    "  return sum([average_precision(x) for x in queries_relevance])/len(queries_relevance)\n",
    "\n",
    "def dcg_at_k(query_relevance: list, k):\n",
    "    acc_dcg = 0\n",
    "    for i in range(k):\n",
    "        acc_dcg += query_relevance[i]/np.log2(max(i+1,2))\n",
    "    return acc_dcg\n",
    "\n",
    "def ndcg_at_k(query_relevance: list, k: int):\n",
    "    dcg_k = dcg_at_k(query_relevance, k)\n",
    "    best_dcg_k = dcg_at_k(sorted(query_relevance, reverse=True), k)\n",
    "    return dcg_k/best_dcg_k\n",
    "\n",
    "\n",
    "# precision([0, 0, 0, 1])\n",
    "# precision_at_k([0, 0, 0, 1], 1)\n",
    "# recall_at_k([0, 0, 0, 1], 4, 1)\n",
    "# average_precision([0, 1, 0, 1, 1, 1, 1])\n",
    "# mean_average_precision([[0, 1, 0, 1, 1, 1, 1], [0, 0, 0, 1]])\n",
    "# dcg_at_k([4,4,3,0,0,1,3,3,3,0], 6)\n",
    "# ndcg_at_k([4,3,4,2,0,0,0,1,1,0], 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJ9mLLiY2RZi"
   },
   "source": [
    "# Implementación de motores de búsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7byIi5YixpL"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMgUaIS6i62k"
   },
   "source": [
    "#### NLTK Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 426,
     "status": "ok",
     "timestamp": 1756160557829,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "0lGIYQcL-LN3",
    "outputId": "aaa6a41a-1c5d-4908-8f66-e4a6d94e5cd0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/erich/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/erich/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "wptk = nltk.WordPunctTokenizer()\n",
    "ps = PorterStemmer()\n",
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRQCNKHoi8_8"
   },
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10215,
     "status": "ok",
     "timestamp": 1756160568047,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "b6wNhGyR2WH1",
    "outputId": "4bdee18c-2ac2-463a-8bf6-104952222bd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './data' exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Define the path to the desired directory\n",
    "directory_path = './data'\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(directory_path):\n",
    "    print(f\"Directory '{directory_path}' exists.\")\n",
    "else:\n",
    "    print(f\"Directory '{directory_path}' does not exist.\")\n",
    "    print(\"Please make sure the path is correct and the folder is shared with your account if it's not your own.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uA3kmxTdirCc"
   },
   "source": [
    "## Búsqueda binaria con índice invertido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEKlAExBieYR"
   },
   "source": [
    "### Normalize documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1756160568051,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "_9LPqIaA9Tjf"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize_document(doc):\n",
    "  doc = re.sub(r\"[^a-zA-Z1-9\\s]\", \" \", doc, flags=re.I|re.A|re.MULTILINE)\n",
    "  doc = doc.lower()\n",
    "  doc = doc.strip()\n",
    "  tokens = wptk.tokenize(doc)\n",
    "  filtered_tokens = [ps.stem(token) for token in tokens if token not in stop_words]\n",
    "  # filtered_tokens = [lm.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "  doc = ' '.join(filtered_tokens)\n",
    "  return doc, filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 11477,
     "status": "ok",
     "timestamp": 1756160579530,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "jlS1LvEh5eMu"
   },
   "outputs": [],
   "source": [
    "files = []\n",
    "tokens = []\n",
    "for file in os.listdir(directory_path + '/docs-raw-texts'):\n",
    "  id = file.split('.')[1]\n",
    "  doc = KafNafParser(directory_path + f\"/docs-raw-texts/{file}\")\n",
    "  title = str(doc.get_header().get_fileDesc().get_title())\n",
    "  text, new_tokens = normalize_document(str(doc.get_raw()))\n",
    "  for token in new_tokens:\n",
    "    tokens.append((token, id))\n",
    "  files.append((id, title, text))\n",
    "\n",
    "# Inv Index has the following shape:\n",
    "# {<term1>: {<doc_1>: tf_doc1, <doc_2>: tf_doc2, ...} }\n",
    "\n",
    "inv_index = {}\n",
    "\n",
    "for t in tokens:\n",
    "  if t[0] not in inv_index:\n",
    "    inv_index[t[0]] = {}\n",
    "\n",
    "  if t[1] not in inv_index[t[0]]:\n",
    "    inv_index[t[0]][t[1]] = 1\n",
    "  else:\n",
    "    inv_index[t[0]][t[1]] += 1\n",
    "\n",
    "for t, v in inv_index.items():\n",
    "  inv_index[t] = {'freq': len(v), 'docs': v}\n",
    "\n",
    "docs = pd.DataFrame(files, columns=['id', 'title', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'freq': 2, 'docs': {'d162': 1, 'd086': 1}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_index[\"barber\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1756160579530,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "TYJviRsAE6xc"
   },
   "outputs": [],
   "source": [
    "# Inv Index API\n",
    "\n",
    "def get_term_docs(inv_index, term):\n",
    "  return set(inv_index[term]['docs'].keys())\n",
    "\n",
    "def get_term_tf(inv_index, term, doc):\n",
    "  return inv_index[term]['docs'].get(doc, 0)\n",
    "\n",
    "def get_term_df(inv_index, term):\n",
    "  return inv_index[term]['freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1756160579531,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "yB2H48LrBFh6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# for t, v in inv_index.items():\n",
    "#   if list(filter(lambda x: x>1, v['docs'].values())):\n",
    "#     print(f\"{t} has repeated docs: {v}\")\n",
    "\n",
    "  # repeated = len(v['docs']) == len(set(v['docs']))\n",
    "  # if repeated:\n",
    "  #   print(f\"{t} has repeated docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1756160579532,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "1YBQBF7aB_xR",
    "outputId": "0a5b1c97-457e-4544-cb50-b30c0106584f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13339"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization + Tokenize + Stemmer = 13702 terms\n",
    "# Regex filter + Normalization + Tokenize + Stemmer = 13339 terms\n",
    "# Regex filter + Normalization + Tokenize + Lemmatizer = 166623 terms\n",
    "len(inv_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3I0h7CKfiiuq"
   },
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1756160579532,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "OPlDmN-sEy4Q",
    "outputId": "99d37363-9c4e-4e99-d03f-c040c2763602"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d006'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def parse_query(query: str):\n",
    "  query_split = query.split()\n",
    "\n",
    "  include_terms = set()\n",
    "  exclude_terms = set()\n",
    "\n",
    "  not_pattern = r'NOT\\s+([a-zA-Z1-9]+)'\n",
    "  not_matches = re.findall(not_pattern, query, re.IGNORECASE)\n",
    "  exclude_terms = {normalize_document(term)[0] for term in not_matches}\n",
    "\n",
    "  query_without_not = re.sub(r'\\s+NOT\\s+\\w+', '', query, flags=re.IGNORECASE)\n",
    "  word_pattern = r'\\b(?!AND\\b|NOT\\b)(\\w+)\\b'\n",
    "  include_matches = re.findall(word_pattern, query_without_not, re.IGNORECASE)\n",
    "  include_terms = {normalize_document(term)[0] for term in include_matches}\n",
    "\n",
    "  return include_terms, exclude_terms\n",
    "\n",
    "def calculate_query(query: str):\n",
    "  include_terms, exclude_terms = parse_query(query)\n",
    "\n",
    "  docs = []\n",
    "  for t in include_terms:\n",
    "    if t not in inv_index:\n",
    "      continue\n",
    "    docs.append(set(inv_index[t]['docs'].keys()))\n",
    "\n",
    "  result = set.intersection(*docs)\n",
    "  for t in exclude_terms:\n",
    "    excluded_docs = set(inv_index[t]['docs'].keys())\n",
    "    result = result.difference(excluded_docs)\n",
    "\n",
    "  return result\n",
    "\n",
    "\n",
    "# query = \"move AND three AND chair\" # d006, d312\n",
    "query = \"Move AND Three AND Chair NOT brigham\" # d006\n",
    "calculate_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpI_jntTiPUU"
   },
   "source": [
    "### Calculate queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 11145,
     "status": "ok",
     "timestamp": 1756160590674,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "5VzArMJPuFEO"
   },
   "outputs": [],
   "source": [
    "queries = []\n",
    "for file in os.listdir(directory_path + '/queries-raw-texts'):\n",
    "  id = file.split('.')[1]\n",
    "  doc = KafNafParser(directory_path + f\"/queries-raw-texts/{file}\")\n",
    "  _, tokens = normalize_document(str(doc.get_raw()))\n",
    "  queries.append((id, ' AND '.join(tokens)))\n",
    "queries = sorted(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1756160590676,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "aZ8nZgS6wK0o"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for query in queries:\n",
    "  result = calculate_query(query[1])\n",
    "  results.append((query[0], result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1756160590978,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "ypl-tewIxUmu"
   },
   "outputs": [],
   "source": [
    "with open(directory_path + '/BSII-AND-queries_results.tsv', 'w') as f:\n",
    "  for r in results:\n",
    "    res_queries_str = \",\".join([f\"{doc}:1\" for doc in r[1]])\n",
    "    f.write(f\"{r[0]}\\t{res_queries_str}\")\n",
    "    f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUX8ay1CiLId"
   },
   "source": [
    "# Recuperación ranqueada y vectorización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ov81DvRzk71H"
   },
   "source": [
    "## Estrategia construcción de representación tf.idf\n",
    "\n",
    "**TODO**: Describir estrategia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1756160591185,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "gbmfK5_ujLV7"
   },
   "outputs": [],
   "source": [
    "def create_tf_idf(inv_index):\n",
    "  term_index = []\n",
    "  doc_map = {}\n",
    "  doc_index = []\n",
    "  doc_i = 0\n",
    "\n",
    "  # Calculate number of docs and assing indexes\n",
    "  for t, v in inv_index.items():\n",
    "    term_index.append(t)\n",
    "    for d, tf in v['docs'].items():\n",
    "      if d not in doc_map:\n",
    "        doc_map[d] = len(doc_index)\n",
    "        doc_index.append(d)\n",
    "\n",
    "  N = len(doc_index)\n",
    "  tf_idf = np.zeros((len(term_index), N))\n",
    "\n",
    "  # Calculate idf per term and tf-idf per document per term\n",
    "  for i, t in enumerate(term_index):\n",
    "    idf = np.log10(N/inv_index[t]['freq'])\n",
    "\n",
    "    for d, tf in inv_index[t]['docs'].items():\n",
    "      tf_idf[i, doc_map[d]] = np.log10(1+tf) * idf\n",
    "\n",
    "  return tf_idf, term_index, doc_index, doc_map\n",
    "\n",
    "tf_idf, term_index, doc_index, doc_map = create_tf_idf(inv_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1756160591190,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "wHma82cllH_q"
   },
   "outputs": [],
   "source": [
    "def cos_similarity(doc1_v, doc2_v):\n",
    "  norm_doc1_v = doc1_v/np.linalg.norm(np.array(doc1_v))\n",
    "  norm_doc2_v = doc2_v/np.linalg.norm(np.array(doc2_v))\n",
    "  return np.dot(norm_doc1_v, norm_doc2_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1756160591198,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "ZiUY8p5RqYcx",
    "outputId": "492fc1e7-f81a-4413-d5aa-451a3198b221"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2617"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_index.index('poetri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1756160591209,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "ZmeX-__rnBK9"
   },
   "outputs": [],
   "source": [
    "def calculate_doc_tfidf(query):\n",
    "  _, filtered_tokens = normalize_document(query)\n",
    "  tokens_index = {term: filtered_tokens.count(term) for term in filtered_tokens if term in inv_index}\n",
    "  query_tf_idf = np.zeros(len(term_index))\n",
    "\n",
    "  for t, tf in tokens_index.items():\n",
    "    idf = np.log10(len(doc_index)/inv_index[t]['freq'])\n",
    "    tfidf = np.log10(1+tf)*idf\n",
    "    query_tf_idf[term_index.index(t)] = tfidf\n",
    "\n",
    "  return query_tf_idf\n",
    "\n",
    "# Sample term index: {'famou': 156, 'german': 894, 'poetri': 204}\n",
    "# query = \"famous German poetry\"\n",
    "# calculate_doc_tfidf(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1756160591391,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "eg9isUq0q-U9"
   },
   "outputs": [],
   "source": [
    "queries = []\n",
    "for file in os.listdir(directory_path + '/queries-raw-texts'):\n",
    "  id = file.split('.')[1]\n",
    "  doc = KafNafParser(directory_path + f\"/queries-raw-texts/{file}\")\n",
    "  _, tokens = normalize_document(str(doc.get_raw()))\n",
    "  queries.append((id, ' AND '.join(tokens)))\n",
    "queries = sorted(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 5542,
     "status": "ok",
     "timestamp": 1756162055956,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "nZ1hjQXJrgMS"
   },
   "outputs": [],
   "source": [
    "def calculate_query_rank(query):\n",
    "  query_tf_idf = calculate_doc_tfidf(query)\n",
    "  similarity = [cos_similarity(query_tf_idf, tf_idf[:, i]) for i in range(tf_idf.shape[1])]\n",
    "  query_result = zip(doc_index, similarity)\n",
    "  query_result = list(filter(lambda x: x[1] > 0, query_result))\n",
    "  query_result = sorted(query_result, key=lambda x: x[1], reverse=True)\n",
    "  return query_result\n",
    "\n",
    "results = []\n",
    "for query in queries:\n",
    "  result = calculate_query_rank(query[1])\n",
    "  results.append((query[0], result))\n",
    "results = sorted(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1756162055964,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "lkXs8T5y4uJL"
   },
   "outputs": [],
   "source": [
    "with open(directory_path + '/RRDV-consultas_resultados.tsv', 'w') as f:\n",
    "  for r in results:\n",
    "    res_queries_str = \",\".join([f\"{doc[0]}:{doc[1]}\" for doc in r[1]])\n",
    "    f.write(f\"{r[0]}\\t{res_queries_str}\")\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38hfhuopjFBV"
   },
   "source": [
    "# Judgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1756160647319,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "jSXPk29Ysxa_"
   },
   "outputs": [],
   "source": [
    "judgements = pd.read_csv(directory_path + '/relevance-judgments.tsv', sep='\\t', header=None)\n",
    "judgements[1] = judgements[1].str.split(',', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1756163966218,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "FYah0kDkW8oo"
   },
   "outputs": [],
   "source": [
    "metrics = [] # query, P@M, R@M, NDCG@M\n",
    "queries_precision = []\n",
    "\n",
    "for q in range(len(results)):\n",
    "# for q in range(1):\n",
    "  query_id, relevant_docs = judgements.iloc[q]\n",
    "  M = len(relevant_docs)\n",
    "  relevant_docs_bin = {doc.split(':')[0]: doc.split(':')[1] for doc in relevant_docs}\n",
    "\n",
    "  bin_relevance = np.zeros(M)\n",
    "  ranked_relevance = np.zeros(M)\n",
    "  query_result = results[q][1]\n",
    "  for k in range(M):\n",
    "    if query_result[k][0] in relevant_docs_bin:\n",
    "      bin_relevance[k] = 1\n",
    "\n",
    "    ranked_relevance[k] = relevant_docs_bin.get(query_result[k][0], 0)\n",
    "\n",
    "  metrics.append((\n",
    "      query_id,\n",
    "      precision_at_k(bin_relevance, M),\n",
    "      recall_at_k(bin_relevance, M, M),\n",
    "      ndcg_at_k(ranked_relevance, M)\n",
    "  ))\n",
    "  queries_precision.append(bin_relevance)\n",
    "\n",
    "res_map = mean_average_precision(queries_precision)\n",
    "res_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]),\n",
       " array([1., 1., 0., 1., 0., 0.]),\n",
       " array([1., 1., 0., 1., 1., 1., 0.]),\n",
       " array([1., 1., 1., 0., 1., 0.]),\n",
       " array([0., 1., 0., 0.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.]),\n",
       " array([1., 1., 1., 1., 1., 0.]),\n",
       " array([1., 0., 1., 0., 1., 0., 0., 0.]),\n",
       " array([1., 1., 1., 0.]),\n",
       " array([1., 1., 0., 1., 1.]),\n",
       " array([0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.]),\n",
       " array([0., 1.]),\n",
       " array([1., 1., 1., 0.]),\n",
       " array([1., 1., 1., 0., 1., 1., 1.]),\n",
       " array([1., 0.]),\n",
       " array([1., 1., 1., 0., 0., 0., 1.]),\n",
       " array([1., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 1., 0.]),\n",
       " array([1., 1., 0., 0.]),\n",
       " array([1.]),\n",
       " array([1., 1., 0., 1., 0., 0., 0., 0.]),\n",
       " array([1., 1., 0.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.]),\n",
       " array([1., 1., 1., 1., 1.]),\n",
       " array([1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 0., 1.]),\n",
       " array([1., 1., 0.]),\n",
       " array([1., 0., 1., 0., 0., 1., 0., 0.]),\n",
       " array([1., 1., 1., 1., 0., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 0.]),\n",
       " array([1., 0., 0.]),\n",
       " array([1., 1., 1., 0., 1., 1., 0., 0., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 0., 0.]),\n",
       " array([1., 0., 0., 0., 1., 0.])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_precision[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_precision[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1756164031134,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "ZdR67Zvhh4rL"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(directory_path + '/IR_Metrics.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['query_id', 'P@M', 'R@M', 'NDCG@M'])\n",
    "    writer.writerows(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1756163832813,
     "user": {
      "displayName": "Cristobal Arroyo",
      "userId": "09735819368631376087"
     },
     "user_tz": 300
    },
    "id": "dACl4VHFg98C",
    "outputId": "1df9d18f-4138-4955-e8ae-929c4748cd66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8698643828142127"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_map = mean_average_recall(queries_precision)\n",
    "res_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]),\n",
       " array([1., 1., 0., 1., 0., 0.]),\n",
       " array([1., 1., 0., 1., 1., 1., 0.]),\n",
       " array([1., 1., 1., 0., 1., 0.]),\n",
       " array([0., 1., 0., 0.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.]),\n",
       " array([1., 1., 1., 1., 1., 0.]),\n",
       " array([1., 0., 1., 0., 1., 0., 0., 0.]),\n",
       " array([1., 1., 1., 0.]),\n",
       " array([1., 1., 0., 1., 1.]),\n",
       " array([0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.]),\n",
       " array([0., 1.]),\n",
       " array([1., 1., 1., 0.]),\n",
       " array([1., 1., 1., 0., 1., 1., 1.]),\n",
       " array([1., 0.]),\n",
       " array([1., 1., 1., 0., 0., 0., 1.]),\n",
       " array([1., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 1., 0.]),\n",
       " array([1., 1., 0., 0.]),\n",
       " array([1.]),\n",
       " array([1., 1., 0., 1., 0., 0., 0., 0.]),\n",
       " array([1., 1., 0.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.]),\n",
       " array([1., 1., 1., 1., 1.]),\n",
       " array([1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 0., 1.]),\n",
       " array([1., 1., 0.]),\n",
       " array([1., 0., 1., 0., 0., 1., 0., 0.]),\n",
       " array([1., 1., 1., 1., 0., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 0.]),\n",
       " array([1., 0., 0.]),\n",
       " array([1., 1., 1., 0., 1., 1., 0., 0., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 0., 0.]),\n",
       " array([1., 0., 0., 0., 1., 0.])]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "EiHW70al82yF"
   },
   "outputs": [],
   "source": [
    "docs.to_csv(directory_path + '/docs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN8tNGVyPP6shXq9fAoKxLf",
   "mount_file_id": "1osJ_L0tmHWjsSWoR5da-KG0PV5WPNttb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
